<!DOCTYPE html>
<html lang="en">
	<head>
    <title>Initialization - Bilal Khan</title>
    
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="My Blog"/>

    <meta property="og:title" content="
    Bilal&#x27;s Blog -&nbsp;Initialization" />
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;initialization&#x2F;"/>
    <meta property="og:description" content="I&#x27;m starting this series of blog posts by writing down my notes on the different types of normalization in neural networks. Let&#x27;s see how this goes."/>
    <link rel="stylesheet" href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;style.css">
    <link rel="stylesheet" href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;color&#x2F;red.css">
<link rel="alternate" type="application/rss+xml" title="Bilal&#x27;s Blog RSS" href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;rss.xml"><script
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
			async
		></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
	</head>
	<body>
		<div class="container">
<header class="header">
    <div class="header__inner">
        <div class="header__logo">
            <a href="&#x2F;blog">
    <div class="logo">
        Bilal&#x27;s Blog
    </div>
</a>
        </div>
        <div class="menu-trigger">menu</div>
    </div>
    
<nav class="menu">
	<ul class="menu__inner menu__inner--desktop">
		 
		<li>
			<a href="https://bilal2vec.github.io">Home</a>
		</li>
		 </ul>

	<ul class="menu__inner menu__inner--mobile">
		
<li>
	<a href="https://bilal2vec.github.io">Home</a>
</li>
	</ul>
</nav>

    </header>
<div class="content"><div class="post">
        <h1 class="post-title">
            <a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;initialization&#x2F;">Initialization</a>
        </h1>
        
    <div class="post-meta">
        <span class="post-date">2020.07.03
                </span>

        <span class="post-author"></span>

        

    
    :: {<a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;categories&#x2F;notes&#x2F;">notes</a>} 

            
    ::
    #<a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;tags&#x2F;ai&#x2F;">AI</a>
        
    #<a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;tags&#x2F;ml&#x2F;">ML</a>
        
    
            
        
    </div>



        

        <div class="post-content">
            <ul>
<li><a href="/blog/normalization">Notes Part 1: Normalization</a></li>
<li><a href="/blog/perplexity">Notes Part 2: Perplexity</a></li>
<li><a href="/blog/initialization">Notes Part 3: Initialization</a></li>
<li><a href="/blog/memory-usage">Notes Part 4: GPU Memory Usage Breakdown</a></li>
<li><a href="/blog/adafactor">Part 5: Adafactor</a></li>
</ul>
<hr />
<h2 id="purpose">Purpose<a class="zola-anchor" href="#purpose" aria-label="Anchor link for: purpose">§</a>
</h2>
<hr />
<p>The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in learning, help me keep track of everything I've learned over the last three years, and to practice my Latex skills.</p>
<p>This is my third blog post in the series, and this time I'm really just Cmd+C'ing and Cmd+V'ing over some of my notes on initialization for neural networks</p>
<hr />
<h2 id="notation">Notation<a class="zola-anchor" href="#notation" aria-label="Anchor link for: notation">§</a>
</h2>
<hr />
<ul>
<li>$\mu$: Mean</li>
<li>$\sigma ^ 2$: Standard Deviation</li>
<li>$c_{in}$: Number of input channels to a layer</li>
<li>$c_{out}$: Number of output channels to a layer</li>
</ul>
<hr />
<h2 id="initialization">Initialization<a class="zola-anchor" href="#initialization" aria-label="Anchor link for: initialization">§</a>
</h2>
<hr />
<ul>
<li>$\mu$ and $\sigma ^ 2$ of activations should be close to $0$ and $1$ to prevent the gradients from exploding or vanishing</li>
<li>activations of layers have $\sigma ^ 2$ close to $\sqrt {c_{in}}$</li>
<li>so, to get the $\sigma ^ 2$ back to $1$, multiply randomly initialized weights by $1 / sqrt(c_{in})$</li>
<li>this works well without activations, but results in vanishing or exploding gradients when used with a tanh or sigmoid activation function</li>
<li>bias weights should be initialized to $0$</li>
<li>intializations can either be from a uniform distribution or a normal distribution</li>
<li>use <strong>Xavier Initialization</strong> for sigmoid and softmax activations</li>
<li>use <strong>Kaiming Initialization</strong> for ReLU or Leaky ReLU activations</li>
</ul>
<hr />
<h3 id="xavier-or-glorot-initialization">Xavier or Glorot Initialization<a class="zola-anchor" href="#xavier-or-glorot-initialization" aria-label="Anchor link for: xavier-or-glorot-initialization">§</a>
</h3>
<hr />
<h4 id="uniform-initialization">Uniform initialization:<a class="zola-anchor" href="#uniform-initialization" aria-label="Anchor link for: uniform-initialization">§</a>
</h4>
<hr />
<ul>
<li>bound a uniform distribution between $\pm \sqrt { \frac {6} {c_{in} + c_{out}}}$</li>
</ul>
<hr />
<h4 id="normal-initialization">Normal initialization:<a class="zola-anchor" href="#normal-initialization" aria-label="Anchor link for: normal-initialization">§</a>
</h4>
<hr />
<ul>
<li>multiply a normal distribution by $\sqrt \frac {2} {c_{in} + c_{out}}$</li>
<li>or create a normal distribution with $\mu = 0$ and $\sigma ^ 2 = \sqrt \frac {2} {c_{in} + c_{out}}$</li>
<li>helps keep identical variances across layers</li>
</ul>
<hr />
<h3 id="kaiming-or-he-initialization">Kaiming or He initialization<a class="zola-anchor" href="#kaiming-or-he-initialization" aria-label="Anchor link for: kaiming-or-he-initialization">§</a>
</h3>
<hr />
<ul>
<li>when using a ReLU activation, $\sigma ^ 2$ will be close to $\sqrt \frac {c_{in}} {2}$, so multiplying the normally distributed activations by $\sqrt \frac {2} {c_{in}}$ will make the activations have a $\sigma ^ 2$ close to $1$</li>
</ul>
<hr />
<h4 id="uniform-initialization-1">Uniform initialization:<a class="zola-anchor" href="#uniform-initialization-1" aria-label="Anchor link for: uniform-initialization-1">§</a>
</h4>
<hr />
<ul>
<li>bound a uniform distribution between $\pm \sqrt \frac {6} {c_{in}}$</li>
</ul>
<hr />
<h4 id="normal-initialization-1">Normal initialization<a class="zola-anchor" href="#normal-initialization-1" aria-label="Anchor link for: normal-initialization-1">§</a>
</h4>
<hr />
<ul>
<li>multiply a normal distribution by $\sqrt \frac {2} {c_{in}}$</li>
<li>or create a normal distribution with $\mu = 0$ and $\sigma ^ 2 = \sqrt \frac {2} {c_{in}}$</li>
</ul>
<hr />
<h3 id="gain">Gain<a class="zola-anchor" href="#gain" aria-label="Anchor link for: gain">§</a>
</h3>
<hr />
<ul>
<li>multiplied to init bounds/stddevs</li>
<li>$\sqrt 2$ for ReLU</li>
<li>none for Kaiming</li>
</ul>
<hr />
<h3 id="pytorch-defaults">Pytorch defaults<a class="zola-anchor" href="#pytorch-defaults" aria-label="Anchor link for: pytorch-defaults">§</a>
</h3>
<hr />
<ul>
<li>most layers are initialized with Kaiming uniform as a reasonable default</li>
<li>use Kaiming with correct gain (https://pytorch.org/docs/stable/nn.html#torch.nn.init.calculate_gain)</li>
</ul>
<hr />
<h3 id="resources">Resources<a class="zola-anchor" href="#resources" aria-label="Anchor link for: resources">§</a>
</h3>
<hr />
<ul>
<li>https://github.com/pytorch/pytorch/issues/15314</li>
<li>https://medium.com/@sakeshpusuluri123/activation-functions-and-weight-initialization-in-deep-learning-ebc326e62a5c</li>
<li>https://pytorch.org/docs/stable/_modules/torch/nn/init.html</li>
<li>https://discuss.pytorch.org/t/whats-the-default-initialization-methods-for-layers/3157/21</li>
<li>https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79</li>
<li>https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404</li>
<li>https://pytorch.org/docs/stable/nn.html#torch.nn.init.calculate_gain</li>
<li>https://github.com/mratsim/Arraymancer/blob/master/src/nn/init.nim</li>
<li>https://jamesmccaffrey.wordpress.com/2018/08/21/pytorch-neural-network-weights-and-biases-initialization/</li>
</ul>

        </div>
        
    
</div></div>
			
    <div class="pagination">
        <div class="pagination__buttons">
            </div>
    </div>
<footer class="footer">
				<div class="footer__inner"><div class="copyright">
            <span>© 2020 <a href="https://github.com/ejmg/zerm">zerm</a> :: Powered by <a href="https://www.getzola.org/">Zola</a></span>
            <span>:: Theme made by <a href="https://github.com/ejmg">ejmg</a></span>
        </div>
    <script type="text/javascript" src="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;assets&#x2F;js&#x2F;main.js"></script>
</div>
				

			</footer></div>
	</body>
</html>
