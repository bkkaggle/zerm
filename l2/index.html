<!DOCTYPE html>
<html lang="en">
	<head>
    <title>[Extremely WIP] Make your own fast PyTorch-style ML library in Rust - Bilal Khan</title>
    
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="My Blog"/>

    <meta property="og:title" content="
    Bilal&#x27;s Blog -&nbsp;[Extremely WIP] Make your own fast PyTorch-style ML library in Rust" />
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;l2&#x2F;"/>
    <meta property="og:description" content="My Blog"/>
    <link rel="stylesheet" href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;style.css">
    <link rel="stylesheet" href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;color&#x2F;red.css">
<link rel="alternate" type="application/rss+xml" title="Bilal&#x27;s Blog RSS" href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;rss.xml"><script
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
			async
		></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
	</head>
	<body>
		<div class="container">
<header class="header">
    <div class="header__inner">
        <div class="header__logo">
            <a href="&#x2F;blog">
    <div class="logo">
        Bilal&#x27;s Blog
    </div>
</a>
        </div>
        <div class="menu-trigger">menu</div>
    </div>
    
<nav class="menu">
	<ul class="menu__inner menu__inner--desktop">
		 
		<li>
			<a href="https://bilal2vec.github.io">Home</a>
		</li>
		 </ul>

	<ul class="menu__inner menu__inner--mobile">
		
<li>
	<a href="https://bilal2vec.github.io">Home</a>
</li>
	</ul>
</nav>

    </header>
<div class="content"><div class="post">
        <h1 class="post-title">
            <a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;l2&#x2F;">[Extremely WIP] Make your own fast PyTorch-style ML library in Rust</a>
        </h1>
        
    <div class="post-meta">
        <span class="post-date">2020.08.02
                </span>

        <span class="post-author"></span>

        

    
    :: {<a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;categories&#x2F;deep-dives&#x2F;">deep-dives</a>,
            <a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;categories&#x2F;rust&#x2F;">rust</a>} 

            
    ::
    #<a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;tags&#x2F;ml&#x2F;">ML</a>
        
    #<a href="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;tags&#x2F;rust&#x2F;">Rust</a>
        
    
            
        
    </div>



        

        <div class="post-content">
            <hr />
<p><img alt='A code example showing how my library could be used' src='https://raw.githubusercontent.com/bilal2vec/L2/master/screenshot.png' width='100%'></img></p>
<p align='center'>
    <a href="">
        <img src="https://github.com/bilal2vec/l2/workflows/Rust/badge.svg" alt="Rust: CI">
    </a>
    <a href="https://opensource.org/licenses/MIT">
        <img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT">
    </a>
    <a href="https://crates.io/crates/l2">
        <img alt="crates.io l2 badge" src="http://meritbadge.herokuapp.com/l2">
    </a>
    <a href=" https://docs.rs/l2">
        <img alt="docs.rs l2 badge" src="https://docs.rs/l2/badge.svg">
    </a>
</p>
<hr />
<h1 id="tl-dr"><a name='tldr' href='#tldr'>TL;DR</a><a class="zola-anchor" href="#tl-dr" aria-label="Anchor link for: tl-dr">Â§</a>
</h1>
<hr />
<p>This blog post shows you, step-by-step, how to build a fast <a href="https://pytorch.org/">PyTorch</a>-style machine learning library in the <a href="https://www.rust-lang.org/">Rust programming language</a>. This blog post is based on a library called <a href="https://github.com/bilal2vec/L2">L2</a> that I finished working on a while ago.</p>
<p>I <a href="https://bilal2vec.github.io/blog/l2/#resources">compiled</a> quite a long list of blog posts, articles, and GitHub repos that I found useful when I was working on L2, so take a look at that if that's the type of stuff you're interested in.</p>
<hr />
<p><strong>Disclaimers</strong>:</p>
<p>L2 is a small project I was working on during the summer before uni for fun <sup class="footnote-reference"><a href="#1">1</a></sup>, so don't expect it to be production-ready or bug-free. <sup class="footnote-reference"><a href="#2">2</a></sup></p>
<p>I'm going to assume that everyone who's reading this knows or uses Rust relatively well and is familiar with how PyTorch and TF work at a high level. If you want to learn about these topics or just brush up on some things that you aren't ðŸ’¯ clear on, try looking through my <a href="https://bilal2vec.github.io/blog/l2/#resources">resources</a> section.</p>
<p>L2 is surprisingly fast especially since I didn't try very hard to optimize all the operators, it's usually only less than one order of magnitude slower than PyTorch in most of the benchmarks that I ran. L2 only supports a CPU backend at the moment since I'm not familiar enough with Rust to start working with CUDA and cuDNN. So far, it doesn't have any PyTorch-style high level abstractions that are really useful for machine learning like PyTorch's <code>Parameter</code>, <code>Layer</code>, or <code>Module</code> classes. There might still be some bugs in the transpose operators and calling <code>.backward()</code> on broadcasted tensors. The autograd system won't automatically clear unused buffers once they've been used so this won't be as memory efficient as PyTorch.</p>
<p>I wrote dozens of tests and benchmarks to make sure that L2 was working properly when I was developing it. I'm going to be omitting tests in this blog post and instead just going to show some example code in <code>src/bin/main.rs</code>.</p>
<hr />
<h1 id="table-of-contents"><a name='toc' href='#toc'>Table of Contents</a><a class="zola-anchor" href="#table-of-contents" aria-label="Anchor link for: table-of-contents">Â§</a>
</h1>
<hr />
<p><strong>If you just want to skip to the code part of the tutorial, click <a href="https://bilal2vec.github.io/blog/l2/#baseline">here</a></strong></p>
<ul>
<li><a href="https://bilal2vec.github.io/blog/l2/#tldr">TL;DR</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#toc">Table of Contents</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#background">Background</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#getstarted">Part 1: Let's get started</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#baseline">Part 2: A simple baseline</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#broadcasting">Part 3: Broadcasting</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#ops">Part 4: Ops</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#autograd">Part 5: Autograd</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#advancedops">Part 6: Advanced Ops</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#future">Future work</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#conclusion">Conclusion</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#resources">Resources</a></li>
<li><a href="https://bilal2vec.github.io/blog/l2/#references">References</a></li>
</ul>
<hr />
<h1 id="background"><a name='background' href='#background'>Background</a><a class="zola-anchor" href="#background" aria-label="Anchor link for: background">Â§</a>
</h1>
<hr />
<p>Last summer <sup class="footnote-reference"><a href="#3">3</a></sup>, I <a href="https://github.com/bilal2vec/L2/tree/c%2B%2B">wrote</a> a machine learning library as a way of getting better at using C++. The library wasn't really that advanced (I didn't have an autograd system like PyTorch does, instead I just did the backprop calculations by hand for each layer) or very fast (I pretty much passed everything by value and didn't really put a focus on making my code fast and performant), but it was a good way at getting a lot of experience working with a lower level language like c++ that I'd never used before and I learned a lot about how machine learning libraries like Pytorch and Tensorflow work behind the scenes.</p>
<p>This summer, I did a complete rewrite of L2, this time in Rust, with a focus on making it as close to Pytorch as I could (speed and feature wise) and got to learn about and implement a lot of interesting and cool features that are used in all the popular machine learning libraries today.</p>
<p>I'm pretty satisfied with how L2 turned out, here's the pitch I wrote for it on my GitHub repo:</p>
<p>L2 is a Pytorch-style Tensor+Autograd library written in Rust. It contains a multidimensional array class, <code>Tensor</code>, with support for strided arrays, numpy-style array slicing, broadcasting, and most major math operations (including fast, BLAS-accelerated matrix multiplication!). On top of this, L2 has a built-in efficient graph-based autograd engine that keeps track of all operations performed on a tensor and topologically sorts and traverses the graph to compute the gradients.</p>
<p>I'm also pretty happy with how the user-facing API of the library turned out:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">l2::tensor::</span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">;

</span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> x: Tensor </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::normal(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">2</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">4</span><span style="color:#abb2bf;">], </span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">)</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;
</span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> y: Tensor </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::normal(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">4</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">], </span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">)</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

</span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> z: Tensor </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">l2::matmul(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">x, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">y)</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

z.</span><span style="color:#5ebfcc;">backward</span><span style="color:#abb2bf;">();

println!(</span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, z);
</span></code></pre>
<hr />
<h1 id="let-s-get-started"><a name='getstarted' href='#getstarted'>Let's get started</a><a class="zola-anchor" href="#let-s-get-started" aria-label="Anchor link for: let-s-get-started">Â§</a>
</h1>
<hr />
<p>So let's get started. I'm pretty much just copying down the installation instructions from the official <a href="https://www.rust-lang.org/learn/get-started">get started</a> guide, so take a look at that if you want.</p>
<p>Install rustup to your computer:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro ~ </span><span style="color:#abb2bf;">% curl</span><span style="color:#eb6772;"> --proto </span><span style="color:#9acc76;">&#39;=https&#39;</span><span style="color:#eb6772;"> --tlsv1</span><span style="color:#abb2bf;">.2</span><span style="color:#eb6772;"> -sSf</span><span style="color:#abb2bf;"> https://sh.rustup.rs </span><span style="color:#adb7c9;">| </span><span style="color:#eb6772;">sh
</span></code></pre>
<hr />
<p>Switch the default rust version to nightly:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro ~ </span><span style="color:#abb2bf;">% rustup default nightly
</span></code></pre>
<hr />
<p>I'll be using my preferred text editor <a href="https://code.visualstudio.com/">VScode</a> in this post, but feel free to use whatever editor you prefer.</p>
<p>I highly recommend using the (soon to become) official Rust extension for VScode, <a href="https://marketplace.visualstudio.com/items?itemName=matklad.rust-analyzer">Rust-analyzer</a> instead of the old <a href="https://marketplace.visualstudio.com/items?itemName=rust-lang.rust">RLS</a> extension. Just install it from the marketplace and you should be ready to go.</p>
<p>Create a new Rust library called <code>l2</code> with:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro ~ </span><span style="color:#abb2bf;">% cargo new l2</span><span style="color:#eb6772;"> --lib
</span></code></pre>
<hr />
<p>Install clippy (Rust's official linter):</p>
<p><em>You can take a look at all the lint rules and how to fix each one <a href="https://rust-lang.github.io/rust-clippy/master/index.html">here</a></em></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro ~ </span><span style="color:#abb2bf;">% rustup component add clippy
</span></code></pre>
<hr />
<p>And change rust-analyzer to use clippy as its default linter by creating a <code>.vscode/settings.json</code> file and pasting this in it.</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;">{
	</span><span style="color:#9acc76;">&quot;rust-analyzer.checkOnSave.command&quot;</span><span style="color:#abb2bf;">: </span><span style="color:#9acc76;">&quot;clippy&quot;
</span><span style="color:#abb2bf;">}
</span></code></pre>
<hr />
<p>For debugging support, I use the <a href="https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb">Code-LLDB</a> extension, so install that as well.</p>
<p>create a <code>.vscode/launch.json</code> file and paste this into it:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;">{
	</span><span style="color:#9acc76;">&quot;version&quot;</span><span style="color:#abb2bf;">: </span><span style="color:#9acc76;">&quot;0.2.0&quot;</span><span style="color:#abb2bf;">,
	</span><span style="color:#9acc76;">&quot;configurations&quot;</span><span style="color:#abb2bf;">: [
		{
			</span><span style="color:#9acc76;">&quot;type&quot;</span><span style="color:#abb2bf;">: </span><span style="color:#9acc76;">&quot;lldb&quot;</span><span style="color:#abb2bf;">,
			</span><span style="color:#9acc76;">&quot;request&quot;</span><span style="color:#abb2bf;">: </span><span style="color:#9acc76;">&quot;launch&quot;</span><span style="color:#abb2bf;">,
			</span><span style="color:#9acc76;">&quot;name&quot;</span><span style="color:#abb2bf;">: </span><span style="color:#9acc76;">&quot;Debug&quot;</span><span style="color:#abb2bf;">,
			</span><span style="color:#9acc76;">&quot;program&quot;</span><span style="color:#abb2bf;">: </span><span style="color:#9acc76;">&quot;${workspaceRoot}/target/debug/main&quot;</span><span style="color:#abb2bf;">,
			</span><span style="color:#9acc76;">&quot;args&quot;</span><span style="color:#abb2bf;">: [],
			</span><span style="color:#9acc76;">&quot;cwd&quot;</span><span style="color:#abb2bf;">: </span><span style="color:#9acc76;">&quot;${workspaceRoot}&quot;
		</span><span style="color:#abb2bf;">}
	]
}
</span></code></pre>
<hr />
<p>Add a rust binary target <code>src/bin/main.rs</code> that will be linked against our library at <code>src/lib.rs</code>. Your project should now have a directory structure like this:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;">.git/
.vscode/
    settings.json
    launch.json
src/
    bin/
        main.rs
    lib.rs
target/
.gitignore
Cargo.lock
Cargo.toml
</span></code></pre>
<hr />
<p>We'll code up the library in <code>src/lib.rs</code> and any other files in the <code>src/</code> directory. We'll use <code>src/bin/main.rs</code> to interact with L2 as you would when using it in your own project.</p>
<hr />
<h1 id="a-simple-baseline"><a name='baseline' href='#baseline'>A simple baseline</a><a class="zola-anchor" href="#a-simple-baseline" aria-label="Anchor link for: a-simple-baseline">Â§</a>
</h1>
<hr />
<p>Ok, so let's start by creating a simple <code>Tensor</code> struct and defining a few simple operations on it.</p>
<p>A <code>Tensor</code> is really just a multidimensional array. For this library, we'll keep it simple and restrict tensors to have at most $2$ dimensions (You'll see why later).</p>
<p>The simplest way to store a multidimensional array of say, dimensions $m \times n$ would be to create an array of length $m$ that holds a pointers to $m$ distinct arrays of length $n$, each holding the elements of a single row. This would be the simplest way to represent a <code>Tensor</code> but isn't really optimal when you need to create and process large <code>Tensors</code> quickly.</p>
<p>Most (if not all) people use <em>strided arrays</em>, where elements of a multidimensional array are layed out contigously in memory (the $m \times n$ <code>Tensor</code> would then be represented as single array of length $m * n$).</p>
<p>Take a look at http://blog.ezyang.com/2019/05/pytorch-internals/ for a good in-depth look into how PyTorch uses strided arrays. I'll summarize the main parts below:</p>
<p>Say you have a $2 \times 2$ <code>Tensor</code> like this:</p>
<p>$$
\begin{bmatrix}
1 &amp; 2 \
3 &amp; 4
\end{bmatrix}
$$</p>
<p>If you wanted to represent this as a strided array, you could either store them in row-major or column-major order, storing either values from a single row or column contigously in memory (the same idea would still apply if you have a <code>Tensor</code> of more dimensions):</p>
<p>$$
\text {row-major:}
\begin{bmatrix}
1, 2, 3, 4
\end{bmatrix}
$$</p>
<p>$$
\text {column-major:}
\begin{bmatrix}
1, 3, 2, 4
\end{bmatrix}
$$</p>
<p>Most machine learning libraries like Numpy, PyTorch, and Tensorflow store Tensors in row-major order by default. This lets you quickly access the next element in the same row just by moving one element to the right in the <code>Tensor</code>. Column-major order isn't as commonly used, the only time I had to use it when I was integrating a BLAS library written in very optimized Fortran into L2 in order to use its super fast matrix multiplication implementations (using BLAS sped up my matrix multiplication code by about 200 times IIRC). </p>
<p>The choice of whether to store your data in column-major or row-major order depends on whether you prefer to have contigous access to elements in the first or last dimensions of your <code>Tensor</code>. For example, if you store a batch of $N$ three-channel image in a <code>Tensor</code> of dimensions ($256$, $256$, $3$), you would be able to either access the channels or the batch dimension contigously (i.e. have the elements in that dimension be next to each other in memory) depending on whether it's stored in row-major or column-major order.</p>
<p>The <em>stride</em> for each dimension of a <em>strided array</em> is the number of elements you want to skip between neighboring elements of a <code>Tensor</code> in a particular dimension. For example, our original <code>Tensor</code> of shape $\begin{bmatrix} 2, 2 \end{bmatrix}$ has strides of $\begin{bmatrix} 2, 1 \end{bmatrix}$.</p>
<p>This means that if we want to advance one element in the column dimension (from the element $1$ to the element $3$) of the <em>logical</em> <code>Tensor</code>, we need to advance $2$ elements at a time in the <em>strided</em> <code>Tensor</code>.</p>
<p>$$
\text {Logical Tensor:}</p>
<p>\begin{bmatrix}
1 &amp; \color{gray} 2 \
3 &amp; \color{gray} 4
\end{bmatrix}
$$</p>
<p>$$
\text {Strided Tensor:}
\begin{bmatrix}
1, \color{gray} 2, \color{white} 3, \color{gray} 4
\end{bmatrix}
$$</p>
<p>The same would be true for the other dimensions as well. If we want to advance one element in the row dimension (from the element $1$ to the element $2$) of the <em>logical</em> <code>Tensor</code>, we would advance $1$ element in the <em>strided</em> <code>Tensor</code>.</p>
<p>$$
\text {Logical Tensor:}</p>
<p>\begin{bmatrix}
1 &amp;  2 \
\color{gray} 3 &amp; \color{gray} 4
\end{bmatrix}
$$</p>
<p>$$
\text {Strided Tensor:}
\begin{bmatrix}
1,  2, \color{gray} 3, 4
\end{bmatrix}
$$</p>
<p>If we wanted to get the <em>physical</em> location in memory of a specific element in the <code>Tensor</code> from the <em>logical</em> location, we can simply &quot;multiply each index with the respective stride for that dimension, and sum them all together&quot; <sup class="footnote-reference"><a href="#4">4</a></sup>. So for an example, if we want to get the <em>physical</em> index of the element at the <em>logical</em> indices $[ 1, 1]$, we would calculate it like this:</p>
<p>$$
\text{logical index: } [\color{red} 1, \color{blue} 1 \color{white}] \ \text{strides: } [\color{red} 2, \color{blue} 1 \color{white}]
$$</p>
<br />
<p>$$
\text{physical index} = {\color{blue} 1} {\color{white} *} {\color{blue} 1} {\color{white} +} {\color{red} 1} {\color{white} *} {\color{red} 2} 
$$</p>
<p>$$
\text{physical index} =  1 + 2 = 3
$$</p>
<p>$$
\text {element at [1, 1]} =
\begin{bmatrix}
\color{gray} 1,  2,  3, \color{white} 4
\end{bmatrix}
$$</p>
<hr />
<p>So now that we have that out of the way, let's start writing some code.</p>
<p>In this section, we'll make a basic <code>Tensor</code> struct the just creates and stores a strided array. We'll also take advantage of Rust's excellent error handling primitives to add robust error handling and add pretty printing of our <code>Tensors</code>.</p>
<p>Let's make a new file at <code>src/tensor.rs</code> to house our <code>Tensor</code> struct.</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#cd74e8;">use crate</span><span style="color:#abb2bf;">::errors::TensorError;

</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">std::fmt;

#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, PartialEq)]
</span><span style="color:#cd74e8;">pub struct </span><span style="color:#abb2bf;">Tensor {
    </span><span style="color:#cd74e8;">pub </span><span style="color:#eb6772;">data</span><span style="color:#abb2bf;">: Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt;,
    </span><span style="color:#cd74e8;">pub </span><span style="color:#eb6772;">shape</span><span style="color:#abb2bf;">: Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt;,
    </span><span style="color:#cd74e8;">pub </span><span style="color:#eb6772;">strides</span><span style="color:#abb2bf;">: Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt;,
}

</span><span style="font-style:italic;color:#5f697a;">// Pretty print Tensors
</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Tensor {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> graph </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">format!(</span><span style="color:#9acc76;">&quot;Value: </span><span style="color:#db9d63;">{:?} </span><span style="color:#5ebfcc;">\n</span><span style="color:#9acc76;">Shape: </span><span style="color:#db9d63;">{:?}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">,
            </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.data, </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape);

        write!(f, </span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, graph)
    }
}

</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Clone </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">clone</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">) -&gt; </span><span style="color:#cd74e8;">Self </span><span style="color:#abb2bf;">{
        Tensor::new(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.data.</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">(), </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">()
    }
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Tensor {
    </span><span style="font-style:italic;color:#5f697a;">// Calculate the number of elements in a tensor from the shape
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">calc_tensor_len_from_shape</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">shape</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">]) -&gt; </span><span style="color:#cd74e8;">usize </span><span style="color:#abb2bf;">{
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> length </span><span style="color:#adb7c9;">= </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">;
        </span><span style="color:#cd74e8;">for</span><span style="color:#abb2bf;"> i </span><span style="color:#adb7c9;">in</span><span style="color:#abb2bf;"> shape {
            length </span><span style="color:#adb7c9;">*=</span><span style="color:#abb2bf;"> i;
        }

        length
    }

    </span><span style="font-style:italic;color:#5f697a;">// calculate the strides for each dimension from the shape
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">calc_strides_from_shape</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">shape</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">]) -&gt; Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt; {
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> strides </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Vec::with_capacity(shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">());

        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> current_stride </span><span style="color:#adb7c9;">= </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">;
        </span><span style="color:#cd74e8;">for</span><span style="color:#abb2bf;"> i </span><span style="color:#adb7c9;">in</span><span style="color:#abb2bf;"> shape.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">rev</span><span style="color:#abb2bf;">() {
            strides.</span><span style="color:#5ebfcc;">insert</span><span style="color:#abb2bf;">(</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">, current_stride);
            current_stride </span><span style="color:#adb7c9;">*=</span><span style="color:#abb2bf;"> i;
        }

        strides
    }

    </span><span style="font-style:italic;color:#5f697a;">// Create a new tensor from some data with a specific shape
    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">new</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">data</span><span style="color:#abb2bf;">: Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt;, </span><span style="color:#eb6772;">shape</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">]) -&gt; Result&lt;Tensor,
        TensorError&gt; {
        </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> data.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">== </span><span style="color:#abb2bf;">Tensor::calc_tensor_len_from_shape(shape)
            </span><span style="color:#adb7c9;">&amp;&amp; !</span><span style="color:#abb2bf;">shape.</span><span style="color:#5ebfcc;">is_empty</span><span style="color:#abb2bf;">()
            </span><span style="color:#adb7c9;">&amp;&amp;</span><span style="color:#abb2bf;"> shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">&lt; </span><span style="color:#db9d63;">3
        </span><span style="color:#abb2bf;">{
            Ok(Tensor {
                data,
                shape: shape.</span><span style="color:#5ebfcc;">to_vec</span><span style="color:#abb2bf;">(),
                strides: Tensor::calc_strides_from_shape(shape),
            })
        } </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
            Err(TensorError::InvalidTensor)
        }
    }
}
</span></code></pre>
<hr />
<p>Let's add the error handling struct <code>TensorError</code> to <code>src/errors.rs</code></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/errors.rs

</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">std::error;
</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">std::fmt;

#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, Clone)]
</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">TensorError {
    MaxDimsError,
    InvalidTensor,
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            TensorError::MaxDimsError </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(
                f,
                </span><span style="color:#9acc76;">&quot;L2 currently only supports
                tensors with up to 2 dimensions&quot;
            </span><span style="color:#abb2bf;">),
            TensorError::InvalidTensor </span><span style="color:#adb7c9;">=&gt;
            </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Invalid parameters for Tensor&quot;</span><span style="color:#abb2bf;">),
        }
    }
}

</span><span style="font-style:italic;color:#5f697a;">// This is important for other errors to wrap this one.
</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">error::Error </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">source</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">) -&gt; Option&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">(dyn error::Error + </span><span style="color:#cd74e8;">&#39;static</span><span style="color:#abb2bf;">)&gt; {
        </span><span style="font-style:italic;color:#5f697a;">// Generic error, underlying cause isn&#39;t tracked.
        </span><span style="color:#abb2bf;">None
    }
}
</span></code></pre>
<hr />
<p>Add the relevant imports to <code>src/lib.rs</code> and <code>src/bin/main.rs</code></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/lib.rs

</span><span style="color:#cd74e8;">pub mod </span><span style="color:#abb2bf;">errors;
</span><span style="color:#cd74e8;">pub mod </span><span style="color:#abb2bf;">tensor;
</span></code></pre>
<hr />
<p>And finally, lets test out our library by creating a simple <code>Tensor</code> in <code>src/bin/main.rs</code></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/bin/main.rs

</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">l2::errors::</span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">;
</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">l2::tensor::</span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">;

</span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">main</span><span style="color:#abb2bf;">() -&gt; Result&lt;(), TensorError&gt; {
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> x </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">])</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

    println!(</span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, x);

    Ok(())
}
</span></code></pre>
<hr />
<p>and run <code>cargo run</code> to see the output.</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro</span><span style="color:#abb2bf;"> l2 % cargo run
   </span><span style="color:#eb6772;">Compiling</span><span style="color:#abb2bf;"> l2 v0.1.0 (/Users/bilal/Desktop/l2)
    </span><span style="color:#eb6772;">Finished</span><span style="color:#abb2bf;"> dev </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">unoptimized + debuginfo</span><span style="color:#cd74e8;">]</span><span style="color:#abb2bf;"> target(s) </span><span style="color:#eb6772;">in</span><span style="color:#abb2bf;"> 1.02s
     </span><span style="color:#eb6772;">Running </span><span style="color:#abb2bf;">`</span><span style="color:#eb6772;">target/debug/main</span><span style="color:#abb2bf;">`

</span><span style="color:#eb6772;">Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 2.0, 3.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">3</span><span style="color:#cd74e8;">]
</span></code></pre>
<hr />
<p>ðŸŽ‰! you now have a very simple machine learning library. Now that we have the general structure of the library set up, I'll be speeding up the pace of this blog post.</p>
<hr />
<h1 id="broadcasting"><a name='broadcasting' href='#broadcasting'>Broadcasting</a><a class="zola-anchor" href="#broadcasting" aria-label="Anchor link for: broadcasting">Â§</a>
</h1>
<hr />
<p>Storing a bunch of values in a <code>Tensor</code> is useless if we can't operate over them.</p>
<p>Before we can create some <code>Tensor</code>â€”<code>Tensor</code> operations, we need to implement <em>broadcasting</em>. I won't go into what exactly broadcasting is here, since there are a lot of better explanations out there. Numpy's <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html#general-broadcasting-rules">documentation</a> on their broadcasting rules is a good technical explanation.</p>
<p>One thing the numpy docs don't go into is how to implement broadcasting. I struggled with how to best implement it when I was making my original C++ version of the library last year <sup class="footnote-reference"><a href="#5">5</a></sup>, but I eventually settled on the pretty simple and efficient solution of adding dimensions of size $1$ to the tensor with the fewer number of dimensions to make their shapes broadcastable, then setting the shapes and strides of a broadcasted dimension to $1$ and $0$ respectively. By doing it this way, the <code>Tensor</code> would use the same value across all values of a specific dimension.</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Tensor {

    </span><span style="color:#adb7c9;">...

    </span><span style="color:#abb2bf;">#[</span><span style="color:#eb6772;">allow</span><span style="color:#abb2bf;">(clippy::ptr_arg, clippy::type_complexity)]
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">broadcast</span><span style="color:#abb2bf;">(
        </span><span style="color:#eb6772;">lhs_shape</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt;,
        </span><span style="color:#eb6772;">rhs_shape</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt;,
    ) -&gt; Result&lt;(Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt;, Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt;, Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt;), TensorError&gt; {

        </span><span style="font-style:italic;color:#5f697a;">// prepend lhs_shape with ones if the length of it is smaller than rhs_shape
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> lhs_shape </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> lhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">&lt;</span><span style="color:#abb2bf;"> rhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() {
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> ones </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">vec![</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">; rhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">-</span><span style="color:#abb2bf;"> lhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">()];
            [</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">ones[</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">lhs_shape[</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">]].</span><span style="color:#5ebfcc;">concat</span><span style="color:#abb2bf;">()
        } </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
            lhs_shape.</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">()
        };

        </span><span style="font-style:italic;color:#5f697a;">// prepend rhs_shape with ones if the length of it is smaller than lhs_shape
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> rhs_shape </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> rhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">&lt;</span><span style="color:#abb2bf;"> lhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() {
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> ones </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">vec![</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">; lhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">-</span><span style="color:#abb2bf;"> rhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">()];
            [</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">ones[</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">rhs_shape[</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">]].</span><span style="color:#5ebfcc;">concat</span><span style="color:#abb2bf;">()
        } </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
            rhs_shape.</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">()
        };

        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> broadcasted_shape: Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=
            </span><span style="color:#abb2bf;">Vec::with_capacity(lhs_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">());
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> broadcasted_lhs_strides: Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=
            </span><span style="color:#abb2bf;">Tensor::calc_strides_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">lhs_shape);
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> broadcasted_rhs_strides: Vec&lt;</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=
            </span><span style="color:#abb2bf;">Tensor::calc_strides_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">rhs_shape);

        </span><span style="font-style:italic;color:#5f697a;">// go over each dimension of lhs and rhs
        </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">(i, (</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">lhs, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">rhs)) </span><span style="color:#adb7c9;">in</span><span style="color:#abb2bf;"> lhs_shape.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">()
            .</span><span style="color:#5ebfcc;">zip</span><span style="color:#abb2bf;">(rhs_shape.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">()).</span><span style="color:#5ebfcc;">enumerate</span><span style="color:#abb2bf;">() {
            </span><span style="font-style:italic;color:#5f697a;">// if both dimensions are the same,
            // the dimension of the broadcasted shape
            // for this dimension doesn&#39;t change
            </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> lhs </span><span style="color:#adb7c9;">==</span><span style="color:#abb2bf;"> rhs {
                broadcasted_shape.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(lhs);

            </span><span style="font-style:italic;color:#5f697a;">// if the size of this dimension of lhs
            // is 1, set the strides of lhs for that
            // dimension to 0
            </span><span style="color:#abb2bf;">} </span><span style="color:#cd74e8;">else if</span><span style="color:#abb2bf;"> lhs </span><span style="color:#adb7c9;">== </span><span style="color:#db9d63;">1 </span><span style="color:#abb2bf;">{
                broadcasted_shape.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(rhs);
                broadcasted_lhs_strides[i] </span><span style="color:#adb7c9;">= </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">;

            </span><span style="font-style:italic;color:#5f697a;">// if the size of this dimension of rhs
            // is 1, set the strides of rhs for
            // that dimension to 0
            </span><span style="color:#abb2bf;">} </span><span style="color:#cd74e8;">else if</span><span style="color:#abb2bf;"> rhs </span><span style="color:#adb7c9;">== </span><span style="color:#db9d63;">1 </span><span style="color:#abb2bf;">{
                broadcasted_shape.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(lhs);
                broadcasted_rhs_strides[i] </span><span style="color:#adb7c9;">= </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">;

            </span><span style="font-style:italic;color:#5f697a;">// return an error if the tensors
            // aren&#39;t broadcastable
            </span><span style="color:#abb2bf;">} </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
                </span><span style="color:#cd74e8;">return </span><span style="color:#abb2bf;">Err(TensorError::BroadcastError);
            }
        }

        Ok((
            broadcasted_shape,
            broadcasted_lhs_strides,
            broadcasted_rhs_strides,
        ))
    }
</span></code></pre><pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/errors.rs

</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#adb7c9;">...</span><span style="color:#abb2bf;">
    BroadcastError,
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            </span><span style="color:#adb7c9;">...
            </span><span style="color:#abb2bf;">TensorError::BroadcastError </span><span style="color:#adb7c9;">=&gt;
                </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Shapes are not broadcastable&quot;</span><span style="color:#abb2bf;">),
        }
    }
}
</span></code></pre>
<hr />
<p>Now that we've implemented broadcasting, we'll add some operations over <code>Tensor</code>s in the next section so we can try it out.</p>
<hr />
<h1 id="ops"><a name='ops' href='#ops'>Ops</a><a class="zola-anchor" href="#ops" aria-label="Anchor link for: ops">Â§</a>
</h1>
<hr />
<p>Let's start by defining a struct <code>Ops</code> that we'll use to keep track of what operation should be performed on a tensor.</p>
<p>We'll be storing the <code>Tensor</code>â€”<code>Tensor</code> ops in an enum called <code>TensorOp</code>, but we'll wrap that in the <code>Ops</code> enum so we can add more different kinds of ops in the future (slicing, matmuls, transposes, etc).</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/ops.rs

</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">std::fmt;

#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, PartialEq)]
</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">TensorOp {
    Add,
    Sub,
    Mul,
    Div,
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">TensorOp {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter&lt;&#39;</span><span style="color:#adb7c9;">_</span><span style="color:#abb2bf;">&gt;) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            TensorOp::Add </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Add&quot;</span><span style="color:#abb2bf;">),
            TensorOp::Sub </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Subtract&quot;</span><span style="color:#abb2bf;">),
            TensorOp::Mul </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Multiply&quot;</span><span style="color:#abb2bf;">),
            TensorOp::Div </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Divide&quot;</span><span style="color:#abb2bf;">),
        }
    }
}

#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, PartialEq)]
</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">Ops {
    TensorOp(TensorOp),
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Ops {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter&lt;&#39;</span><span style="color:#adb7c9;">_</span><span style="color:#abb2bf;">&gt;) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            Ops::TensorOp(tensor_op) </span><span style="color:#adb7c9;">=&gt;
                </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, tensor_op),
        }
    }
}
</span></code></pre>
<hr />
<p>And now let's add an <code>OpError</code> variant to our <code>TensorError</code> enum</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/errors.rs

</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#adb7c9;">...</span><span style="color:#abb2bf;">
    OpError,
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            </span><span style="color:#adb7c9;">...
            </span><span style="color:#abb2bf;">TensorError::OpError </span><span style="color:#adb7c9;">=&gt;
                </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Tensors cannot be operated on&quot;</span><span style="color:#abb2bf;">),
        }
    }
}

</span></code></pre>
<hr />
<p>Now that we have an <code>Ops</code> enum that we can use, let's integrate it into <code>Tensor</code></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">std::ops::{Add, Div, Mul, Sub};

</span><span style="color:#cd74e8;">use crate</span><span style="color:#abb2bf;">::ops::{Ops, TensorOp};

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Tensor {

    </span><span style="color:#adb7c9;">...

    </span><span style="font-style:italic;color:#5f697a;">// calculate the physical index of an element
    // from a `Tensor`&#39;s logical indices and strides
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">get_physical_idx</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">logical_indices</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">],
        </span><span style="color:#eb6772;">strides</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">]) -&gt; </span><span style="color:#cd74e8;">usize </span><span style="color:#abb2bf;">{
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> physical_idx </span><span style="color:#adb7c9;">= </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">;

        </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">(i, idx) </span><span style="color:#adb7c9;">in</span><span style="color:#abb2bf;"> logical_indices.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">enumerate</span><span style="color:#abb2bf;">() {
            physical_idx </span><span style="color:#adb7c9;">+=</span><span style="color:#abb2bf;"> idx </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;"> strides[i];
        }

        physical_idx
    }

    </span><span style="font-style:italic;color:#5f697a;">// perform op on lhs and rhs
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">op</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">lhs</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">rhs</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">op</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Ops) -&gt;
        Result&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">, TensorError&gt; {
        </span><span style="color:#cd74e8;">match</span><span style="color:#abb2bf;"> op {
            Ops::TensorOp(TensorOp::Add) </span><span style="color:#adb7c9;">=&gt;
                </span><span style="color:#abb2bf;">Ok(lhs </span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;"> rhs),
            Ops::TensorOp(TensorOp::Sub) </span><span style="color:#adb7c9;">=&gt;
                </span><span style="color:#abb2bf;">Ok(lhs </span><span style="color:#adb7c9;">-</span><span style="color:#abb2bf;"> rhs),
            Ops::TensorOp(TensorOp::Mul) </span><span style="color:#adb7c9;">=&gt;
                </span><span style="color:#abb2bf;">Ok(lhs </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;"> rhs),
            Ops::TensorOp(TensorOp::Div) </span><span style="color:#adb7c9;">=&gt;
                </span><span style="color:#abb2bf;">Ok(lhs </span><span style="color:#adb7c9;">/</span><span style="color:#abb2bf;"> rhs),
            </span><span style="color:#adb7c9;">_ =&gt; </span><span style="color:#abb2bf;">Err(TensorError::OpError),
        }
    }

    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">tensor_op</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor, </span><span style="color:#eb6772;">op</span><span style="color:#abb2bf;">: Ops) -&gt;
        Result&lt;Tensor, TensorError&gt; {
        </span><span style="font-style:italic;color:#5f697a;">// broadcast tensors
        </span><span style="color:#cd74e8;">let </span><span style="color:#abb2bf;">(new_shape, lhs_strides, rhs_strides) </span><span style="color:#adb7c9;">=
            </span><span style="color:#abb2bf;">Tensor::broadcast(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">other.shape)</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

        </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> new_shape.</span><span style="color:#5ebfcc;">is_empty</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">|| </span><span style="color:#abb2bf;">(new_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">&gt; </span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">) {
            </span><span style="color:#cd74e8;">return </span><span style="color:#abb2bf;">Err(TensorError::MaxDimsError);
        }

        </span><span style="font-style:italic;color:#5f697a;">// allocate a new vector for the result of the op
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> new_data: Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=
            </span><span style="color:#abb2bf;">Vec::with_capacity(Tensor::
                </span><span style="color:#5ebfcc;">calc_tensor_len_from_shape</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape));

        </span><span style="font-style:italic;color:#5f697a;">// call `Tensor::op()` on each element in the tensor
        </span><span style="color:#cd74e8;">for</span><span style="color:#abb2bf;"> i </span><span style="color:#adb7c9;">in </span><span style="color:#db9d63;">0</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">new_shape[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">] {
            </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> new_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">== </span><span style="color:#db9d63;">1 </span><span style="color:#abb2bf;">{
                </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> op_result </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::op(
                    </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.data[Tensor::
                        </span><span style="color:#5ebfcc;">get_physical_idx</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[i], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">lhs_strides)],
                    </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">other.data[Tensor::
                        </span><span style="color:#5ebfcc;">get_physical_idx</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[i], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">rhs_strides)],
                    </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">op,
                )</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

                new_data.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(op_result);
            } </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
                </span><span style="color:#cd74e8;">for</span><span style="color:#abb2bf;"> j </span><span style="color:#adb7c9;">in </span><span style="color:#db9d63;">0</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">new_shape[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">] {
                    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> op_result </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::op(
                        </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.data[Tensor::
                            </span><span style="color:#5ebfcc;">get_physical_idx</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[i, j], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">lhs_strides)],
                        </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">other.data[Tensor::
                            </span><span style="color:#5ebfcc;">get_physical_idx</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[i, j], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">rhs_strides)],
                        </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">op,
                    )</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

                    new_data.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(op_result);
                }
            }
        }

        Tensor::new(new_data, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape)
    }
}
</span></code></pre>
<hr />
<p>Let's also overload Rust's built-in <code>Add</code>, <code>Sub</code>, <code>Mul</code>, and <code>Div</code> traits for <code>Tensor</code> so we can use the native plus and minus operators on <code>Tensor</code>s: <code>let c: Tensor = a + b;</code></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#adb7c9;">...

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Add </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;

    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">add</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">tensor_op</span><span style="color:#abb2bf;">(other,
            Ops::TensorOp(TensorOp::Add)) {
            Ok(t) </span><span style="color:#adb7c9;">=&gt;</span><span style="color:#abb2bf;"> t,
            Err(e) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">panic!(</span><span style="color:#9acc76;">&quot;{}&quot;</span><span style="color:#abb2bf;">, e),
        }
    }
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Sub </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;

    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">sub</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">tensor_op</span><span style="color:#abb2bf;">(other,
            Ops::TensorOp(TensorOp::Sub)) {
            Ok(t) </span><span style="color:#adb7c9;">=&gt;</span><span style="color:#abb2bf;"> t,
            Err(e) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">panic!(</span><span style="color:#9acc76;">&quot;{}&quot;</span><span style="color:#abb2bf;">, e),
        }
    }
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Mul </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;

    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">mul</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">tensor_op</span><span style="color:#abb2bf;">(other,
            Ops::TensorOp(TensorOp::Mul)) {
            Ok(t) </span><span style="color:#adb7c9;">=&gt;</span><span style="color:#abb2bf;"> t,
            Err(e) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">panic!(</span><span style="color:#9acc76;">&quot;{}&quot;</span><span style="color:#abb2bf;">, e),
        }
    }
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Div </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;

    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">div</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">tensor_op</span><span style="color:#abb2bf;">(other,
            Ops::TensorOp(TensorOp::Div)) {
            Ok(t) </span><span style="color:#adb7c9;">=&gt;</span><span style="color:#abb2bf;"> t,
            Err(e) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">panic!(</span><span style="color:#9acc76;">&quot;{}&quot;</span><span style="color:#abb2bf;">, e),
        }
    }
}
</span></code></pre>
<hr />
<p>Now that we have the ops implemented, all we need to do now is to add <code>ops.rs</code> as a module in <code>lib.rs</code></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/lib.rs

</span><span style="color:#cd74e8;">mod </span><span style="color:#abb2bf;">ops;
</span></code></pre>
<hr />
<p>and let's try it out:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/bin/main.rs

</span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">main</span><span style="color:#abb2bf;">() -&gt; Result&lt;(), TensorError&gt; {
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> a </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">])</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> b </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">])</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">a </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">b;

    println!(</span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, c);

    </span><span style="color:#adb7c9;">...
</span></code></pre>
<hr />
<p>Just run <code>cargo run</code> in your terminal to see the results:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro</span><span style="color:#abb2bf;"> l2 % cargo run
    </span><span style="color:#eb6772;">Finished</span><span style="color:#abb2bf;"> dev </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">unoptimized + debuginfo</span><span style="color:#cd74e8;">]</span><span style="color:#abb2bf;"> target(s) </span><span style="color:#eb6772;">in</span><span style="color:#abb2bf;"> 0.75s
    </span><span style="color:#eb6772;">Running </span><span style="color:#abb2bf;">`</span><span style="color:#eb6772;">target/debug/main</span><span style="color:#abb2bf;">`
</span><span style="color:#eb6772;">Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 4.0, 9.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1, 3</span><span style="color:#cd74e8;">]
</span></code></pre>
<hr />
<h1 id="autograd"><a name='autograd' href='#autograd'>Autograd</a><a class="zola-anchor" href="#autograd" aria-label="Anchor link for: autograd">Â§</a>
</h1>
<hr />
<p>We need to implement one more operator before we can start working on our autograd system: <code>.pow()</code></p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/ops.rs
</span><span style="color:#abb2bf;">#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, PartialEq)]
</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">Ops {
    </span><span style="color:#adb7c9;">...</span><span style="color:#abb2bf;">
    Pow(</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">),
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Ops {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter&lt;&#39;</span><span style="color:#adb7c9;">_</span><span style="color:#abb2bf;">&gt;) -&gt;
        fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            </span><span style="color:#adb7c9;">...
            </span><span style="color:#abb2bf;">Ops::Pow(pow) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Pow: </span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, pow),
        }
    }
}
</span></code></pre><pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Tensor {

    </span><span style="color:#adb7c9;">...

    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">pow</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">exp</span><span style="color:#abb2bf;">: </span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">) -&gt;
        Result&lt;Tensor, TensorError&gt; {
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> new_data </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.data.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">()
            .</span><span style="color:#5ebfcc;">map</span><span style="color:#abb2bf;">(|</span><span style="color:#eb6772;">val</span><span style="color:#abb2bf;">| val.</span><span style="color:#5ebfcc;">powf</span><span style="color:#abb2bf;">(exp)).</span><span style="color:#5ebfcc;">collect</span><span style="color:#abb2bf;">();

        Tensor::new(new_data, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape)
    }
}
</span></code></pre>
<hr />
<p>Now that that's out of the way, lets go on to the fun stuff: Autograd. We'll be implementing a simple but efficient graph based autograd system similar to what PyTorch uses.</p>
<p>Every <code>Tensor</code> struct will hold field(s) that hold references to its parent(s) as well as a field holding the op that was used to create it and a <code>Vec&lt;f32&gt;</code> to store its gradient.</p>
<p>Since we're using Rust, a language famous for its focus on guaranteeing memory safety at compile time, we'll need to put a little bit of thought into how to implement all this. A <code>Tensor</code> may or may not have either one or two immutable references to its parent <code>Tensors</code> and also may or may not have been created using an <code>Op</code>. We also need a way to compute a <code>Tensor</code>'s gradient wrt to its children.</p>
<p>To make everything simple, we'll wrap the gradient of a <code>Tensor</code> in a <code>RefCell</code> so we can safely change its value by calling <code>.borrow_mut()</code> without needing to keep a mutable reference to it. <em>Keeping a mutable reference might not be possible if one <code>Tensor</code> has two distinct children â€” Rust only allows one mutable reference to be in scope at a time.</em></p>
<p>Let's get started by adding a few field to our original <code>Tensor</code> struct:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">use </span><span style="color:#abb2bf;">std::cell::RefCell;

</span><span style="color:#cd74e8;">pub struct </span><span style="color:#abb2bf;">Tensor {
    ...

    </span><span style="color:#eb6772;">track_grad</span><span style="color:#abb2bf;">: </span><span style="color:#cd74e8;">bool</span><span style="color:#abb2bf;">,

    </span><span style="color:#eb6772;">lhs_parent</span><span style="color:#abb2bf;">: Option&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor&gt;,
    </span><span style="color:#eb6772;">rhs_parent</span><span style="color:#abb2bf;">: Option&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor&gt;,
    </span><span style="color:#eb6772;">create_op</span><span style="color:#abb2bf;">: Option&lt;Ops&gt;,
    </span><span style="color:#eb6772;">derivative</span><span style="color:#abb2bf;">: RefCell&lt;Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt;&gt;,
}
</span></code></pre>
<hr />
<p>If you add this and press <code>âŒ˜-S</code>, you'll probably see that rust-analyzer starts throwing out dozens of warnings and errors. Now that we're storing references to other <code>Tensor</code>s inside our <code>Tensor</code>, we need to add lifetime parameters to our struct so the Rust compiler can make sure that these references don't go out of scope during any part of our program.</p>
<p>If you're using VSCode with rust-analyzer like I am, fixing lifetime errors in Rust is pretty painless when the compiler literally guides you through it and tells you where the problem is, why it exists, and how to fix it :)</p>
<p>Here's a diff showing the changes that I had to make:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#adb7c9;">-</span><span style="color:#cd74e8;">pub struct </span><span style="color:#abb2bf;">Tensor {
+</span><span style="color:#cd74e8;">pub</span><span style="color:#abb2bf;"> struct Tensor&lt;&#39;a&gt; {
    ...

-    </span><span style="color:#eb6772;">lhs_parent</span><span style="color:#abb2bf;">: Option&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor,
</span><span style="background-color:#e05252;color:#ffffff;">-</span><span style="color:#abb2bf;">    rhs_parent: Option&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">    lhs_parent: Option&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;&gt;,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">    rhs_parent: Option&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;&gt;,
</span><span style="background-color:#e05252;color:#ffffff;">}</span><span style="color:#abb2bf;">

-impl fmt::Display for Tensor {
+impl&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; fmt::Display for Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    ...
}

</span><span style="color:#adb7c9;">-</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Clone </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Tensor {
</span><span style="color:#adb7c9;">+</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Clone </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#adb7c9;">...
</span><span style="color:#abb2bf;">}

</span><span style="color:#adb7c9;">-</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Tensor {
</span><span style="color:#adb7c9;">+</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#adb7c9;">...

-    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">new</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">data</span><span style="color:#abb2bf;">: Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt;, </span><span style="color:#eb6772;">shape</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">])
</span><span style="color:#adb7c9;">-        </span><span style="color:#abb2bf;">-&gt; Result&lt;Tensor, TensorError&gt; {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">new</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;b</span><span style="color:#abb2bf;">&gt;(</span><span style="color:#eb6772;">data</span><span style="color:#abb2bf;">: Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt;, </span><span style="color:#eb6772;">shape</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">])
</span><span style="color:#adb7c9;">+        </span><span style="color:#abb2bf;">-&gt; Result&lt;Tensor&lt;</span><span style="color:#cd74e8;">&#39;b</span><span style="color:#abb2bf;">&gt;, TensorError&gt; {
        </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> data.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">== </span><span style="color:#abb2bf;">Tensor::calc_tensor_len_from_shape(shape)
            </span><span style="color:#adb7c9;">&amp;&amp; !</span><span style="color:#abb2bf;">shape.</span><span style="color:#5ebfcc;">is_empty</span><span style="color:#abb2bf;">()
            </span><span style="color:#adb7c9;">&amp;&amp;</span><span style="color:#abb2bf;"> shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">&lt; </span><span style="color:#db9d63;">3
        </span><span style="color:#abb2bf;">{
            Ok(Tensor {
                data,
                shape: shape.</span><span style="color:#5ebfcc;">to_vec</span><span style="color:#abb2bf;">(),
                strides: Tensor::calc_strides_from_shape(shape),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">                track_grad: </span><span style="color:#db9d63;">true</span><span style="color:#abb2bf;">,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">                create_op: None,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">                derivative: RefCell::new(
</span><span style="color:#adb7c9;">+                    </span><span style="color:#abb2bf;">vec![</span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">; Tensor::calc_tensor_len_from_shape(shape)]),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">                lhs_parent: None,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">                rhs_parent: None,
            })
        } </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
            Err(TensorError::InvalidTensor)
        }
    }

}

</span><span style="color:#adb7c9;">-</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Add </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;
</span><span style="color:#adb7c9;">+</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Add </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;;

</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">add</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">add</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;"> Tensor) -&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
        </span><span style="color:#adb7c9;">...
    </span><span style="color:#abb2bf;">}
}

</span><span style="color:#adb7c9;">-</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Sub </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;
</span><span style="color:#adb7c9;">+</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Sub </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;;

</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">sub</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">sub</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;"> Tensor) -&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
        </span><span style="color:#adb7c9;">...
    </span><span style="color:#abb2bf;">}
}

</span><span style="color:#adb7c9;">-</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Mul </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;
</span><span style="color:#adb7c9;">+</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Mul </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;;

</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">mul</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">mul</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;"> Tensor) -&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
        </span><span style="color:#adb7c9;">...
    </span><span style="color:#abb2bf;">}
}

</span><span style="color:#adb7c9;">-</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">Div </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor {
</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> Tensor;
</span><span style="color:#adb7c9;">+</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Div </span><span style="color:#cd74e8;">for </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">type </span><span style="color:#abb2bf;">Output </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;;

</span><span style="color:#adb7c9;">-    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">div</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor) -&gt; Tensor {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">div</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;"> Tensor) -&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
        </span><span style="color:#adb7c9;">...
    </span><span style="color:#abb2bf;">}
}
</span></code></pre>
<hr />
<p><em>Note: you might notice that you don't need to declare a lifetime parameter on <code>other</code> in the <code>impl</code> blocks for <code>Add</code>, <code>Sub</code>, <code>Mul</code>, and <code>Div</code>. I'm including the lifetime parameters here since we'll need to add them in the next step since the output of <code>Tensor::tensorop()</code> will store a reference to <code>other</code> as one of its parents. This means that lifetime parameters will be needed to make sure that the reference to the parent remains valid for the full lifetime of the output.</em></p>
<p>Now that we've satisfied the Rust compiler, let's modify <code>Tensor::tensor_op()</code> and <code>Tensor::pow()</code> to use the new struct fields we just added.</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#adb7c9;">...

-    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">tensor_op</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor, </span><span style="color:#eb6772;">op</span><span style="color:#abb2bf;">: Ops)
</span><span style="color:#adb7c9;">-       </span><span style="color:#abb2bf;">-&gt; Result&lt;Tensor, TensorError&gt; {
</span><span style="color:#adb7c9;">+    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">tensor_op</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">other</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;"> Tensor, </span><span style="color:#eb6772;">op</span><span style="color:#abb2bf;">: Ops)
</span><span style="color:#adb7c9;">+        </span><span style="color:#abb2bf;">-&gt; Result&lt;Tensor, TensorError&gt; {

        </span><span style="color:#adb7c9;">...

-       </span><span style="color:#abb2bf;">Tensor::new(new_data, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape)
</span><span style="color:#adb7c9;">+        </span><span style="color:#abb2bf;">Ok(Tensor {
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            data: new_data,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            shape: new_shape.</span><span style="color:#5ebfcc;">to_vec</span><span style="color:#abb2bf;">(),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            strides: Tensor::
</span><span style="color:#adb7c9;">+                </span><span style="color:#5ebfcc;">calc_strides_from_shape</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            track_grad: </span><span style="color:#db9d63;">true</span><span style="color:#abb2bf;">,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            create_op: Some(op),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            derivative: RefCell::new(
</span><span style="color:#adb7c9;">+                </span><span style="color:#abb2bf;">vec![</span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">; Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape)]),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            lhs_parent: Some(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            rhs_parent: Some(other),
</span><span style="color:#adb7c9;">+        </span><span style="color:#abb2bf;">})

    }

    </span><span style="color:#adb7c9;">...

    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">pow</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">exp</span><span style="color:#abb2bf;">: </span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">) -&gt; Result&lt;Tensor, TensorError&gt; {

        </span><span style="color:#adb7c9;">...

-       </span><span style="color:#abb2bf;">Tensor::new(new_data, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape)
</span><span style="color:#adb7c9;">+        </span><span style="color:#abb2bf;">Ok(Tensor {
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            data: new_data,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            shape: </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape.</span><span style="color:#5ebfcc;">to_vec</span><span style="color:#abb2bf;">(),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            strides: Tensor::calc_strides_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            track_grad: </span><span style="color:#db9d63;">true</span><span style="color:#abb2bf;">,
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            create_op: Some(Ops::Pow(exp)),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            derivative: RefCell::new(
</span><span style="color:#adb7c9;">+                </span><span style="color:#abb2bf;">vec![</span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">; Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape)]),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            lhs_parent: Some(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">),
</span><span style="color:#adb7c9;">+</span><span style="color:#abb2bf;">            rhs_parent: None,
</span><span style="color:#adb7c9;">+        </span><span style="color:#abb2bf;">})
    }
}

</span></code></pre>
<hr />
<p>Ok, we're halfway there! We can now represent a sequence of operations as a computation graph. Let's update our pretty-printing code to print out the structure of our internal representation (IR) of the computation graph.</p>
<p>This probably isn't the most elegant way of implementing this but it works and I'm not motivated enough right now to try and improve it.</p>
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter&lt;&#39;</span><span style="color:#adb7c9;">_</span><span style="color:#abb2bf;">&gt;) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">recurse</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">tensor</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor, </span><span style="color:#eb6772;">level</span><span style="color:#abb2bf;">: </span><span style="color:#cd74e8;">usize</span><span style="color:#abb2bf;">) -&gt; String {
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> indent </span><span style="color:#adb7c9;">= </span><span style="color:#9acc76;">&quot;  &quot;</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">to_string</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">repeat</span><span style="color:#abb2bf;">(level);

            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> lhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match</span><span style="color:#abb2bf;"> tensor.lhs_parent {
                Some(t) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#5ebfcc;">recurse</span><span style="color:#abb2bf;">(t, level </span><span style="color:#adb7c9;">+ </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">),
                None </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#9acc76;">&quot;None&quot;</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">to_string</span><span style="color:#abb2bf;">(),
            };

            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> rhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match</span><span style="color:#abb2bf;"> tensor.rhs_parent {
                Some(t) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#5ebfcc;">recurse</span><span style="color:#abb2bf;">(t, level </span><span style="color:#adb7c9;">+ </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">),
                None </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#9acc76;">&quot;None&quot;</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">to_string</span><span style="color:#abb2bf;">(),
            };

            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> op </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">tensor.create_op {
                Some(t) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">format!(</span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, t),
                None </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#9acc76;">&quot;None&quot;</span><span style="color:#abb2bf;">.</span><span style="color:#5ebfcc;">to_string</span><span style="color:#abb2bf;">(),
            };

            format!(
                </span><span style="color:#9acc76;">&quot;</span><span style="color:#5ebfcc;">\n</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">Value: </span><span style="color:#db9d63;">{:?} </span><span style="color:#5ebfcc;">\n</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">Shape: </span><span style="color:#db9d63;">{:?} </span><span style="color:#5ebfcc;">\n</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">Lhs: </span><span style="color:#db9d63;">{} </span><span style="color:#5ebfcc;">\n</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">Rhs: </span><span style="color:#db9d63;">{} </span><span style="color:#5ebfcc;">\n</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">Op: </span><span style="color:#db9d63;">{} </span><span style="color:#5ebfcc;">\n</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">TrackGrad: </span><span style="color:#db9d63;">{:?} </span><span style="color:#5ebfcc;">\n</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">Derivative: </span><span style="color:#db9d63;">{:?}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">,
                indent,
                tensor.data,
                indent,
                tensor.shape,
                indent,
                lhs,
                indent,
                rhs,
                indent,
                op,
                indent,
                tensor.track_grad,
                indent,
                </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">(tensor.derivative.</span><span style="color:#5ebfcc;">borrow</span><span style="color:#abb2bf;">())
            )
        }

        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> graph </span><span style="color:#adb7c9;">= </span><span style="color:#5ebfcc;">recurse</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">);

        write!(f, </span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, graph)
    }
}
</span></code></pre><pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/bin/main.rs

</span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">main</span><span style="color:#abb2bf;">() -&gt; Result&lt;(), TensorError&gt; {
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> a </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">])</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> b </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">])</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">a </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">b;

    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> c.</span><span style="color:#5ebfcc;">pow</span><span style="color:#abb2bf;">(</span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">)</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

    println!(</span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, d);

    Ok(())
}
</span></code></pre>
<hr />
<p>Let's run this and take a look at the resulting IR:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro</span><span style="color:#abb2bf;"> l2 % cargo run
</span><span style="color:#eb6772;">Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 16.0, 81.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1, 3</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Lhs:
  Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 4.0, 9.0</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1, 3</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Lhs:
    Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 2.0, 3.0</span><span style="color:#cd74e8;">]
    </span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1, 3</span><span style="color:#cd74e8;">]
    </span><span style="color:#eb6772;">Lhs:</span><span style="color:#abb2bf;"> None
    </span><span style="color:#eb6772;">Rhs:</span><span style="color:#abb2bf;"> None
    </span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> None
    </span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
    </span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">0.0, 0.0, 0.0</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Rhs:
    Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 2.0, 3.0</span><span style="color:#cd74e8;">]
    </span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">3</span><span style="color:#cd74e8;">]
    </span><span style="color:#eb6772;">Lhs:</span><span style="color:#abb2bf;"> None
    </span><span style="color:#eb6772;">Rhs:</span><span style="color:#abb2bf;"> None
    </span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> None
    </span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
    </span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">0.0, 0.0, 0.0</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> Multiply
  </span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
  </span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">0.0, 0.0, 0.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Rhs:</span><span style="color:#abb2bf;"> None
</span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> Pow: 2
</span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
</span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">0.0, 0.0, 0.0</span><span style="color:#cd74e8;">]
</span></code></pre>
<hr />
<p>Maybe it's not the nicest looking graph, but it works well for when you're trying to visually verify that your gradients are being calculated correctly.</p>
<hr />
<p>Now that we have a computation graph, we need to find a way to backpropogate through it.</p>
<p>The simplest way would be to recursively call a function named <code>backward()</code> on the tensor you want to calculate the gradient with respect to. <code>backward()</code> would first take the gradient of the current tensor (the gradient of the output tensor would be with respect to itself so its gradient is $1$) then use it to calculate (and accumulate, if necessary) the gradient of its parent(s) before calling <code>.backward()</code> on the parent <code>Tensor</code>(s) to recursively calculate the gradient for the entire computation graph.</p>
<p>There are a couple of problems with this:</p>
<p>First, recursively calling <code>.backward()</code> on the entire computation graph would be very memory-inefficient.</p>
<p>Second, if the computation graph has multiple branches (like in a Resnet), the backwards pass over the computation graph will have to be computed multiple times as the gradients for the parent <code>Tensor</code> of each branch in the network are accumulated. Doing it this way would have make computing the backwards pass <em>very</em> slow and inefficient.</p>
<p>Luckily, there is a better way of doing this. If we topologically sort and reverse the graph so that all the <code>Tensor</code>s are ordered in a way so that the gradients for all child <code>Tensor</code>s of a certain <code>Tensor</code> have already been computed and the gradient for the current <code>Tensor</code> has already been accumulated (if necessary), we won't have to re-compute any parts of the graph.</p>
<p>Let's see how we could implement this in Rust:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {

    </span><span style="color:#adb7c9;">...

    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">backward</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">) {
        </span><span style="font-style:italic;color:#5f697a;">// from https://github.com/evcu/numpy_autograd/blob/master/my_autograd.py#L147
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> seen: Vec&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor&gt; </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Vec::new();
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> sorted: Vec&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor&gt; </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Vec::new();

        </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">topological_sort</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;(
            </span><span style="color:#eb6772;">vr</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;"> Tensor,
            </span><span style="color:#eb6772;">seen</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">Vec&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;&gt;,
            </span><span style="color:#eb6772;">sorted</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">Vec&lt;</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt;&gt;,
        ) {
            </span><span style="color:#cd74e8;">if </span><span style="color:#adb7c9;">!</span><span style="color:#abb2bf;">seen.</span><span style="color:#5ebfcc;">contains</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">vr) </span><span style="color:#adb7c9;">&amp;&amp; </span><span style="color:#abb2bf;">(vr.lhs_parent.</span><span style="color:#5ebfcc;">is_some</span><span style="color:#abb2bf;">()
                    </span><span style="color:#adb7c9;">||</span><span style="color:#abb2bf;"> vr.rhs_parent.</span><span style="color:#5ebfcc;">is_some</span><span style="color:#abb2bf;">()) {
                seen.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(vr);

                </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> vr.lhs_parent.</span><span style="color:#5ebfcc;">is_some</span><span style="color:#abb2bf;">() {
                    </span><span style="color:#5ebfcc;">topological_sort</span><span style="color:#abb2bf;">(vr.lhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">(),
                        seen, sorted);
                }
                </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> vr.rhs_parent.</span><span style="color:#5ebfcc;">is_some</span><span style="color:#abb2bf;">() {
                    </span><span style="color:#5ebfcc;">topological_sort</span><span style="color:#abb2bf;">(vr.rhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">(),
                        seen, sorted);
                }

                sorted.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(vr);
            }
        }

        </span><span style="font-style:italic;color:#5f697a;">// Topologically sort the computation graph
        </span><span style="color:#5ebfcc;">topological_sort</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut</span><span style="color:#abb2bf;"> seen, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut</span><span style="color:#abb2bf;"> sorted);

        </span><span style="font-style:italic;color:#5f697a;">// reverse it
</span><span style="color:#abb2bf;">        sorted.</span><span style="color:#5ebfcc;">reverse</span><span style="color:#abb2bf;">();

        </span><span style="font-style:italic;color:#5f697a;">// Set the derivative of the output of the computation
        // graph to itself to equal 1 (usually the derivative
        // of the loss wrt itself)
        </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">sorted[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">].derivative.</span><span style="color:#5ebfcc;">borrow_mut</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">;
            Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">sorted[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">].shape)];

        </span><span style="color:#cd74e8;">for</span><span style="color:#abb2bf;"> t </span><span style="color:#adb7c9;">in</span><span style="color:#abb2bf;"> sorted.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">() {
            t.</span><span style="color:#5ebfcc;">grad</span><span style="color:#abb2bf;">()
        }
    }
}
</span></code></pre>
<hr />
<p>The <code>.grad()</code> function doens't exist yet, but its purpose is to take the gradient of the current <code>Tensor</code> <code>t</code> and use it to compute the gradients of its parent(s). Since we wrapped the <code>derivative</code> field of <code>Tensor</code> in a <code>RefCell()</code>, we can use something like <code>*lhs_parent.borrow_mut() = gradient;</code> to safely mutate the parent's gradient.</p>
<p>Here's how I did it:</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">grad</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">) {
        </span><span style="font-style:italic;color:#5f697a;">// get the gradient of the derivative of self wrt output
        // d_x/d_loss
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.derivative.</span><span style="color:#5ebfcc;">borrow</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">(),
            </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

        </span><span style="font-style:italic;color:#5f697a;">// if lhs_parent exists
        </span><span style="color:#cd74e8;">if let </span><span style="color:#abb2bf;">Some(t) </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.lhs_parent {

            </span><span style="font-style:italic;color:#5f697a;">// calculate the gradient of lhs_parent wrt x
            // d_lhs/d_x
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_lhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                Some(Ops::TensorOp(TensorOp::Add)) </span><span style="color:#adb7c9;">=&gt;
                    </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">;
                        Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape)],
                        </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Sub)) </span><span style="color:#adb7c9;">=&gt;
                    </span><span style="color:#abb2bf;">Tensor::new(
                        vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">;
                        Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape)],
                        </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Mul)) </span><span style="color:#adb7c9;">=&gt;
                    </span><span style="color:#abb2bf;">Ok(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.rhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">()),
                Some(Ops::TensorOp(TensorOp::Div)) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">{
                    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> temp </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.rhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">()
                        .</span><span style="color:#5ebfcc;">pow</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">-</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

                    Tensor::new(temp.data.</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">(), </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">temp.shape)
                }
                </span><span style="color:#adb7c9;">_ =&gt; </span><span style="color:#abb2bf;">Err(TensorError::GradError),
            }
            .</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

            </span><span style="font-style:italic;color:#5f697a;">// calculate the gradient of lhs_parent wrt loss
            // d_lhs/d_loss = d_lhs/d_x * d_x/d_loss
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_lhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                </span><span style="color:#adb7c9;">_ =&gt; &amp;</span><span style="color:#abb2bf;">d_lhs </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">d,
            };

            </span><span style="font-style:italic;color:#5f697a;">// accumulate the gradient of d_lhs/d_loss if necessary
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_lhs_prev </span><span style="color:#adb7c9;">=
                </span><span style="color:#abb2bf;">Tensor::new(t.derivative.</span><span style="color:#5ebfcc;">borrow</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">(), </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">t.shape).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_lhs </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">d_lhs </span><span style="color:#adb7c9;">+ &amp;</span><span style="color:#abb2bf;">d_lhs_prev;

            </span><span style="font-style:italic;color:#5f697a;">// assign to the derivative of the parent
            </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">t.derivative.</span><span style="color:#5ebfcc;">borrow_mut</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> d_lhs.data;
        }

        </span><span style="font-style:italic;color:#5f697a;">// if rhs_parent exists
        </span><span style="color:#cd74e8;">if let </span><span style="color:#abb2bf;">Some(t) </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.rhs_parent {

            </span><span style="font-style:italic;color:#5f697a;">// calculate the gradient of rhs_parent wrt x
            // d_rhs/d_x
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                Some(Ops::TensorOp(TensorOp::Add)) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">Tensor::new(
                    vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">; Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape)],
                    </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Sub)) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">Tensor::new(
                    vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">; Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape)],
                    </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Mul)) </span><span style="color:#adb7c9;">=&gt;
                    </span><span style="color:#abb2bf;">Ok(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.lhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">()),
                Some(Ops::TensorOp(TensorOp::Div)) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">{
                    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> neg1 </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#adb7c9;">-</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">]).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
                    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> t_powed </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> t.</span><span style="color:#5ebfcc;">pow</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">-</span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

                    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> temp </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">neg1 </span><span style="color:#adb7c9;">* </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.lhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
                    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> temp </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">temp </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">t_powed;

                    Tensor::new(temp.data.</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">(), </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">temp.shape)
                }
                </span><span style="color:#adb7c9;">_ =&gt; </span><span style="color:#abb2bf;">Err(TensorError::GradError),
            }
            .</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

            </span><span style="font-style:italic;color:#5f697a;">// calculate the gradient of rhs_parent wrt loss
            // d_rhs/d_loss = d_rhs/d_x * d_x/d_loss
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                </span><span style="color:#adb7c9;">_ =&gt; &amp;</span><span style="color:#abb2bf;">d_rhs </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">d,
            };

            </span><span style="font-style:italic;color:#5f697a;">// accumulate the gradient of d_rhs/d_loss if necessary
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs_prev </span><span style="color:#adb7c9;">=
                </span><span style="color:#abb2bf;">Tensor::new(t.derivative.</span><span style="color:#5ebfcc;">borrow</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">(), </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">t.shape).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">d_rhs </span><span style="color:#adb7c9;">+ &amp;</span><span style="color:#abb2bf;">d_rhs_prev;

            </span><span style="font-style:italic;color:#5f697a;">// assign to the derivative of the parent
            </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">t.derivative.</span><span style="color:#5ebfcc;">borrow_mut</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> d_rhs.data;
        }
    }
}
</span></code></pre>
<hr />
<p>That should be pretty much it. Try it out:</p>
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/bin/main.rs

</span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">main</span><span style="color:#abb2bf;">() -&gt; Result&lt;(), TensorError&gt; {
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> a </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">]).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> b </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">]).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">a </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">b;

    c.</span><span style="color:#5ebfcc;">backward</span><span style="color:#abb2bf;">();

    println!(</span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, c);

    Ok(())
}
</span></code></pre><pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro</span><span style="color:#abb2bf;"> l2 % cargo run
</span><span style="color:#eb6772;">Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">6.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Lhs:
  Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">2.0</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Lhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Rhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
  </span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">3.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Rhs:
  Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">3.0</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Lhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Rhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
  </span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">2.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> Multiply
</span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
</span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0</span><span style="color:#cd74e8;">]
</span></code></pre>
<p>ðŸŽ‰, you now have a semi-complete autograd engine!</p>
<hr />
<h1 id="advanced-ops"><a name='advancedops' href='#advancedops'>Advanced Ops</a><a class="zola-anchor" href="#advanced-ops" aria-label="Anchor link for: advanced-ops">Â§</a>
</h1>
<hr />
<p>Let's add support for fast matrix multiplications with BLAS.</p>
<p><em>todo</em> talk about blas</p>
<p>First up, lets implement the <code>transpose()</code> operator</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/ops.rs

</span><span style="color:#abb2bf;">#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, PartialEq)]
</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">Ops {
    </span><span style="color:#adb7c9;">...</span><span style="color:#abb2bf;">
    Transpose,
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Ops {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter&lt;&#39;</span><span style="color:#adb7c9;">_</span><span style="color:#abb2bf;">&gt;) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            </span><span style="color:#adb7c9;">...
            </span><span style="color:#abb2bf;">Ops::Transpose </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Transpose&quot;</span><span style="color:#abb2bf;">),
        }
    }
}
</span></code></pre><pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs

</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">grad</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">) {
        </span><span style="color:#cd74e8;">if let </span><span style="color:#abb2bf;">Some(t) </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.lhs_parent {
            </span><span style="color:#adb7c9;">...
            </span><span style="color:#abb2bf;">Some(Ops::Transpose) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">]), </span><span style="font-style:italic;color:#5f697a;">// dummy value
        </span><span style="color:#abb2bf;">}
        .</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_lhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
            Some(Ops::Transpose) </span><span style="color:#adb7c9;">=&gt;</span><span style="color:#abb2bf;"> d.</span><span style="color:#5ebfcc;">transpose</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">(),
            </span><span style="color:#adb7c9;">_ =&gt; &amp;</span><span style="color:#abb2bf;">d_lhs </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">d,
        };
    }

    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">transpose</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">) -&gt; Result&lt;Tensor, TensorError&gt; {
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> transposed_shape </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.shape.</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">();
        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> transposed_strides </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.strides.</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">();

        transposed_shape.</span><span style="color:#5ebfcc;">reverse</span><span style="color:#abb2bf;">();
        transposed_strides.</span><span style="color:#5ebfcc;">reverse</span><span style="color:#abb2bf;">();

        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> new_data: Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=
            </span><span style="color:#abb2bf;">Vec::with_capacity(
                Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">transposed_shape));

        </span><span style="color:#cd74e8;">for</span><span style="color:#abb2bf;"> i </span><span style="color:#adb7c9;">in </span><span style="color:#db9d63;">0</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">transposed_shape[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">] {
            </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> transposed_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">== </span><span style="color:#db9d63;">1 </span><span style="color:#abb2bf;">{
                new_data.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.data[Tensor::
                    </span><span style="color:#5ebfcc;">get_physical_idx</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[i], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">transposed_strides)]
                );
            } </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
                </span><span style="color:#cd74e8;">for</span><span style="color:#abb2bf;"> j </span><span style="color:#adb7c9;">in </span><span style="color:#db9d63;">0</span><span style="color:#adb7c9;">..</span><span style="color:#abb2bf;">transposed_shape[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">] {
                    new_data.</span><span style="color:#5ebfcc;">push</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.data[Tensor::
                        </span><span style="color:#5ebfcc;">get_physical_idx</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[i, j], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">transposed_strides)]);
                }
            }
        }

        Ok(Tensor {
            data: new_data,
            shape: transposed_shape.</span><span style="color:#5ebfcc;">to_vec</span><span style="color:#abb2bf;">(),
            strides: Tensor::calc_strides_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">transposed_shape),
            track_grad: </span><span style="color:#db9d63;">true</span><span style="color:#abb2bf;">,
            create_op: Some(Ops::Transpose),
            derivative: RefCell::new(vec![
                </span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">;
                Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">transposed_shape)
            ]),
            lhs_parent: Some(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">),
            rhs_parent: None,
        })
    }
}
</span></code></pre>
<hr />
<p>Now that we have this, let's add matmul support.</p>
<p>First up, let's add a BLAS crate to <code>Cargo.toml</code>. Note that I'm using Apple's accelerate as the BLAS library backend since its already installed on my Macbook pro, but you can change it to use <a href="https://crates.io/crates/blas-src">another</a> BLAS library if you want.</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;">[dependencies]
</span><span style="color:#eb6772;">blas </span><span style="color:#abb2bf;">= </span><span style="color:#9acc76;">&quot;0.20.0&quot;
</span><span style="color:#eb6772;">blas-src </span><span style="color:#abb2bf;">= { </span><span style="color:#eb6772;">version </span><span style="color:#abb2bf;">= </span><span style="color:#9acc76;">&quot;0.6&quot;</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">features </span><span style="color:#abb2bf;">= [</span><span style="color:#9acc76;">&quot;accelerate&quot;</span><span style="color:#abb2bf;">] }
</span></code></pre>
<hr />
<p>Let's add some Ops and errors for matmul</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/ops.rs

</span><span style="color:#abb2bf;">#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, PartialEq)]
</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">Ops {
    </span><span style="color:#adb7c9;">...</span><span style="color:#abb2bf;">
    Matmul,
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">Ops {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter&lt;&#39;</span><span style="color:#adb7c9;">_</span><span style="color:#abb2bf;">&gt;) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            </span><span style="color:#adb7c9;">...
            </span><span style="color:#abb2bf;">Ops::Matmul </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(f, </span><span style="color:#9acc76;">&quot;Matmul&quot;</span><span style="color:#abb2bf;">),
        }
    }
}
</span></code></pre><pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/errors.rs
</span><span style="color:#abb2bf;">#[</span><span style="color:#eb6772;">derive</span><span style="color:#abb2bf;">(Debug, Clone)]
</span><span style="color:#cd74e8;">pub enum </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#adb7c9;">...</span><span style="color:#abb2bf;">
    MatmulShapeError,
}

</span><span style="color:#cd74e8;">impl </span><span style="color:#abb2bf;">fmt::Display </span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">TensorError {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">fmt</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">f</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">fmt::Formatter) -&gt; fmt::Result {
        </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self </span><span style="color:#abb2bf;">{
            </span><span style="color:#adb7c9;">...
            </span><span style="color:#abb2bf;">TensorError::MatmulShapeError </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#abb2bf;">write!(
                f,
                </span><span style="color:#9acc76;">&quot;Tensors must be two dimensions each and must be matrix multipliable&quot;
            </span><span style="color:#abb2bf;">),
        }
    }
}

</span></code></pre>
<hr />
<p>Let's add the matrix multiplication code</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    #[</span><span style="color:#eb6772;">allow</span><span style="color:#abb2bf;">(clippy::many_single_char_names)]
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">two_dimension_matmul</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">lhs</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor, </span><span style="color:#eb6772;">rhs</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">Tensor, </span><span style="color:#eb6772;">out</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut </span><span style="color:#abb2bf;">Vec&lt;</span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">&gt;) {
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> lhs </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> lhs.</span><span style="color:#5ebfcc;">transpose</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> rhs </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> rhs.</span><span style="color:#5ebfcc;">transpose</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> a: Vec&lt;</span><span style="color:#cd74e8;">f64</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> lhs.data.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">map</span><span style="color:#abb2bf;">(|</span><span style="color:#eb6772;">val</span><span style="color:#abb2bf;">| </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">val </span><span style="color:#adb7c9;">as </span><span style="color:#cd74e8;">f64</span><span style="color:#abb2bf;">).</span><span style="color:#5ebfcc;">collect</span><span style="color:#abb2bf;">();
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> b: Vec&lt;</span><span style="color:#cd74e8;">f64</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> rhs.data.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">map</span><span style="color:#abb2bf;">(|</span><span style="color:#eb6772;">val</span><span style="color:#abb2bf;">| </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">val </span><span style="color:#adb7c9;">as </span><span style="color:#cd74e8;">f64</span><span style="color:#abb2bf;">).</span><span style="color:#5ebfcc;">collect</span><span style="color:#abb2bf;">();

        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> c: Vec&lt;</span><span style="color:#cd74e8;">f64</span><span style="color:#abb2bf;">&gt; </span><span style="color:#adb7c9;">=
            </span><span style="color:#abb2bf;">vec![</span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">; Tensor::calc_tensor_len_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[lhs.shape[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">],
                rhs.shape[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">]])];

        </span><span style="color:#cd74e8;">let </span><span style="color:#abb2bf;">(m, n, k) </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">(
            lhs.shape[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">] </span><span style="color:#adb7c9;">as </span><span style="color:#cd74e8;">i32</span><span style="color:#abb2bf;">,
            rhs.shape[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">] </span><span style="color:#adb7c9;">as </span><span style="color:#cd74e8;">i32</span><span style="color:#abb2bf;">,
            lhs.shape[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">] </span><span style="color:#adb7c9;">as </span><span style="color:#cd74e8;">i32</span><span style="color:#abb2bf;">,
        );

        </span><span style="color:#cd74e8;">unsafe </span><span style="color:#abb2bf;">{
            </span><span style="color:#5ebfcc;">dgemm</span><span style="color:#abb2bf;">(</span><span style="color:#cd74e8;">b</span><span style="color:#9acc76;">&#39;N&#39;</span><span style="color:#abb2bf;">, </span><span style="color:#cd74e8;">b</span><span style="color:#9acc76;">&#39;N&#39;</span><span style="color:#abb2bf;">, m, n, k, </span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">a, m, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">b, k, </span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut</span><span style="color:#abb2bf;"> c, m);
        }

        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> c.</span><span style="color:#5ebfcc;">iter</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">map</span><span style="color:#abb2bf;">(|</span><span style="color:#eb6772;">val</span><span style="color:#abb2bf;">| </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">val </span><span style="color:#adb7c9;">as </span><span style="color:#cd74e8;">f32</span><span style="color:#abb2bf;">).</span><span style="color:#5ebfcc;">collect</span><span style="color:#abb2bf;">();
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(c, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[rhs.shape[</span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">], lhs.shape[</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">]]).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> c.</span><span style="color:#5ebfcc;">transpose</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> c.data;

        out.</span><span style="color:#5ebfcc;">append</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut</span><span style="color:#abb2bf;"> c);
    }

    </span><span style="color:#cd74e8;">pub fn </span><span style="color:#5cb3fa;">matmul</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#eb6772;">rhs</span><span style="color:#abb2bf;">: </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;"> Tensor) -&gt; Result&lt;Tensor, TensorError&gt; {
        </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> new_shape </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::validate_tensors(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">rhs)</span><span style="color:#adb7c9;">?</span><span style="color:#abb2bf;">;

        </span><span style="color:#cd74e8;">if </span><span style="color:#abb2bf;">(new_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">&lt;= </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">) </span><span style="color:#adb7c9;">|| </span><span style="color:#abb2bf;">(new_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">&gt; </span><span style="color:#db9d63;">2</span><span style="color:#abb2bf;">) {
            </span><span style="color:#cd74e8;">return </span><span style="color:#abb2bf;">Err(TensorError::MaxDimsError);
        }

        </span><span style="color:#cd74e8;">let mut</span><span style="color:#abb2bf;"> new_data </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Vec::with_capacity(Tensor::
            </span><span style="color:#5ebfcc;">calc_tensor_len_from_shape</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape));

        </span><span style="color:#cd74e8;">if</span><span style="color:#abb2bf;"> new_shape.</span><span style="color:#5ebfcc;">len</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">== </span><span style="color:#db9d63;">2 </span><span style="color:#abb2bf;">{
            Tensor::two_dimension_matmul(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">, rhs, </span><span style="color:#adb7c9;">&amp;</span><span style="color:#cd74e8;">mut</span><span style="color:#abb2bf;"> new_data)
        } </span><span style="color:#cd74e8;">else </span><span style="color:#abb2bf;">{
            </span><span style="color:#cd74e8;">return </span><span style="color:#abb2bf;">Err(TensorError::MatmulShapeError);
        }

        Ok(Tensor {
            data: new_data,
            shape: new_shape.</span><span style="color:#5ebfcc;">to_vec</span><span style="color:#abb2bf;">(),
            strides: Tensor::calc_strides_from_shape(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape),
            track_grad: </span><span style="color:#db9d63;">true</span><span style="color:#abb2bf;">,
            create_op: Some(Ops::Matmul),
            derivative: RefCell::new(vec![</span><span style="color:#db9d63;">0.0</span><span style="color:#abb2bf;">; Tensor::
                </span><span style="color:#5ebfcc;">calc_tensor_len_from_shape</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">new_shape)]),
            lhs_parent: Some(</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">),
            rhs_parent: Some(rhs),
        })
    }
}
</span></code></pre>
<hr />
<p>Now that we have that, let's add autograd support for matmul</p>
<hr />
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/tensor.rs
</span><span style="color:#cd74e8;">impl</span><span style="color:#abb2bf;">&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; Tensor&lt;</span><span style="color:#cd74e8;">&#39;a</span><span style="color:#abb2bf;">&gt; {
    </span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">grad</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">) {

        </span><span style="color:#cd74e8;">if let </span><span style="color:#abb2bf;">Some(t) </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.lhs_parent {
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_lhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#adb7c9;">&amp;</span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                </span><span style="color:#adb7c9;">...
                </span><span style="color:#abb2bf;">Some(Ops::Matmul) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.rhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">transpose</span><span style="color:#abb2bf;">(),
            }
            .</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_lhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                </span><span style="color:#adb7c9;">...
                </span><span style="color:#abb2bf;">Some(Ops::Matmul) </span><span style="color:#adb7c9;">=&gt;</span><span style="color:#abb2bf;"> d.</span><span style="color:#5ebfcc;">matmul</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">d_lhs).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">(),
                </span><span style="color:#adb7c9;">_ =&gt; &amp;</span><span style="color:#abb2bf;">d_lhs </span><span style="color:#adb7c9;">* &amp;</span><span style="color:#abb2bf;">d,
            };
        }

        </span><span style="color:#cd74e8;">if let </span><span style="color:#abb2bf;">Some(t) </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.rhs_parent {
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                </span><span style="color:#adb7c9;">...
                </span><span style="color:#abb2bf;">Some(Ops::Matmul) </span><span style="color:#adb7c9;">=&gt; </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.lhs_parent.</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">transpose</span><span style="color:#abb2bf;">(),
            }
            .</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs </span><span style="color:#adb7c9;">= </span><span style="color:#cd74e8;">match </span><span style="color:#eb6772;">self</span><span style="color:#abb2bf;">.create_op {
                </span><span style="color:#adb7c9;">...
                </span><span style="color:#abb2bf;">Some(Ops::Matmul) </span><span style="color:#adb7c9;">=&gt;</span><span style="color:#abb2bf;"> d_rhs.</span><span style="color:#5ebfcc;">matmul</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">d).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">(),
            };

            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs_prev </span><span style="color:#adb7c9;">=
                </span><span style="color:#abb2bf;">Tensor::new(t.derivative.</span><span style="color:#5ebfcc;">borrow</span><span style="color:#abb2bf;">().</span><span style="color:#5ebfcc;">clone</span><span style="color:#abb2bf;">(), </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">t.shape).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
            </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> d_rhs </span><span style="color:#adb7c9;">= &amp;</span><span style="color:#abb2bf;">d_rhs </span><span style="color:#adb7c9;">+ &amp;</span><span style="color:#abb2bf;">d_rhs_prev;
            </span><span style="color:#adb7c9;">*</span><span style="color:#abb2bf;">t.derivative.</span><span style="color:#5ebfcc;">borrow_mut</span><span style="color:#abb2bf;">() </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> d_rhs.data;
        }
    }
}
</span></code></pre>
<hr />
<p>Let's try it out:</p>
<pre style="background-color:#2b303b;">
<code><span style="font-style:italic;color:#5f697a;">// src/main.rs
</span><span style="color:#cd74e8;">fn </span><span style="color:#5cb3fa;">main</span><span style="color:#abb2bf;">() -&gt; Result&lt;(), TensorError&gt; {
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> a </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">4.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">5.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">6.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">2</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">]).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();
    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> b </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">Tensor::new(vec![</span><span style="color:#db9d63;">1.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">3.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">4.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">5.0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">6.0</span><span style="color:#abb2bf;">], </span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">3</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">2</span><span style="color:#abb2bf;">]).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

    </span><span style="color:#cd74e8;">let</span><span style="color:#abb2bf;"> c </span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;"> a.</span><span style="color:#5ebfcc;">matmul</span><span style="color:#abb2bf;">(</span><span style="color:#adb7c9;">&amp;</span><span style="color:#abb2bf;">b).</span><span style="color:#5ebfcc;">unwrap</span><span style="color:#abb2bf;">();

    c.</span><span style="color:#5ebfcc;">backward</span><span style="color:#abb2bf;">();

    println!(</span><span style="color:#9acc76;">&quot;</span><span style="color:#db9d63;">{}</span><span style="color:#9acc76;">&quot;</span><span style="color:#abb2bf;">, c);

    Ok(())
}
</span></code></pre><pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">bilal@Bilals-MacBook-Pro</span><span style="color:#abb2bf;"> l2 % cargo run
   </span><span style="color:#eb6772;">Compiling</span><span style="color:#abb2bf;"> l2 v0.1.0 (/Users/bilal/Desktop/l2)
    </span><span style="color:#eb6772;">Finished</span><span style="color:#abb2bf;"> dev </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">unoptimized + debuginfo</span><span style="color:#cd74e8;">]</span><span style="color:#abb2bf;"> target(s) </span><span style="color:#eb6772;">in</span><span style="color:#abb2bf;"> 1.46s
     </span><span style="color:#eb6772;">Running </span><span style="color:#abb2bf;">`</span><span style="color:#eb6772;">target/debug/main</span><span style="color:#abb2bf;">`

</span><span style="color:#eb6772;">Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">22.0, 28.0, 49.0, 64.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">2, 2</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Lhs:
  Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 2.0, 3.0, 4.0, 5.0, 6.0</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">2, 3</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Lhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Rhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
  </span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">3.0, 7.0, 11.0, 3.0, 7.0, 11.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Rhs:
  Value: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 2.0, 3.0, 4.0, 5.0, 6.0</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Shape: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">3, 2</span><span style="color:#cd74e8;">]
  </span><span style="color:#eb6772;">Lhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Rhs:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> None
  </span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
  </span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">5.0, 5.0, 7.0, 7.0, 9.0, 9.0</span><span style="color:#cd74e8;">]
</span><span style="color:#eb6772;">Op:</span><span style="color:#abb2bf;"> Matmul
</span><span style="color:#eb6772;">TrackGrad:</span><span style="color:#abb2bf;"> true
</span><span style="color:#eb6772;">Derivative: </span><span style="color:#cd74e8;">[</span><span style="color:#abb2bf;">1.0, 1.0, 1.0, 1.0</span><span style="color:#cd74e8;">]
</span></code></pre>
<hr />
<p>Well thats pretty much it for the first draft. Ill see about adding more stuff when I redo this whole post.</p>
<hr />
<h1 id="future-work"><a name='future' href='#future'>Future Work</a><a class="zola-anchor" href="#future-work" aria-label="Anchor link for: future-work">Â§</a>
</h1>
<hr />
<ul>
<li>rust arrays vs vec
<ul>
<li>const generics</li>
</ul>
</li>
<li>jax</li>
<li>compiler in rust</li>
</ul>
<hr />
<h1 id="conclusions"><a name='conclusions' href='#conclusions'>Conclusions</a><a class="zola-anchor" href="#conclusions" aria-label="Anchor link for: conclusions">Â§</a>
</h1>
<hr />
<p><em>todo</em></p>
<ul>
<li>benchmarks</li>
<li>subsections</li>
<li>gradient vs derivative</li>
<li>standardize code snippets</li>
<li>move implementing ops to beginning</li>
<li>naive matmul</li>
<li>slicing?</li>
</ul>
<hr />
<h1 id="resources"><a name='resources' href='#resources'>Resources</a><a class="zola-anchor" href="#resources" aria-label="Anchor link for: resources">Â§</a>
</h1>
<hr />
<hr />
<h1 id="conclusions-1"><a name='conclusions' href='#conclusions'>Conclusions</a><a class="zola-anchor" href="#conclusions-1" aria-label="Anchor link for: conclusions-1">Â§</a>
</h1>
<hr />
<hr />
<h1 id="references"><a name='references' href='#references'>References</a><a class="zola-anchor" href="#references" aria-label="Anchor link for: references">Â§</a>
</h1>
<hr />
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>I guess the fact that I like to spend my last free summer working on a side project says a lot about me :p</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>I'm almost certain that there are a few bugs in how I handle backpropogation through broadcasted tensors</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>That's the summer of 2019, for those of you reading this in the near or not so near future :)</p>
</div>
<div class="footnote-definition" id="4"><sup class="footnote-definition-label">4</sup>
<p>https://blog.ezyang.com/2019/05/pytorch-internals/'&gt;http://blog.ezyang.com/2019/05/pytorch-internals</p>
</div>
<div class="footnote-definition" id="5"><sup class="footnote-definition-label">5</sup>
<p>In my defense, I was pretty bad at algorithmy stuff back then</p>
</div>

        </div>
        
    
</div></div>
			
    <div class="pagination">
        <div class="pagination__buttons">
            </div>
    </div>
<footer class="footer">
				<div class="footer__inner"><div class="copyright">
            <span>Â© 2020 <a href="https://github.com/ejmg/zerm">zerm</a> :: Powered by <a href="https://www.getzola.org/">Zola</a></span>
            <span>:: Theme made by <a href="https://github.com/ejmg">ejmg</a></span>
        </div>
    <script type="text/javascript" src="https:&#x2F;&#x2F;bilal2vec.github.io&#x2F;blog&#x2F;assets&#x2F;js&#x2F;main.js"></script>
</div>
				

			</footer></div>
	</body>
</html>
