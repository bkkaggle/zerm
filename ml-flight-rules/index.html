<!DOCTYPE html>
<html lang="en">
	<head>
    <title>Machine Learning Flight Rules - Bilal Khan</title>
    
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="My Blog"/>

    <meta property="og:title" content="
    Bilal&#x27;s Blog -&nbsp;Machine Learning Flight Rules" />
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;ml-flight-rules&#x2F;"/>
    <meta property="og:description" content="A guide for astronauts (now, people doing machine learning) about what to do when things go wrong."/>
    <link rel="stylesheet" href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;style.css">
    <link rel="stylesheet" href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;color&#x2F;red.css">
<link rel="alternate" type="application/rss+xml" title="Bilal&#x27;s Blog RSS" href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;rss.xml"><script
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
			async
		></script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
	</head>
	<body>
		<div class="container">
<header class="header">
    <div class="header__inner">
        <div class="header__logo">
            <a href="&#x2F;">
    <div class="logo">
        Bilal&#x27;s Blog
    </div>
</a>
        </div>
        <div class="menu-trigger">menu</div>
    </div>
    
    <nav class="menu">
        <ul class="menu__inner menu__inner--desktop">
            
            
                    <li>
                        <a href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm">Home</a>
                    </li>
                
            </ul>

        <ul class="menu__inner menu__inner--mobile">
            
        <li>
            <a href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm">Home</a>
        </li>
        </ul>
    </nav>

    </header>
<div class="content"><div class="post">
        <h1 class="post-title">
            <a href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;ml-flight-rules&#x2F;">Machine Learning Flight Rules</a>
        </h1>
        
    <div class="post-meta">
        <span class="post-date">2020.07.03
                </span>

        <span class="post-author"></span>

        

    
    :: {<a href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;categories&#x2F;cross-posts&#x2F;">cross-posts</a>} 

            
    ::
    #<a href="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;tags&#x2F;ml&#x2F;">ML</a>
        
    
            
        
    </div>



        

        <div class="post-content">
            <p><em>My Repository: https://github.com/bkkaggle/machine-learning-flight-rules</em></p>
<p><em>A guide for astronauts (now, people doing machine learning) about what to do when things go wrong.</em></p>
<h2 id="what-are-flight-rules">What are &quot;flight rules&quot;?<a class="zola-anchor" href="#what-are-flight-rules" aria-label="Anchor link for: what-are-flight-rules">§</a>
</h2>
<hr />
<p><em>Copied from: https://github.com/k88hudson/git-flight-rules</em></p>
<blockquote>
<p>Flight Rules are the hard-earned body of knowledge recorded in manuals that list, step-by-step, what to do if X occurs, and why. Essentially, they are extremely detailed, scenario-specific standard operating procedures._</p>
<p>NASA has been capturing our missteps, disasters and solutions since the early 1960s, when Mercury-era ground teams first started gathering &quot;lessons learned&quot; into a compendium that now lists thousands of problematic situations, from engine failure to busted hatch handles to computer glitches, and their solutions._</p>
</blockquote>
<p>— Chris Hadfield, <em>An Astronaut's Guide to Life</em>.</p>
<hr />
<h2 id="general-tips">General tips<a class="zola-anchor" href="#general-tips" aria-label="Anchor link for: general-tips">§</a>
</h2>
<hr />
<p>https://karpathy.github.io/2019/04/25/recipe has some great best practices for training neural networks. Some of his tips include:</p>
<h3 id="look-at-the-wrongly-classified-predictions-of-your-network">Look at the wrongly classified predictions of your network<a class="zola-anchor" href="#look-at-the-wrongly-classified-predictions-of-your-network" aria-label="Anchor link for: look-at-the-wrongly-classified-predictions-of-your-network">§</a>
</h3>
<hr />
<p>This can help tell you what might be wrong with your dataset or model.</p>
<h3 id="always-set-the-random-seed">Always set the random seed<a class="zola-anchor" href="#always-set-the-random-seed" aria-label="Anchor link for: always-set-the-random-seed">§</a>
</h3>
<hr />
<p>This will prevent (most, but not all!) variation in results between otherwise identical training runs.</p>
<h3 id="make-a-baseline-and-then-increase-the-size-of-your-model-until-it-overfits">Make a baseline and then increase the size of your model until it overfits<a class="zola-anchor" href="#make-a-baseline-and-then-increase-the-size-of-your-model-until-it-overfits" aria-label="Anchor link for: make-a-baseline-and-then-increase-the-size-of-your-model-until-it-overfits">§</a>
</h3>
<hr />
<h4 id="use-a-very-simplified-baseline-to-test-that-your-code-works-correctly">Use a very simplified baseline to test that your code works correctly<a class="zola-anchor" href="#use-a-very-simplified-baseline-to-test-that-your-code-works-correctly" aria-label="Anchor link for: use-a-very-simplified-baseline-to-test-that-your-code-works-correctly">§</a>
</h4>
<hr />
<p>Use a simple model (e.g. a small resnet18 or linear regression) and confirm that your code works properly and as it is supposed to.</p>
<h4 id="overfit-on-a-single-batch">Overfit on a single batch<a class="zola-anchor" href="#overfit-on-a-single-batch" aria-label="Anchor link for: overfit-on-a-single-batch">§</a>
</h4>
<hr />
<p>Try using as small of a batch size as you can (if you're using batch normalization, that would be a batch of two examples). Your loss should go down to zero within a few iterations. If it doesn't, that means you have a problem somewhere in your code.</p>
<h4 id="be-sure-that-you-re-data-has-been-correctly-processed">Be sure that you're data has been correctly processed<a class="zola-anchor" href="#be-sure-that-you-re-data-has-been-correctly-processed" aria-label="Anchor link for: be-sure-that-you-re-data-has-been-correctly-processed">§</a>
</h4>
<hr />
<p>Visualize your input data right before the <code>out = model(x)</code> to be sure that the data being sent to the network is correct (data has normalized properly, augmentations have been applied correctly, etc)</p>
<h4 id="simple-models-complex-models">Simple models -&gt; complex models<a class="zola-anchor" href="#simple-models-complex-models" aria-label="Anchor link for: simple-models-complex-models">§</a>
</h4>
<hr />
<p>In most cases, start with a simple model (eg: resnet18) then go on to using larger and more complex models (eg: SE-ResNeXt-101).</p>
<h4 id="start-with-a-simple-optimizer">Start with a simple optimizer<a class="zola-anchor" href="#start-with-a-simple-optimizer" aria-label="Anchor link for: start-with-a-simple-optimizer">§</a>
</h4>
<hr />
<p>Adam is almost always a safe choice, It works well and doesn't need extensive hyperparameter tuning. Kaparthy suggests using it with a learning rate of 3e-4.
I usually start with SGD with a learning rate of 0.1 and a momentum of 0.9 for most image classification and segmentation tasks.</p>
<h4 id="change-one-thing-at-a-time">Change one thing at a time<a class="zola-anchor" href="#change-one-thing-at-a-time" aria-label="Anchor link for: change-one-thing-at-a-time">§</a>
</h4>
<hr />
<p>Change one hyperparameter/augmentation/architecture and see its effects on the performance of your network. Changing multiple things at a time won't tell you what changes helped and which didn't.</p>
<h3 id="regularize-your-model">Regularize your model<a class="zola-anchor" href="#regularize-your-model" aria-label="Anchor link for: regularize-your-model">§</a>
</h3>
<hr />
<h4 id="get-more-data">Get more data<a class="zola-anchor" href="#get-more-data" aria-label="Anchor link for: get-more-data">§</a>
</h4>
<hr />
<p>Training on more data will always decrease the amount of overfitting and is the easiest way to regularize a model</p>
<h4 id="data-augmentation">Data augmentation<a class="zola-anchor" href="#data-augmentation" aria-label="Anchor link for: data-augmentation">§</a>
</h4>
<hr />
<p>This will artificially increase the size of your dataset and is the next best thing to collecting more data. Be sure that the augmentations you use make sense in the context of the task (flipping images of text in an OCR task left to right will hurt your model instead of helping it).</p>
<h4 id="use-a-pretrained-network">Use a pretrained network<a class="zola-anchor" href="#use-a-pretrained-network" aria-label="Anchor link for: use-a-pretrained-network">§</a>
</h4>
<hr />
<p>Pretrained networks (usually on Imagenet) help jumpstart your model especially when you have a smaller dataset. The domain of the pretrained network doesn't usually prevent it from helping although pretraining on a similar domain will be better.</p>
<h4 id="decrease-the-batch-size">Decrease the batch size<a class="zola-anchor" href="#decrease-the-batch-size" aria-label="Anchor link for: decrease-the-batch-size">§</a>
</h4>
<hr />
<p>Smaller batch sizes usually help increase regularization</p>
<h4 id="use-early-stopping">Use early stopping<a class="zola-anchor" href="#use-early-stopping" aria-label="Anchor link for: use-early-stopping">§</a>
</h4>
<hr />
<p>Use the validation loss to only save the best performing checkpoint of the network after the val loss hasn't gone down for a certain number of epochs</p>
<h3 id="squeeze-out-more-performance-out-of-the-network">Squeeze out more performance out of the network<a class="zola-anchor" href="#squeeze-out-more-performance-out-of-the-network" aria-label="Anchor link for: squeeze-out-more-performance-out-of-the-network">§</a>
</h3>
<hr />
<h4 id="ensemble">Ensemble<a class="zola-anchor" href="#ensemble" aria-label="Anchor link for: ensemble">§</a>
</h4>
<hr />
<p>Ensemble multiple models either trained on different cross validation splits of the dataset or using different architectures. This always boosts performance by a few percentage points and gives you a more confident measure of the performance of the model on the dataset. Averaging metrics from models in an ensemble will help you figure out whether a change in the model is actually an improvement or random noise.</p>
<h4 id="use-early-stopping-on-the-val-metric">Use early stopping on the val metric<a class="zola-anchor" href="#use-early-stopping-on-the-val-metric" aria-label="Anchor link for: use-early-stopping-on-the-val-metric">§</a>
</h4>
<hr />
<ul>
<li>Increase the size of the model until you overfit, then add regularization</li>
<li>augmentation on mask</li>
<li>correlation in ensembles</li>
<li>noise in ensembling</li>
</ul>
<hr />
<p>Another great resource for best practices when training neural networks is (http://amid.fish/reproducing-deep-rl). This article focused on best practices for deep rl, but most of its recommendations are still useful on normal machine learning. Some of these tips include:</p>
<h3 id="learn-to-deal-with-long-iteration-times">Learn to deal with long iteration times<a class="zola-anchor" href="#learn-to-deal-with-long-iteration-times" aria-label="Anchor link for: learn-to-deal-with-long-iteration-times">§</a>
</h3>
<hr />
<p>Most normal programming (web development, IOS development, etc) iteration times usually range in the seconds, but iteration times in machine learning range from minutes to hours. This means that &quot;experimenting a lot and thinking a little&quot;, which is usually fine in other programming contexts, will make you waste a lot of time waiting for a training run to finish. Instead, spending more time thinking about what your code does and how it might not work will help you make less mistakes and waste less time.</p>
<h3 id="keep-a-log-of-what-you-re-working-on">Keep a log of what you're working on<a class="zola-anchor" href="#keep-a-log-of-what-you-re-working-on" aria-label="Anchor link for: keep-a-log-of-what-you-re-working-on">§</a>
</h3>
<hr />
<p>Keeping records (tensorboard graphs/model checkpoints/metrics) of training runs and configurations will really help you out when figuring out what worked and what didn't. Additionally, keeping track of what you're working on and your mindset as you're working through a problem will help you when you have to come back to your work days or weeks later.</p>
<h3 id="try-to-predict-how-your-code-will-fail">Try to predict how your code will fail<a class="zola-anchor" href="#try-to-predict-how-your-code-will-fail" aria-label="Anchor link for: try-to-predict-how-your-code-will-fail">§</a>
</h3>
<hr />
<p>Doing this will cut down on the amount of failures that seem obvious in retrospect. I've sometimes had problems where I knew what was wrong with my code before going through the code to debug it. To stop making as many obvious mistakes, I wouldn't start a new training run if I was uncertain about whether it would work, and then would find and fix what might have gone wrong.</p>
<h3 id="resources">Resources<a class="zola-anchor" href="#resources" aria-label="Anchor link for: resources">§</a>
</h3>
<hr />
<ul>
<li>https://karpathy.github.io/2019/04/25/recipe</li>
<li>http://amid.fish/reproducing-deep-rl</li>
</ul>
<hr />
<h2 id="advanced-tips">Advanced tips<a class="zola-anchor" href="#advanced-tips" aria-label="Anchor link for: advanced-tips">§</a>
</h2>
<hr />
<ul>
<li>some tips should be taken with a grain of salt</li>
<li>from: https://gist.github.com/bkkaggle/67bb9b5e6132e5d3c30e366c8d403369</li>
</ul>
<hr />
<h3 id="basic-architectures-are-sometimes-better">Basic architectures are sometimes better<a class="zola-anchor" href="#basic-architectures-are-sometimes-better" aria-label="Anchor link for: basic-architectures-are-sometimes-better">§</a>
</h3>
<hr />
<p>Always using the latest, most advanced, SOTA model for a task isn't always the best choice. For example, Although more advanced semantic segmentation models like deeplab and pspnet are SOTA on datasets like PASCAL VOC and cityscapes, simpler architectures like U-nets are easier to train and adapt to new tasks and preform almost just as well on several recent kaggle competitions (https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/107824#latest-623920) (https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/69291#latest-592781).</p>
<h3 id="be-sure-that-code-that-you-copied-from-github-or-stackoverflow-is-correct">Be sure that code that you copied from Github or Stackoverflow is correct<a class="zola-anchor" href="#be-sure-that-code-that-you-copied-from-github-or-stackoverflow-is-correct" aria-label="Anchor link for: be-sure-that-code-that-you-copied-from-github-or-stackoverflow-is-correct">§</a>
</h3>
<hr />
<p>It's a good idea to check code from Github and Stackoverflow to make sure it is correct and that you are using it in the correct way. In the Quora insincere questions classification Kaggle competition, a popular implementation of attention summed up the weighted features instead of weighting the actual features with the attention weights (https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/79911) (https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/76583#450210).</p>
<h3 id="don-t-excessively-tune-hyperparameters">Don't excessively tune hyperparameters<a class="zola-anchor" href="#don-t-excessively-tune-hyperparameters" aria-label="Anchor link for: don-t-excessively-tune-hyperparameters">§</a>
</h3>
<hr />
<p>Every time you tune hyperparameters on a validation set, you risk overfitting those hyperparameters to that validation set. If done correctly, the improvement from having better hyperparameters will outweigh the risk of having hyperparameters that don't work well on the test set.</p>
<h3 id="set-up-cyclic-learning-rates-correctly">Set up cyclic learning rates correctly<a class="zola-anchor" href="#set-up-cyclic-learning-rates-correctly" aria-label="Anchor link for: set-up-cyclic-learning-rates-correctly">§</a>
</h3>
<hr />
<p>If you're using a cyclic learning rate, be sure that the learning rate is at it's lowest point when you have finished training.</p>
<h3 id="manually-init-layers">Manually init layers<a class="zola-anchor" href="#manually-init-layers" aria-label="Anchor link for: manually-init-layers">§</a>
</h3>
<hr />
<p>Pytorch will automatically initialize layers for you, but depending on your activation function, you might want to use the correct gain for your activation function. Take a look at the pytorch <a href="https://pytorch.org/docs/stable/nn.init.html">documentation</a> for more information.</p>
<h3 id="mixed-half-precision-training">Mixed/half precision training<a class="zola-anchor" href="#mixed-half-precision-training" aria-label="Anchor link for: mixed-half-precision-training">§</a>
</h3>
<hr />
<p>Mixed or half precision training lets you train on larger batch sizes and can speed up your training. Take a look at <a href="https://discuss.pytorch.org/t/training-with-half-precision/11815">this</a> if you want to simply use half precision training.</p>
<h4 id="what-is-the-difference-between-mixed-and-half-precision-training">What is the difference between mixed and half precision training?<a class="zola-anchor" href="#what-is-the-difference-between-mixed-and-half-precision-training" aria-label="Anchor link for: what-is-the-difference-between-mixed-and-half-precision-training">§</a>
</h4>
<hr />
<p>Nvidia's Volta and Turing GPUs contain tensor cores that can do fast fp16 matrix multiplications and significantly speed up your training.</p>
<p>&quot;True&quot; half precision training casts the inputs and the model's parameters to 16 bit floats and computes everything using 16 bit floats. The advantages of this are that fp16 floats only use half the amount of vram as normal fp32 floats, letting you double the batch size while training. This is the fastest and most optimized way to take advantage of tensor cores, but comes at a cost. Using fp16 floats for the model's parameters and batch norm statistics means that if the gradients are small enough, they can underflow and be replaced by zeros.</p>
<p>Mixed precision solves these problems by keeping a master copy of the model's parameters in 32 bit floats. The inputs and the model's parameters are still cast to fp16, but after the backwards pass, the gradients are copied to the master copy and cast to fp32. The parameters are updated in fp32 to prevent gradients from underflowing, and the new, updated master copy's parameters are cast to fp16 and copied to the original fp16 model. Nvidia's apex library recommends using mixed precision in a different way by casting inputs to tensor core-friendly operations to fp16 and keeping other operations in fp32. Both of these mixed precision approaches have an overhead compared to half precision training, but are faster and use less vram than fp32 training.</p>
<p>Take a look at (https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) for more information.</p>
<h3 id="apex-won-t-install-on-gcp-s-deep-learning-vm">Apex won't install on GCP's deep learning vm<a class="zola-anchor" href="#apex-won-t-install-on-gcp-s-deep-learning-vm" aria-label="Anchor link for: apex-won-t-install-on-gcp-s-deep-learning-vm">§</a>
</h3>
<hr />
<p>This is a known issue with apex, take a look at (https://github.com/NVIDIA/apex/issues/259) for some possible solutions.</p>
<h4 id="resources-1">Resources<a class="zola-anchor" href="#resources-1" aria-label="Anchor link for: resources-1">§</a>
</h4>
<hr />
<p>If you're using pytorch, Nvidia's apex library (https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) is the easiest way to start using mixed precision.
If you want to read more about half and mixed precision training, take a look at https://forums.fast.ai/t/mixed-precision-training/20720</p>
<h3 id="gradient-accumulation">gradient accumulation<a class="zola-anchor" href="#gradient-accumulation" aria-label="Anchor link for: gradient-accumulation">§</a>
</h3>
<hr />
<p>If you want to train larger batches on a gpu without enough vram, gradient accumulation can help you out.</p>
<p>The basic idea is this: call <code>optimizer.step()</code> every n minibatches, accumulating the gradients at each minibatch, effectively training on a minibatch of size <code>batch_size x n</code>.</p>
<p>Here's a example showing how you could use gradient accumulation in pytorch, from (https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3#file-gradient_accumulation-py):</p>
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;">model.</span><span style="color:#eb6772;">zero_grad</span><span style="color:#abb2bf;">()                                   </span><span style="font-style:italic;color:#5f697a;"># Reset gradients tensors
</span><span style="color:#cd74e8;">for </span><span style="color:#abb2bf;">i, (inputs, labels) </span><span style="color:#cd74e8;">in </span><span style="color:#5ebfcc;">enumerate</span><span style="color:#abb2bf;">(training_set):
    predictions </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">model</span><span style="color:#abb2bf;">(inputs)                     </span><span style="font-style:italic;color:#5f697a;"># Forward pass
    </span><span style="color:#abb2bf;">loss </span><span style="color:#adb7c9;">= </span><span style="color:#eb6772;">loss_function</span><span style="color:#abb2bf;">(predictions, labels)       </span><span style="font-style:italic;color:#5f697a;"># Compute loss function
    </span><span style="color:#abb2bf;">loss </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">loss </span><span style="color:#adb7c9;">/ </span><span style="color:#abb2bf;">accumulation_steps                </span><span style="font-style:italic;color:#5f697a;"># Normalize our loss (if averaged)
    </span><span style="color:#abb2bf;">loss.</span><span style="color:#eb6772;">backward</span><span style="color:#abb2bf;">()                                 </span><span style="font-style:italic;color:#5f697a;"># Backward pass
    </span><span style="color:#cd74e8;">if </span><span style="color:#abb2bf;">(i</span><span style="color:#adb7c9;">+</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">) </span><span style="color:#adb7c9;">% </span><span style="color:#abb2bf;">accumulation_steps </span><span style="color:#adb7c9;">== </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">:             </span><span style="font-style:italic;color:#5f697a;"># Wait for several backward steps
        </span><span style="color:#abb2bf;">optimizer.</span><span style="color:#eb6772;">step</span><span style="color:#abb2bf;">()                            </span><span style="font-style:italic;color:#5f697a;"># Now we can do an optimizer step
        </span><span style="color:#abb2bf;">model.</span><span style="color:#eb6772;">zero_grad</span><span style="color:#abb2bf;">()                           </span><span style="font-style:italic;color:#5f697a;"># Reset gradients tensors
        </span><span style="color:#cd74e8;">if </span><span style="color:#abb2bf;">(i</span><span style="color:#adb7c9;">+</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">) </span><span style="color:#adb7c9;">% </span><span style="color:#abb2bf;">evaluation_steps </span><span style="color:#adb7c9;">== </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">:           </span><span style="font-style:italic;color:#5f697a;"># Evaluate the model when we...
            </span><span style="color:#eb6772;">evaluate_model</span><span style="color:#abb2bf;">()                        </span><span style="font-style:italic;color:#5f697a;"># ...have no gradients accumulated
</span></code></pre>
<p>If you want to read more about gradient accumulation, check out this blog post (https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)</p>
<h3 id="multi-gpu-machine-training">multi gpu/machine training<a class="zola-anchor" href="#multi-gpu-machine-training" aria-label="Anchor link for: multi-gpu-machine-training">§</a>
</h3>
<hr />
<p>If you have multiple gpus, you can easily convert your current code to train your model on multiple gpus. Just follow the official tutorials on pytorch.org (https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html). The only problem with this is that Pytorch's build in <code>DataParallel</code> will gather the outputs from all the other gpus to gpu 1 to compute the loss and calculate gradients, using up more vram. There <em>is</em> an alternative to this though, just use this alternative balanced data parallel implementation (https://gist.github.com/thomwolf/7e2407fbd5945f07821adae3d9fd1312).</p>
<p>Take a look at (https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255#) for more information about multi gpu and distributed training.</p>
<h3 id="determinism">determinism<a class="zola-anchor" href="#determinism" aria-label="Anchor link for: determinism">§</a>
</h3>
<hr />
<p>Pytorch will give you different results every time you run a script unless you set random seeds for python, numpy, and pytorch. Fortunately, doing this is very simple and only requires you to add a few lines to the top of each python file. There is a catch though, setting <code>torch.backends.cudnn.deterministic</code> to <code>True</code> will slightly slow down your network.</p>
<pre style="background-color:#2b303b;">
<code><span style="color:#eb6772;">SEED </span><span style="color:#adb7c9;">= </span><span style="color:#db9d63;">42
</span><span style="color:#abb2bf;">np.random.</span><span style="color:#eb6772;">seed</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">SEED</span><span style="color:#abb2bf;">)
torch.</span><span style="color:#eb6772;">manual_seed</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">SEED</span><span style="color:#abb2bf;">)
torch.cuda.</span><span style="color:#eb6772;">manual_seed</span><span style="color:#abb2bf;">(</span><span style="color:#eb6772;">SEED</span><span style="color:#abb2bf;">)
torch.backends.cudnn.deterministic </span><span style="color:#adb7c9;">= </span><span style="color:#db9d63;">True
</span></code></pre>
<p>If you want a simple one-line way to do this, check out my <code>pytorch_zoo</code> library on github (https://github.com/bkkaggle/pytorch_zoo#seed_environmentseed42).</p>
<pre style="background-color:#2b303b;">
<code><span style="color:#cd74e8;">from </span><span style="color:#abb2bf;">pytorch_zoo.utils </span><span style="color:#cd74e8;">import </span><span style="color:#abb2bf;">seed_environment

</span><span style="color:#eb6772;">seed_environment</span><span style="color:#abb2bf;">(</span><span style="color:#db9d63;">42</span><span style="color:#abb2bf;">)
</span></code></pre>
<p>If you want more information on determinism in pytorch, take a look at these links:</p>
<ul>
<li>https://discuss.pytorch.org/t/how-to-get-deterministic-behavior/18177/7</li>
<li>https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/72770</li>
<li>https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch</li>
<li>https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/72040</li>
</ul>
<hr />
<h3 id="initialization">Initialization<a class="zola-anchor" href="#initialization" aria-label="Anchor link for: initialization">§</a>
</h3>
<p>Take a look at my <a href="/blog/2020/7/3/initialization">post</a> for more information.</p>
<hr />
<h3 id="normalization">Normalization<a class="zola-anchor" href="#normalization" aria-label="Anchor link for: normalization">§</a>
</h3>
<p>Take a look at my <a href="/blog/2020/3/25/normalization">post</a> for an overview.</p>
<hr />
<h4 id="batch-norm">Batch norm<a class="zola-anchor" href="#batch-norm" aria-label="Anchor link for: batch-norm">§</a>
</h4>
<hr />
<p>The original batch normalization paper put the batch norm layer before the activation function, recent research shows that putting the batch norm layer after the activation gives better results. A great article on batch norm and why it works can be found here (https://blog.paperspace.com/busting-the-myths-about-batch-normalization/).</p>
<h5 id="you-can-t-use-a-batch-size-of-1-with-batch-norm">You can't use a batch size of 1 with batch norm<a class="zola-anchor" href="#you-can-t-use-a-batch-size-of-1-with-batch-norm" aria-label="Anchor link for: you-can-t-use-a-batch-size-of-1-with-batch-norm">§</a>
</h5>
<hr />
<p>Batch norm relies on the mean and variance of all the elements in a batch, it won't work if you're using a batch size of one while training, so either skip over any leftover batches with batch sizes of 1 or increase the batch size to atleast 2.</p>
<h5 id="be-sure-to-use-model-eval-with-batch-norm">Be sure to use model.eval() with batch norm<a class="zola-anchor" href="#be-sure-to-use-model-eval-with-batch-norm" aria-label="Anchor link for: be-sure-to-use-model-eval-with-batch-norm">§</a>
</h5>
<hr />
<p>Run <code>model.eval()</code> before your validation loop to make sure pytorch uses the running mean and variance calculated over the training set. Also make sure to call <code>model.train()</code> before your training loop to start calculating the batch norm statistics again. You can read more about this at (https://discuss.pytorch.org/t/what-does-model-eval-do-for-batchnorm-layer/7146)</p>
<h5 id="resources-2">Resources<a class="zola-anchor" href="#resources-2" aria-label="Anchor link for: resources-2">§</a>
</h5>
<hr />
<p>http://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/ is a really good blog post on the different types of normalizations and when to them.</p>
<hr />
<h2 id="common-errors">Common errors<a class="zola-anchor" href="#common-errors" aria-label="Anchor link for: common-errors">§</a>
</h2>
<hr />
<h2 id="pytorch">Pytorch<a class="zola-anchor" href="#pytorch" aria-label="Anchor link for: pytorch">§</a>
</h2>
<hr />
<h3 id="losses">Losses<a class="zola-anchor" href="#losses" aria-label="Anchor link for: losses">§</a>
</h3>
<hr />
<h4 id="cross-entropy-vs-nll-loss-for-multi-class-classification"><code>cross_entropy</code> vs nll loss for multi-class classification<a class="zola-anchor" href="#cross-entropy-vs-nll-loss-for-multi-class-classification" aria-label="Anchor link for: cross-entropy-vs-nll-loss-for-multi-class-classification">§</a>
</h4>
<hr />
<p>Either pass the logits for a multi-class classification task to <code>log_softmax</code> first, then through the <code>nll</code> loss or pass the logits directly to <code>cross_entropy</code>. They will give you the same result, but <code>cross_entropy</code> is more numerically stable. Use <code>softmax</code> separately to convert logits into probabilities for prediction or for calculating metrics. Take a look at (https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html) for more information.</p>
<h4 id="binary-cross-entropy-vs-binary-cross-entropy-with-logits-for-binary-classification-tasks"><code>binary_cross_entropy</code> vs <code>binary_cross_entropy_with_logits</code> for binary classification tasks<a class="zola-anchor" href="#binary-cross-entropy-vs-binary-cross-entropy-with-logits-for-binary-classification-tasks" aria-label="Anchor link for: binary-cross-entropy-vs-binary-cross-entropy-with-logits-for-binary-classification-tasks">§</a>
</h4>
<hr />
<p>Either pass the logits for a binary classification task to <code>sigmoid</code> first, then through <code>binary_cross_entropy</code> or pass the logits directly to <code>binary_cross_entropy_with_logits</code>. Just as the example above, they will give you the same result, but <code>binary_cross_entropy</code> is more numerically stable. Use <code>sigmoid</code> separately to conver the logits into probabilities for prediction or for calculating metrics. Again, take a look at (https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html) for more information.</p>
<h4 id="binary-classification-vs-multi-class-classification">Binary classification vs multi-class classification<a class="zola-anchor" href="#binary-classification-vs-multi-class-classification" aria-label="Anchor link for: binary-classification-vs-multi-class-classification">§</a>
</h4>
<hr />
<p>A binary classification task can also be represented as a multi-class classification task with two classes, positive and negative. They will give you the same result and should be numerically identical.</p>
<p>Here's an example, taken from (https://sebastianraschka.com/faq/docs/pytorch-crossentropy.html), on how you could do this:</p>
<pre style="background-color:#2b303b;">
<code><span style="color:#adb7c9;">&gt;&gt;&gt; </span><span style="color:#cd74e8;">import </span><span style="color:#abb2bf;">torch
</span><span style="color:#adb7c9;">&gt;&gt;&gt; </span><span style="color:#abb2bf;">labels </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">torch.</span><span style="color:#eb6772;">tensor</span><span style="color:#abb2bf;">([</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">], </span><span style="color:#eb6772;">dtype</span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;">torch.float)
</span><span style="color:#adb7c9;">&gt;&gt;&gt; </span><span style="color:#abb2bf;">probas </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">torch.</span><span style="color:#eb6772;">tensor</span><span style="color:#abb2bf;">([</span><span style="color:#db9d63;">0.9</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0.1</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0.8</span><span style="color:#abb2bf;">], </span><span style="color:#eb6772;">dtype</span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;">torch.float)
</span><span style="color:#adb7c9;">&gt;&gt;&gt; </span><span style="color:#abb2bf;">torch.nn.functional.</span><span style="color:#eb6772;">binary_cross_entropy</span><span style="color:#abb2bf;">(probas, labels)
</span><span style="color:#eb6772;">tensor</span><span style="color:#abb2bf;">(</span><span style="color:#db9d63;">0.1446</span><span style="color:#abb2bf;">)

</span><span style="color:#adb7c9;">&gt;&gt;&gt; </span><span style="color:#abb2bf;">labels </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">torch.</span><span style="color:#eb6772;">tensor</span><span style="color:#abb2bf;">([</span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">1</span><span style="color:#abb2bf;">], </span><span style="color:#eb6772;">dtype</span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;">torch.long)
</span><span style="color:#adb7c9;">&gt;&gt;&gt; </span><span style="color:#abb2bf;">probas </span><span style="color:#adb7c9;">= </span><span style="color:#abb2bf;">torch.</span><span style="color:#eb6772;">tensor</span><span style="color:#abb2bf;">([[</span><span style="color:#db9d63;">0.1</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0.9</span><span style="color:#abb2bf;">],
</span><span style="color:#db9d63;">...                        </span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">0.9</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0.1</span><span style="color:#abb2bf;">],
</span><span style="color:#db9d63;">...                        </span><span style="color:#abb2bf;">[</span><span style="color:#db9d63;">0.2</span><span style="color:#abb2bf;">, </span><span style="color:#db9d63;">0.8</span><span style="color:#abb2bf;">]], </span><span style="color:#eb6772;">dtype</span><span style="color:#adb7c9;">=</span><span style="color:#abb2bf;">torch.float)
</span><span style="color:#adb7c9;">&gt;&gt;&gt; </span><span style="color:#abb2bf;">torch.nn.functional.</span><span style="color:#eb6772;">nll_loss</span><span style="color:#abb2bf;">(torch.</span><span style="color:#eb6772;">log</span><span style="color:#abb2bf;">(probas), labels)
</span><span style="color:#eb6772;">tensor</span><span style="color:#abb2bf;">(</span><span style="color:#db9d63;">0.1446</span><span style="color:#abb2bf;">)
</span></code></pre>
<hr />
<h4 id="pin-memory-in-the-dataloader">Pin memory in the dataloader<a class="zola-anchor" href="#pin-memory-in-the-dataloader" aria-label="Anchor link for: pin-memory-in-the-dataloader">§</a>
</h4>
<hr />
<p>Set <code>pin_memory</code> to <code>true</code> in your dataloader to speed up transferring your data from cpu to gpu. Take a look at this for more information (https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723).</p>
<h4 id="model-eval-vs-torch-no-grad"><code>model.eval()</code> vs <code>torch.no_grad()</code><a class="zola-anchor" href="#model-eval-vs-torch-no-grad" aria-label="Anchor link for: model-eval-vs-torch-no-grad">§</a>
</h4>
<hr />
<p><code>model.eval()</code> will switch your dropout and batch norm layers to eval mode, turning off dropout and using the running mean and stddev for the batch norm layers. <code>torch.no_grad()</code> will tell pytorch to stop tracking operations, reducing memory usage and speeding up your evaluation loop. To use these properly, run <code>model.train()</code> before each training loop, run <code>model.eval()</code> before each evaluation loop, and wrap your evaluation loop with <code>with torch.no_grad():</code> Take a look at this for more information (https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/11).</p>
<h4 id="what-to-use-for-num-workers-in-the-dataloader">What to use for <code>num_workers</code> in the dataloader<a class="zola-anchor" href="#what-to-use-for-num-workers-in-the-dataloader" aria-label="Anchor link for: what-to-use-for-num-workers-in-the-dataloader">§</a>
</h4>
<hr />
<p>If your gpu utilization fluctuates a lot and generally remains low (&lt; 90%), this might mean that your gpu is waiting for the cpu to finish processing all the elements in your batch and that <code>num_workers</code> might be your main bottleneck. <code>num_workers</code> in the dataloader is used to tell pytorch how many parallel workers to use to preprocess the data ahead of time. Set <code>num_workers</code> to the number of cores that you have in your cpu. This will fully utilize all your cpu cores to minimize the amount of time the gpu spends waiting for the cpu to process the data. If your gpu utilization still remains low, you should get more cpu cores or preprocess the data ahead of time and save it to disk. Take a look at these articles for more information: (https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader) and (https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel).</p>
<h3 id="tensorboard">Tensorboard<a class="zola-anchor" href="#tensorboard" aria-label="Anchor link for: tensorboard">§</a>
</h3>
<hr />
<p>Tensorboard is really useful when you want to view your model's training progress in real time. Now that Pytorch 1.1 is out, you can now log metrics directly to tensorboard from Pytorch.</p>
<h4 id="how-to-use-it">How to use it<a class="zola-anchor" href="#how-to-use-it" aria-label="Anchor link for: how-to-use-it">§</a>
</h4>
<hr />
<p>Follow these instructions for a quickstart (https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html).</p>
<h3 id="use-tensorboard-in-a-kaggle-kernel">Use Tensorboard in a kaggle kernel<a class="zola-anchor" href="#use-tensorboard-in-a-kaggle-kernel" aria-label="Anchor link for: use-tensorboard-in-a-kaggle-kernel">§</a>
</h3>
<hr />
<p>Just copy this code snippet into a cell at the top of your kernel</p>
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;">!mkdir logs
</span><span style="color:#eb6772;">get_ipython</span><span style="color:#abb2bf;">().</span><span style="color:#eb6772;">system_raw</span><span style="color:#abb2bf;">(</span><span style="color:#9acc76;">&#39;tensorboard --logdir ./logs --host 0.0.0.0 --port 6006 &amp;&#39;</span><span style="color:#abb2bf;">)
!ssh </span><span style="color:#adb7c9;">-</span><span style="color:#abb2bf;">o </span><span style="color:#9acc76;">&quot;StrictHostKeyChecking no&quot; </span><span style="color:#adb7c9;">-</span><span style="color:#abb2bf;">R </span><span style="color:#db9d63;">80</span><span style="color:#abb2bf;">:localhost:</span><span style="color:#db9d63;">6006 </span><span style="color:#abb2bf;">serveo.net
</span></code></pre>
<p>I also have another quickstart at my <a href="https://github.com/bkkaggle/pytorch_zoo#viewing-training-progress-with-tensorboard-in-a-kaggle-kernel">pytorch_zoo</a> repository.</p>
<h4 id="what-do-all-the-tensorboard-histograms-mean">What do all the Tensorboard histograms mean?<a class="zola-anchor" href="#what-do-all-the-tensorboard-histograms-mean" aria-label="Anchor link for: what-do-all-the-tensorboard-histograms-mean">§</a>
</h4>
<hr />
<p>Take a look at these stackoberflow posts:</p>
<ul>
<li>https://stackoverflow.com/questions/42315202/understanding-tensorboard-weight-histograms</li>
<li>https://stackoverflow.com/questions/38149622/what-is-a-good-explanation-of-how-to-read-the-histogram-feature-of-tensorboard</li>
</ul>
<hr />
<h3 id="common-errors-1">Common errors<a class="zola-anchor" href="#common-errors-1" aria-label="Anchor link for: common-errors-1">§</a>
</h3>
<hr />
<h4 id="runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation<a class="zola-anchor" href="#runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation" aria-label="Anchor link for: runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation">§</a>
</h4>
<hr />
<p>In place operations and operations on slices of tensors can cause problems with Pytorch's autograd. To fix this, convert your inplace operation, <code>x[:, 0, :] += 1</code>, to a non inplace operation, <code>x[:, 0, :] = x[:, 0, :].clone() + 1</code>, and use <code>.clone()</code> to avoid problems with operations on tensor slices. Take a look at (https://discuss.pytorch.org/t/encounter-the-runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation/836) for more information.</p>
<h4 id="creating-mtgp-constants-failed-error">Creating MTGP constants failed error<a class="zola-anchor" href="#creating-mtgp-constants-failed-error" aria-label="Anchor link for: creating-mtgp-constants-failed-error">§</a>
</h4>
<hr />
<p>This error happens when &quot;using an embedding layer and passing out of range indexes (indexes &gt; num_embeddings)&quot; from (https://discuss.pytorch.org/t/solved-creating-mtgp-constants-failed-error/15084/4). For more information, take a look at (https://discuss.pytorch.org/t/solved-creating-mtgp-constants-failed-error/15084).</p>
<h4 id="valueerror-expected-more-than-1-value-per-channel-when-training">ValueError: Expected more than 1 value per channel when training<a class="zola-anchor" href="#valueerror-expected-more-than-1-value-per-channel-when-training" aria-label="Anchor link for: valueerror-expected-more-than-1-value-per-channel-when-training">§</a>
</h4>
<hr />
<p>This error happens when you're using a batch size of 1 while training with batch norm. Batch norm expects to have a batch size of at least 2. For more information, take a look at (https://github.com/pytorch/pytorch/issues/4534)</p>
<hr />
<h3 id="how-to">How to<a class="zola-anchor" href="#how-to" aria-label="Anchor link for: how-to">§</a>
</h3>
<hr />
<h4 id="how-to-implement-gradient-clipping">How to implement gradient clipping<a class="zola-anchor" href="#how-to-implement-gradient-clipping" aria-label="Anchor link for: how-to-implement-gradient-clipping">§</a>
</h4>
<hr />
<p>Here's the code for gradient clipping:</p>
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;">torch.nn.utils.</span><span style="color:#eb6772;">clip_grad_norm</span><span style="color:#abb2bf;">(model.</span><span style="color:#eb6772;">parameters</span><span style="color:#abb2bf;">(), value)
</span></code></pre>
<p>If you want to read more about gradient clipping in pytorch, take a look at (https://discuss.pytorch.org/t/proper-way-to-do-gradient-clipping/191).</p>
<h4 id="how-to-implement-global-max-avg-pooling">How to implement global max/avg pooling<a class="zola-anchor" href="#how-to-implement-global-max-avg-pooling" aria-label="Anchor link for: how-to-implement-global-max-avg-pooling">§</a>
</h4>
<hr />
<p>Follow the instructions from (https://discuss.pytorch.org/t/global-max-pooling/1345/2)</p>
<h4 id="how-to-release-gpu-memory">How to release gpu memory<a class="zola-anchor" href="#how-to-release-gpu-memory" aria-label="Anchor link for: how-to-release-gpu-memory">§</a>
</h4>
<hr />
<p>There is no simple way to do this, but you can release as much memory as you can by running <code>torch.cuda.empty_cache()</code>. Take a look at (https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530) for more information.</p>
<h4 id="how-to-concatenate-hidden-states-of-a-bidirectional-lstm">How to concatenate hidden states of a bidirectional lstm<a class="zola-anchor" href="#how-to-concatenate-hidden-states-of-a-bidirectional-lstm" aria-label="Anchor link for: how-to-concatenate-hidden-states-of-a-bidirectional-lstm">§</a>
</h4>
<hr />
<p>Follow the instructions from (https://discuss.pytorch.org/t/concatenation-of-the-hidden-states-produced-by-a-bidirectional-lstm/3686/2).</p>
<h3 id="torchtext">Torchtext<a class="zola-anchor" href="#torchtext" aria-label="Anchor link for: torchtext">§</a>
</h3>
<hr />
<p>Torchtext is Pytorch's official NLP library, The library's official <a href="https://torchtext.readthedocs.io/en/latest/index.html">docs</a> are the best way to get started with the library, but are a bit limited and there are some blog posts that help you get a better sense of how to use the library:</p>
<ul>
<li>https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html</li>
<li>https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html</li>
<li>https://pytorch.org/tutorials/beginner/transformer_tutorial.html</li>
<li>http://anie.me/On-Torchtext/</li>
<li>http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/</li>
<li>http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/</li>
<li>https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84</li>
<li>https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496</li>
<li>https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html</li>
</ul>
<hr />
<h4 id="sort-batches-by-length">Sort batches by length<a class="zola-anchor" href="#sort-batches-by-length" aria-label="Anchor link for: sort-batches-by-length">§</a>
</h4>
<hr />
<p>Your recurrent models will train best if all the examples in a batch have similar lengths. Since all the examples in a batch are padded with zeros to the length of the longest example, grouping examples with identical or similar lengths will make your model more efficient and waste less of the GPU's memory. Use the iterator's <code>sort_key</code> attribute to tell it to group examples of similar lengths into each batch. If you're using <code>pack_padded_sequence</code>, set <code>sort_within_batch</code> to <code>True</code> since <code>pack_padded_sequence</code> expects examples in a batch to be in ascending order. Take a look at <a href="https://github.com/pytorch/text/issues/303">this</a> for more information.</p>
<ul>
<li>https://github.com/pytorch/text/issues/303</li>
</ul>
<hr />
<h4 id="pretrained-embeddings">Pretrained embeddings<a class="zola-anchor" href="#pretrained-embeddings" aria-label="Anchor link for: pretrained-embeddings">§</a>
</h4>
<hr />
<p>If you want to use a pretrained embedding like word2vec or glove, you will have to load in the pretrained vectors and update the field's vectors.</p>
<pre style="background-color:#2b303b;">
<code><span style="color:#abb2bf;"># Load in the vectors
vectors = torchtext.vocab.Vectors(&#39;/path/to/vectors&#39;)

# Create the text field
text_field = data.Field(tokenize=tokenizer, lower=True, batch_first=True, include_lengths=True)

# Built the vocab for the field using the train dataset
text_field.build_vocab(train_dataset)

# Set the vectors of the field to be the pretrained vectors
text_field.vocab.set_vectors(vectors.stoi, vectors.vectors, vectors.dim)
</span></code></pre>
<p>Take a look at <a href="https://discuss.pytorch.org/t/aligning-torchtext-vocab-index-to-loaded-embedding-pre-trained-weights/20878">this</a> for more information.</p>
<h4 id="serializing-datasets">Serializing datasets<a class="zola-anchor" href="#serializing-datasets" aria-label="Anchor link for: serializing-datasets">§</a>
</h4>
<hr />
<p>If you're working with large datasets that take time to load and process, being able to serialize and save processed datasets to disk is a really nice feature. Unfortunately, this feature is <a href="https://github.com/pytorch/text/issues/140">still</a> a work in progress (the issue was created in 2017, and there doesn't seem to be that much work being done on torchtext as of late 2019), so the only way to do this at the moment is to follow <a href="https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496">this</a> article.</p>
<h2 id="kaggle">Kaggle<a class="zola-anchor" href="#kaggle" aria-label="Anchor link for: kaggle">§</a>
</h2>
<hr />
<p>Here are some of tips and tricks I picked up while participating in kaggle competitions.</p>
<h3 id="tips">Tips<a class="zola-anchor" href="#tips" aria-label="Anchor link for: tips">§</a>
</h3>
<hr />
<h4 id="trust-your-local-validation">Trust your local validation<a class="zola-anchor" href="#trust-your-local-validation" aria-label="Anchor link for: trust-your-local-validation">§</a>
</h4>
<hr />
<p>Your score on your local validation set should be the most important, and sometimes the only, metric to pay attention to. Creating a validation set that you can trust to tell you whether you are or are not making progress is very important.</p>
<h4 id="optimize-for-the-metric">Optimize for the metric<a class="zola-anchor" href="#optimize-for-the-metric" aria-label="Anchor link for: optimize-for-the-metric">§</a>
</h4>
<hr />
<p>The goal of kaggle competitions is to get the highest (or lowest, depending on the metric) score on a specific metric. To do this, you might need to modify your model's loss function. For example, if the competition metric penalizes mistakes on rare classes more than common classes, oversampling or weighting the loss in favor of those classes can force the model to optimize for that metric.</p>
<h4 id="something-that-works-for-someone-might-not-work-for-you">Something that works for someone might not work for you<a class="zola-anchor" href="#something-that-works-for-someone-might-not-work-for-you" aria-label="Anchor link for: something-that-works-for-someone-might-not-work-for-you">§</a>
</h4>
<hr />
<p>Just because someone says on the discussion forum that a particular technique or module works better for them doesn't automatically mean that it will work for you.</p>
<h3 id="tricks">Tricks<a class="zola-anchor" href="#tricks" aria-label="Anchor link for: tricks">§</a>
</h3>
<hr />
<h4 id="removing-negative-samples-from-a-dataset-is-equivalent-to-loss-weighting">Removing negative samples from a dataset is equivalent to loss weighting<a class="zola-anchor" href="#removing-negative-samples-from-a-dataset-is-equivalent-to-loss-weighting" aria-label="Anchor link for: removing-negative-samples-from-a-dataset-is-equivalent-to-loss-weighting">§</a>
</h4>
<hr />
<p>This usually works well and is easier to do than loss weighting.</p>
<h4 id="thresholding">Thresholding<a class="zola-anchor" href="#thresholding" aria-label="Anchor link for: thresholding">§</a>
</h4>
<hr />
<h5 id="using-the-optimal-threshold-on-a-dataset-can-lead-to-brittle-results">Using the optimal threshold on a dataset can lead to brittle results<a class="zola-anchor" href="#using-the-optimal-threshold-on-a-dataset-can-lead-to-brittle-results" aria-label="Anchor link for: using-the-optimal-threshold-on-a-dataset-can-lead-to-brittle-results">§</a>
</h5>
<hr />
<p>If you choose thresholds for (binary) classification problems by choosing whatever value gives you the optimal score on a validation set, the threshold might be overfitting to the specific train-val split or to the specific architecture/hyperparameters. This can have two effects. First, the optimial threshold you found on the val set might not be the optimal threshold on the held out test set, decreasing your score. Second, this makes comparing results between runs with different model architectures or hyperparameters more difficult. Using different thresholds means that a model that is actually worse might get a higher score than a better model if you find a 'lucky' threshold.</p>
<h4 id="shakeup">Shakeup<a class="zola-anchor" href="#shakeup" aria-label="Anchor link for: shakeup">§</a>
</h4>
<hr />
<p>Shakeup prediction is a powerful tool to predict the likely range of scores for your model when evaluated on an unknown test set. It was first introduced by the winner of a kaggle competition as a way to stabilize his models in (https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/discussion/36809). It has also been used <a href="https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/67090">here</a> and <a href="https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/75821">here</a>.</p>
<h3 id="encoding-categorical-features">Encoding categorical features<a class="zola-anchor" href="#encoding-categorical-features" aria-label="Anchor link for: encoding-categorical-features">§</a>
</h3>
<hr />
<p>Encoding categorical features is a pretty important thing to do when working with tabular data.</p>
<p>Some resources I found for this are:</p>
<ul>
<li>https://www.kaggle.com/c/microsoft-malware-prediction/discussion/79045</li>
<li>https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study</li>
<li>https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features</li>
<li>https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76668</li>
</ul>
<h3 id="optimizing-code">Optimizing code<a class="zola-anchor" href="#optimizing-code" aria-label="Anchor link for: optimizing-code">§</a>
</h3>
<hr />
<h4 id="save-processed-datasets-to-disk">Save processed datasets to disk<a class="zola-anchor" href="#save-processed-datasets-to-disk" aria-label="Anchor link for: save-processed-datasets-to-disk">§</a>
</h4>
<hr />
<p>As long as your dataset isn't too large, saving the processed dataset to disk as a <code>.pkl</code> file, then loading it in whenever you need to use it, will save you time and will help increase your GPU utilization.</p>
<h4 id="use-multiprocessing">Use multiprocessing<a class="zola-anchor" href="#use-multiprocessing" aria-label="Anchor link for: use-multiprocessing">§</a>
</h4>
<hr />
<p>Python's <a href="https://docs.python.org/2/library/multiprocessing.html"><code>multiprocessing</code></a> library can help you take full advantage of all the cores in your CPU.</p>
<h3 id="data-leaks">Data Leaks<a class="zola-anchor" href="#data-leaks" aria-label="Anchor link for: data-leaks">§</a>
</h3>
<hr />
<p>Finding leaks in a dataset is a difficult, but sometimes useful skill.</p>
<p>Some good examples of how kagglers found leaks are:</p>
<ul>
<li>https://www.kaggle.com/raddar/towards-de-anonymizing-the-data-some-insights</li>
<li>https://www.kaggle.com/cpmpml/raddar-magic-explained-a-bit/</li>
</ul>
<h3 id="tools">Tools<a class="zola-anchor" href="#tools" aria-label="Anchor link for: tools">§</a>
</h3>
<hr />
<ul>
<li>https://github.com/mxbi/mlcrate</li>
<li>https://github.com/bkkaggle/pytorch_zoo (I made this)</li>
</ul>
<hr />
<h4 id="ctr-click-through-rate-prediction-tools">CTR (Click Through Rate prediction) tools<a class="zola-anchor" href="#ctr-click-through-rate-prediction-tools" aria-label="Anchor link for: ctr-click-through-rate-prediction-tools">§</a>
</h4>
<ul>
<li>https://github.com/guoday/ctrNet-tool</li>
<li>https://www.kaggle.com/c/avazu-ctr-prediction/discussion/10927</li>
<li>https://www.kaggle.com/c/microsoft-malware-prediction/discussion/75149</li>
<li>https://www.kaggle.com/scirpus/microsoft-libffm-munger</li>
<li>https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56497#331685</li>
</ul>
<hr />
<h4 id="ftrl-follow-the-regularized-leader">FTRL (Follow The Regularized Leader)<a class="zola-anchor" href="#ftrl-follow-the-regularized-leader" aria-label="Anchor link for: ftrl-follow-the-regularized-leader">§</a>
</h4>
<ul>
<li>https://www.kaggle.com/c/microsoft-malware-prediction/discussion/75246</li>
</ul>
<hr />
<h3 id="ensembling">Ensembling<a class="zola-anchor" href="#ensembling" aria-label="Anchor link for: ensembling">§</a>
</h3>
<h4 id="correlation">Correlation<a class="zola-anchor" href="#correlation" aria-label="Anchor link for: correlation">§</a>
</h4>
<hr />
<p>Ensembling models with low correlations is better than ensembling models with high correlations.</p>
<p>More information can be found here:</p>
<ul>
<li>https://www.kaggle.com/c/microsoft-malware-prediction/discussion/80368</li>
<li>https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/51058</li>
</ul>
<hr />
<h2 id="semantic-segmentation">Semantic segmentation<a class="zola-anchor" href="#semantic-segmentation" aria-label="Anchor link for: semantic-segmentation">§</a>
</h2>
<hr />
<p>Some good resources for semantic segmentation include:</p>
<ul>
<li>http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review</li>
<li>https://tuatini.me/practical-image-segmentation-with-unet/</li>
<li>https://www.jeremyjordan.me/semantic-segmentation/#loss</li>
<li>https://becominghuman.ai/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c</li>
</ul>
<hr />
<h2 id="nlp">NLP<a class="zola-anchor" href="#nlp" aria-label="Anchor link for: nlp">§</a>
</h2>
<hr />
<p>Take a look at some of these blog posts:</p>
<ul>
<li>http://ruder.io/a-review-of-the-recent-history-of-nlp/</li>
<li>https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e</li>
<li>https://medium.com/huggingface/100-times-faster-natural-language-processing-in-python-ee32033bdced</li>
</ul>
<hr />
<h3 id="awd-lstm">awd-LSTM<a class="zola-anchor" href="#awd-lstm" aria-label="Anchor link for: awd-lstm">§</a>
</h3>
<p>Take a look at these links:</p>
<ul>
<li>https://github.com/salesforce/awd-lstm-lm</li>
<li>https://www.fast.ai/2017/08/25/language-modeling-sota/</li>
</ul>
<hr />
<h3 id="multitask-learning">Multitask learning<a class="zola-anchor" href="#multitask-learning" aria-label="Anchor link for: multitask-learning">§</a>
</h3>
<p>Take a look at these links:</p>
<ul>
<li>http://ruder.io/multi-task/</li>
<li>http://ruder.io/multi-task-learning-nlp/</li>
</ul>
<hr />
<h3 id="combine-pretrained-embeddings">Combine pretrained embeddings<a class="zola-anchor" href="#combine-pretrained-embeddings" aria-label="Anchor link for: combine-pretrained-embeddings">§</a>
</h3>
<p>Adding/concatenating/(weighted) averaging multiple pretrained embeddings almost always leads to a boost in accuracy.</p>
<hr />
<h3 id="reinitialize-random-embedding-matrices-between-models">Reinitialize random embedding matrices between models<a class="zola-anchor" href="#reinitialize-random-embedding-matrices-between-models" aria-label="Anchor link for: reinitialize-random-embedding-matrices-between-models">§</a>
</h3>
<p>Initializing embeddings for unknown words randomly helps increase the diversity between models.</p>
<p>From: (https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/79720)</p>
<h3 id="try-out-dropout-or-gaussian-noise-after-the-embedding-layer">Try out dropout or gaussian noise after the embedding layer<a class="zola-anchor" href="#try-out-dropout-or-gaussian-noise-after-the-embedding-layer" aria-label="Anchor link for: try-out-dropout-or-gaussian-noise-after-the-embedding-layer">§</a>
</h3>
<hr />
<p>It can help increase model diversity and decrease overfitting</p>
<h3 id="correctly-use-masking-with-softmax">Correctly use masking with softmax<a class="zola-anchor" href="#correctly-use-masking-with-softmax" aria-label="Anchor link for: correctly-use-masking-with-softmax">§</a>
</h3>
<hr />
<h3 id="use-dynamic-minibatches-when-training-sequence-models">Use dynamic minibatches when training sequence models<a class="zola-anchor" href="#use-dynamic-minibatches-when-training-sequence-models" aria-label="Anchor link for: use-dynamic-minibatches-when-training-sequence-models">§</a>
</h3>
<hr />
<p>Using this will try to create batches of examples with equal lengths to minimize unncessary padding and wasted calculations. The code to use this is available at (https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/94779)</p>
<h3 id="reduce-the-amount-of-oov-out-of-vocabulary-words">Reduce the amount of OOV (Out Of Vocabulary) words<a class="zola-anchor" href="#reduce-the-amount-of-oov-out-of-vocabulary-words" aria-label="Anchor link for: reduce-the-amount-of-oov-out-of-vocabulary-words">§</a>
</h3>
<hr />
<h3 id="creating-a-vocabulary-on-the-train-val-sets-between-folds-can-lead-to-information-being-leaked-and-artificially-increasing-your-score">Creating a vocabulary on the train, val sets between folds can lead to information being leaked and artificially increasing your score<a class="zola-anchor" href="#creating-a-vocabulary-on-the-train-val-sets-between-folds-can-lead-to-information-being-leaked-and-artificially-increasing-your-score" aria-label="Anchor link for: creating-a-vocabulary-on-the-train-val-sets-between-folds-can-lead-to-information-being-leaked-and-artificially-increasing-your-score">§</a>
</h3>
<hr />
<ul>
<li>https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/79556</li>
</ul>
<hr />
<h3 id="how-to-use-pad-packed-sequence-and-pack-padded-sequence">How to use <code>pad_packed_sequence</code> and <code>pack_padded_sequence</code><a class="zola-anchor" href="#how-to-use-pad-packed-sequence-and-pack-padded-sequence" aria-label="Anchor link for: how-to-use-pad-packed-sequence-and-pack-padded-sequence">§</a>
</h3>
<p>Take a look at these links:</p>
<ul>
<li>https://discuss.pytorch.org/t/packedsequence-for-seq2seq-model/3907</li>
<li>https://discuss.pytorch.org/t/solved-multiple-packedsequence-input-ordering/2106/7</li>
</ul>
<hr />
<h3 id="transformers">Transformers<a class="zola-anchor" href="#transformers" aria-label="Anchor link for: transformers">§</a>
</h3>
<p>Take a look at these links:</p>
<ul>
<li>https://blog.floydhub.com/the-transformer-in-pytorch/</li>
<li>http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/</li>
<li>https://jalammar.github.io/illustrated-transformer/</li>
</ul>
<hr />
<h2 id="gradient-boosting">Gradient boosting<a class="zola-anchor" href="#gradient-boosting" aria-label="Anchor link for: gradient-boosting">§</a>
</h2>
<h3 id="how-to-set-hyperparameters">How to set hyperparameters<a class="zola-anchor" href="#how-to-set-hyperparameters" aria-label="Anchor link for: how-to-set-hyperparameters">§</a>
</h3>
<hr />
<p>Laurae's <a href="https://sites.google.com/view/lauraepp/parameters">website</a> is the best place to understand what parameters to use and what values to set them to.</p>
<h3 id="resources-3">Resources<a class="zola-anchor" href="#resources-3" aria-label="Anchor link for: resources-3">§</a>
</h3>
<hr />
<ul>
<li>https://www.kaggle.com/c/microsoft-malware-prediction/discussion/78253</li>
<li>http://mlexplained.com/2018/01/05/lightgbm-and-xgboost-explained</li>
<li>https://xgboost.readthedocs.io/en/latest/tutorials/model.html</li>
<li>https://lightgbm.readthedocs.io/en/latest/</li>
<li>https://xlearn-doc.readthedocs.io/en/latest/index.html</li>
<li>https://catboost.ai/docs/</li>
</ul>
<h2 id="setting-up-your-environment">Setting up your environment<a class="zola-anchor" href="#setting-up-your-environment" aria-label="Anchor link for: setting-up-your-environment">§</a>
</h2>
<hr />
<h3 id="jupyter-notebooks">Jupyter notebooks<a class="zola-anchor" href="#jupyter-notebooks" aria-label="Anchor link for: jupyter-notebooks">§</a>
</h3>
<hr />
<ul>
<li>https://stackoverflow.com/questions/43759610/how-to-add-python-3-6-kernel-alongside-3-5-on-jupyter</li>
<li>https://forums.fast.ai/t/jupyter-notebook-keyerror-allow-remote-access/24392</li>
</ul>
<hr />
<h3 id="python-3-6">Python 3.6+<a class="zola-anchor" href="#python-3-6" aria-label="Anchor link for: python-3-6">§</a>
</h3>
<ul>
<li>https://www.rosehosting.com/blog/how-to-install-python-3-6-4-on-debian-9/</li>
</ul>
<hr />
<h4 id="conda">Conda<a class="zola-anchor" href="#conda" aria-label="Anchor link for: conda">§</a>
</h4>
<ul>
<li>https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment</li>
<li>https://stackoverflow.com/questions/35245401/combining-conda-environment-yml-with-pip-requirements-txt</li>
<li>https://stackoverflow.com/questions/42352841/how-to-update-an-existing-conda-environment-with-a-yml-file</li>
</ul>
<hr />
<h2 id="build-your-own-library">Build your own library<a class="zola-anchor" href="#build-your-own-library" aria-label="Anchor link for: build-your-own-library">§</a>
</h2>
<p>I recently built my own machine learning <a href="https://github.com/bkkaggle/L2">library</a>, here are some of the resources I used:</p>
<ul>
<li>https://medium.com/@florian.caesar/how-to-create-a-machine-learning-framework-from-scratch-in-491-steps-93428369a4eb</li>
<li>https://github.com/joelgrus/joelnet</li>
<li>https://medium.com/@johan.mabille/how-we-wrote-xtensor-1-n-n-dimensional-containers-f79f9f4966a7</li>
<li>https://mlfromscratch.com</li>
<li>https://eisenjulian.github.io/deep-learning-in-100-lines/</li>
<li>http://blog.ezyang.com/2019/05/pytorch-internals/</li>
</ul>
<hr />
<h2 id="resources-4">Resources<a class="zola-anchor" href="#resources-4" aria-label="Anchor link for: resources-4">§</a>
</h2>
<h3 id="essential-tools">Essential tools<a class="zola-anchor" href="#essential-tools" aria-label="Anchor link for: essential-tools">§</a>
</h3>
<hr />
<ul>
<li>https://paperswithcode.com - This website lists available implementations of papers along with leaderboards showing which models are currently SOTA on a range of tasks and datasets</li>
<li>https://www.arxiv-vanity.com - This site converts PDF papers from Arxiv to mobile-friendly responsive web pages.</li>
<li>http://www.arxiv-sanity.com - This site is a better way to keep up to date with popular and interesting papers.</li>
</ul>
<hr />
<h3 id="model-zoos">Model zoos<a class="zola-anchor" href="#model-zoos" aria-label="Anchor link for: model-zoos">§</a>
</h3>
<ul>
<li>https://modelzoo.co/blog</li>
<li>https://modeldepot.io/search</li>
<li>https://github.com/sebastianruder/NLP-progress</li>
</ul>
<hr />
<h3 id="arxiv-alternatives">Arxiv alternatives<a class="zola-anchor" href="#arxiv-alternatives" aria-label="Anchor link for: arxiv-alternatives">§</a>
</h3>
<ul>
<li>https://www.arxiv-vanity.com</li>
<li>http://www.arxiv-sanity.com</li>
<li>https://www.scihive.org</li>
</ul>
<hr />
<h3 id="machine-learning-demos">Machine learning demos<a class="zola-anchor" href="#machine-learning-demos" aria-label="Anchor link for: machine-learning-demos">§</a>
</h3>
<ul>
<li>https://ganbreeder.app</li>
<li>https://talktotransformer.com</li>
<li>https://transformer.huggingface.co</li>
<li>https://www.nvidia.com/en-us/research/ai-playground/</li>
<li>https://alantian.net/ganshowcase/</li>
<li>https://rowanzellers.com/grover/</li>
<li>http://nvidia-research-mingyuliu.com/gaugan/</li>
<li>http://nvidia-research-mingyuliu.com/petswap/</li>
</ul>
<hr />
<h3 id="link-aggregators">Link aggregators<a class="zola-anchor" href="#link-aggregators" aria-label="Anchor link for: link-aggregators">§</a>
</h3>
<ul>
<li>https://news.ycombinator.com</li>
<li>https://www.sciencewiki.com</li>
<li>https://git.news/?ref=producthunt</li>
</ul>
<hr />
<h3 id="machine-learning-as-a-service">Machine learning as a service<a class="zola-anchor" href="#machine-learning-as-a-service" aria-label="Anchor link for: machine-learning-as-a-service">§</a>
</h3>
<ul>
<li>https://runwayml.com</li>
<li>https://supervise.ly</li>
</ul>
<hr />
<h3 id="coreml">Coreml<a class="zola-anchor" href="#coreml" aria-label="Anchor link for: coreml">§</a>
</h3>
<ul>
<li>https://developer.apple.com/machine-learning/models/</li>
<li>https://github.com/huggingface/swift-coreml-transformers</li>
<li>https://www.fritz.ai</li>
</ul>
<hr />
<h3 id="courses">Courses<a class="zola-anchor" href="#courses" aria-label="Anchor link for: courses">§</a>
</h3>
<ul>
<li>https://fast.ai</li>
<li>https://www.coursera.org/learn/competitive-data-science</li>
<li>https://www.deeplearning.ai</li>
<li>https://www.kaggle.com/learn/overview</li>
</ul>
<hr />
<h3 id="miscelaneous">Miscelaneous<a class="zola-anchor" href="#miscelaneous" aria-label="Anchor link for: miscelaneous">§</a>
</h3>
<ul>
<li>https://markus-beuckelmann.de/blog/boosting-numpy-blas.html</li>
<li>https://github.com/Wookai/paper-tips-and-tricks</li>
<li>https://github.com/dennybritz/deeplearning-papernotes</li>
<li>https://github.com/HarisIqbal88/PlotNeuralNet</li>
</ul>
<hr />
<h1 id="contributing">Contributing<a class="zola-anchor" href="#contributing" aria-label="Anchor link for: contributing">§</a>
</h1>
<p>I've tried to make sure that all the information in this repository is accurate, but if you find something that you think is wrong, please let me know by opening an issue.</p>
<p>This repository is still a work in progress, so if you find a bug, think there is something missing, or have any suggestions for new features, feel free to open an issue or a pull request. Feel free to use the library or code from it in your own projects, and if you feel that some code used in this project hasn't been properly accredited, please open an issue.</p>
<hr />
<h1 id="authors">Authors<a class="zola-anchor" href="#authors" aria-label="Anchor link for: authors">§</a>
</h1>
<ul>
<li><em>Bilal Khan</em></li>
</ul>
<hr />
<h1 id="license">License<a class="zola-anchor" href="#license" aria-label="Anchor link for: license">§</a>
</h1>
<p>This project is licensed under the CC-BY-SA-4.0 License - see the <a href="https://bkkaggle.github.io/zerm/ml-flight-rules/LICENSE">license</a> file for details</p>
<hr />
<h1 id="acknowledgements">Acknowledgements<a class="zola-anchor" href="#acknowledgements" aria-label="Anchor link for: acknowledgements">§</a>
</h1>
<ul>
<li><em>k88hudson</em> - <em>Parts of https://github.com/k88hudson/git-flight-rules were used in this repository</em></li>
</ul>
<p>This repository was inspired by https://github.com/k88hudson/git-flight-rules and copied over parts of it</p>

        </div>
        
    
</div></div>
			
    <div class="pagination">
        <div class="pagination__buttons">
            </div>
    </div>
<footer class="footer">
				<div class="footer__inner"><div class="copyright">
            <span>© 2020 <a href="https://github.com/ejmg/zerm">zerm</a> :: Powered by <a href="https://www.getzola.org/">Zola</a></span>
            <span>:: Theme made by <a href="https://github.com/ejmg">ejmg</a></span>
        </div>
    <script type="text/javascript" src="https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;zerm&#x2F;assets&#x2F;js&#x2F;main.js"></script>
</div>
				

			</footer></div>
	</body>
</html>
