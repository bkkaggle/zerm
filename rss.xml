<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Bilal&#x27;s Blog</title>
        <link>https://bkkaggle.github.io/blog</link>
        <description>My Blog</description>
        <generator>Zola</generator>
        <language>en</language>
        <atom:link href="https://bkkaggle.github.io/blog/rss.xml" rel="self" type="application/rss+xml"/>
        <lastBuildDate>Fri, 14 Aug 2020 00:00:00 +0000</lastBuildDate>
        <item>
            <title>Unorganized notes on Rust&#x27;s async primitives</title>
            <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/rust-async/</link>
            <guid>https://bkkaggle.github.io/blog/rust-async/</guid>
            <description>&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;some-definitions&quot;&gt;Some definitions&lt;a class=&quot;zola-anchor&quot; href=&quot;#some-definitions&quot; aria-label=&quot;Anchor link for: some-definitions&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;green threads: threads scheduled by vm or runtime&lt;&#x2F;li&gt;
&lt;li&gt;native threads are scheduled by os&lt;&#x2F;li&gt;
&lt;li&gt;runtime: the env where your code runs and the libraries it has access to (e.g. jvm, stdlib, malloc).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;why-use-async-over-os-provided-threads&quot;&gt;Why use async over OS-provided threads&lt;a class=&quot;zola-anchor&quot; href=&quot;#why-use-async-over-os-provided-threads&quot; aria-label=&quot;Anchor link for: why-use-async-over-os-provided-threads&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;native threads are expensive&lt;&#x2F;li&gt;
&lt;li&gt;the async runtime creates its own green threads from the os&#x2F;kernel and schedules access to them&lt;&#x2F;li&gt;
&lt;li&gt;it handles keeping track of the state of async functions&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&amp;quot;It is registering incoming Future requests and saves a pointer to the async function handler. It then triggers an event in the kernel. Once the I&#x2F;O operation is done, we call the pointer and execute the async method with the results from the I&#x2F;O (kernel).
For this, we need a Reactor, which notifies if data is coming over the network or a file writing operation is in progress, and an executor which takes this data and executes the async function (Future) with it.&amp;quot;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;manishearth.github.io&#x2F;blog&#x2F;2018&#x2F;01&#x2F;10&#x2F;whats-tokio-and-async-io-all-about&#x2F; (outdated?)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&amp;quot;You can wait() on a Future, which will block until you have a result, and you can also poll() it, asking it if it’s done yet (it will give you the result if it is).&amp;quot;&lt;&#x2F;p&gt;
&lt;p&gt;&amp;quot;You have to manually set up the Tokio event loop (the “scheduler”), but once you do you can feed it tasks which intermittently do I&#x2F;O, and the event loop takes care of swapping over to a new task when one is blocked on I&#x2F;O&amp;quot;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resouces&quot;&gt;Resouces&lt;a class=&quot;zola-anchor&quot; href=&quot;#resouces&quot; aria-label=&quot;Anchor link for: resouces&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;manishearth.github.io&#x2F;blog&#x2F;2018&#x2F;01&#x2F;10&#x2F;whats-tokio-and-async-io-all-about&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;rust-lang.github.io&#x2F;async-book&#x2F;01_getting_started&#x2F;01_chapter.html&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;levelup.gitconnected.com&#x2F;explained-how-does-async-work-in-rust-c406f411b2e2&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;softwareengineering.stackexchange.com&#x2F;questions&#x2F;304427&#x2F;what-really-is-the-runtime-environment&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;areweasyncyet.rs&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;www.arewewebyet.org&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;tokio.rs&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
        </item>
        <item>
            <title>Adafactor</title>
            <pubDate>Sun, 09 Aug 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/adafactor/</link>
            <guid>https://bkkaggle.github.io/blog/adafactor/</guid>
            <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;normalization&quot;&gt;Part 1: Normalization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;perplexity&quot;&gt;Part 2: Perplexity&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;initialization&quot;&gt;Part 3: Initialization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;memory-usage&quot;&gt;Part 4: GPU Memory Usage Breakdown&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;adafactor&quot;&gt;Part 5: Adafactor&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;purpose&quot;&gt;Purpose&lt;a class=&quot;zola-anchor&quot; href=&quot;#purpose&quot; aria-label=&quot;Anchor link for: purpose&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in machine learning, help me keep track of everything I&#x27;ve learned over the last three years, and to practice my Latex skills.&lt;&#x2F;p&gt;
&lt;p&gt;This is my fifth blog post in the series, and this time I&#x27;m taking some notes on the Adafactor optimization paper&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;adam-default-hyperparmeters&quot;&gt;Adam default hyperparmeters&lt;a class=&quot;zola-anchor&quot; href=&quot;#adam-default-hyperparmeters&quot; aria-label=&quot;Anchor link for: adam-default-hyperparmeters&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_1 = 0.9$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$\beta_2 = 0.999$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;linear warmup + inv sqrt decay&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The default&#x2F;initial lr for most experiments (the ones with a step size of $a_t = 0.1 * s_t$) is $1e-3$.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The authors use an inverse sqrt learning rate decay schedule for all experiments&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;warmup helps but not 100% necessary&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;adafactor&quot;&gt;Adafactor&lt;a class=&quot;zola-anchor&quot; href=&quot;#adafactor&quot; aria-label=&quot;Anchor link for: adafactor&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;Arxiv: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1804.04235&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Adafactor factorizes the second moment running averages of the gradient into row and column vectors. The matrix is then &amp;quot;divided by the sum of all entries&amp;quot; in the matrix to approximate the original matrix (Section 3)&lt;&#x2F;p&gt;
&lt;p&gt;By default, Adafactor doesn&#x27;t work if you don&#x27;t use a learning rate warmup. The authors tried using either only the row or column running averages. Using only the row running averages works almost just as well, but using only the column running averages doesn&#x27;t work at all.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Note: check if you can get away with only using row means without warmup&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Also, most of the Adafactor benchmarks in the paper were done with $\beta_1 = 0$, but IIRC some pretraining papers use it (?).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-problem-with-adam-adafactor-section-5&quot;&gt;The problem with Adam&#x2F;Adafactor (section 5)&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-problem-with-adam-adafactor-section-5&quot; aria-label=&quot;Anchor link for: the-problem-with-adam-adafactor-section-5&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Adam without $\beta_1$ works almost just as well as the original Adam implementation and it saves you quite a bit of memory. Note that this only holds true when you&#x27;re using a linear warmup with it. The problem is that using a fast ($0.9$) $\beta_2$ leads to Adam not converging no matter what, while using a slow ($0.999$) $\beta_2$ leads to your model only training well if you also use warmup_.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;You either need a $\beta_1$ of $0.9$ or warmup with a $\beta_2$ of $0.999$.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Here&#x27;s why: Using a slow $\beta_2$ means that second moment information is updated very slowly, leading to the current value of the running average matrix be out of date (this is shown in section 6, figure 1).&lt;&#x2F;p&gt;
&lt;p&gt;How do we fix this?&lt;&#x2F;p&gt;
&lt;p&gt;Well, the authors outline a few ways on how to do so...&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;1-gradient-clipping-section-6&quot;&gt;1. Gradient clipping (section 6)&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-gradient-clipping-section-6&quot; aria-label=&quot;Anchor link for: 1-gradient-clipping-section-6&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Having an out of date second moment estimator means that the raw gradient updates are often larger than they should be. A simple way to fix this would be to just scale down the magnitude of the update if it is larger than a particular &amp;quot;clipping&amp;quot; value. Empirically, update clipping works well when training without warmup but doesn&#x27;t match the original&#x27;s performance. The authors show that clipping at $1$ works well with both Adam and Adafactor.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;This is referred to in the paper as clipping, which it technically is, but acts more like gradient scaling since you&#x27;re really only scaling down the magnitude of the gradient update when it passes a particular &amp;quot;clipping&amp;quot; threshold.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;2-gradually-increasing-beta-2&quot;&gt;2. Gradually increasing $\beta_2$&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-gradually-increasing-beta-2&quot; aria-label=&quot;Anchor link for: 2-gradually-increasing-beta-2&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Add a schedule ($1 - t ^ {- x}$) that gradually increases the $\beta_2$ from $0$ to $1$. The quality of the results for when you&#x27;re training without warmup are very dependent on the value of $x$ that you choose. It seems like it stabilizes when you use this with update clipping, but the end result of using a $\beta_2$ schedule + update clipping is really no better than just update clipping.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Does this mean that a $\beta_2$ schedule is practically useless?&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;3-relative-update-size&quot;&gt;3. Relative update size&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-relative-update-size&quot; aria-label=&quot;Anchor link for: 3-relative-update-size&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Instead of hardcoded learning rate, multiply the gradient update by &amp;quot;the root-mean-square of its components, lower-bounded by a small constant 2&amp;quot;. In equation form (taken from Section 9, algorithm 4), it&#x27;s&lt;&#x2F;p&gt;
&lt;p&gt;$$
\epsilon_2 = 1e-3
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
p_t = \max(\epsilon_2, RMS(X_{t - 1}))
$$&lt;&#x2F;p&gt;
&lt;p&gt;In practice, the authors combine this with $\beta_2$ scheduling and update clipping.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;It would be nice to see how the relative update size method performs by itself without the scheduler or update clipping but 🤷‍♂️&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The authors try adding a $\beta_1$ of $0.9$, but that actually makes the results slightly worse.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusion&quot; aria-label=&quot;Anchor link for: conclusion&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Most codebases that I&#x27;ve seen (this includes all of mine too!) use the Adafactor optimizer don&#x27;t use it the way that the authors reccommend to use it in their paper. It&#x27;s pretty common to see people use Adafactor without a $\beta_1$, without the $\beta_2$ decay schedule, and with a simple linear warmup and decay.&lt;&#x2F;p&gt;
&lt;p&gt;For my future self looking back at this post to figure out what hyperparmeters they should use for Adafactor (or anyone else who&#x27;s reading this), here&#x27;s a summary for what hyperparameters to use with Adafactor:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;no warmup&lt;&#x2F;li&gt;
&lt;li&gt;no $\beta_1$&lt;&#x2F;li&gt;
&lt;li&gt;Adafactor&#x27;s built-in inv sqrt lr decay&lt;&#x2F;li&gt;
&lt;li&gt;update clipping at $1.0$&lt;&#x2F;li&gt;
&lt;li&gt;Relative update step sizes instead of a fixed learning rate&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
        </item>
        <item>
            <title>[Extremely WIP] Make your own fast PyTorch-style ML library in Rust</title>
            <pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/l2/</link>
            <guid>https://bkkaggle.github.io/blog/l2/</guid>
            <description>&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;img alt=&#x27;A code example showing how my library could be used&#x27; src=&#x27;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;bkkaggle&#x2F;L2&#x2F;master&#x2F;screenshot.png&#x27; width=&#x27;100%&#x27;&gt;&lt;&#x2F;img&gt;&lt;&#x2F;p&gt;
&lt;p align=&#x27;center&#x27;&gt;
    &lt;a href=&quot;&quot;&gt;
        &lt;img src=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;l2&#x2F;workflows&#x2F;Rust&#x2F;badge.svg&quot; alt=&quot;Rust: CI&quot;&gt;
    &lt;&#x2F;a&gt;
    &lt;a href=&quot;https:&#x2F;&#x2F;opensource.org&#x2F;licenses&#x2F;MIT&quot;&gt;
        &lt;img src=&quot;https:&#x2F;&#x2F;img.shields.io&#x2F;badge&#x2F;License-MIT-yellow.svg&quot; alt=&quot;License: MIT&quot;&gt;
    &lt;&#x2F;a&gt;
    &lt;a href=&quot;https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;l2&quot;&gt;
        &lt;img alt=&quot;crates.io l2 badge&quot; src=&quot;http:&#x2F;&#x2F;meritbadge.herokuapp.com&#x2F;l2&quot;&gt;
    &lt;&#x2F;a&gt;
    &lt;a href=&quot; https:&#x2F;&#x2F;docs.rs&#x2F;l2&quot;&gt;
        &lt;img alt=&quot;docs.rs l2 badge&quot; src=&quot;https:&#x2F;&#x2F;docs.rs&#x2F;l2&#x2F;badge.svg&quot;&gt;
    &lt;&#x2F;a&gt;
&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;tl-dr&quot;&gt;&lt;a name=&#x27;tldr&#x27; href=&#x27;#tldr&#x27;&gt;TL;DR&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#tl-dr&quot; aria-label=&quot;Anchor link for: tl-dr&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This blog post shows you, step-by-step, how to build a fast &lt;a href=&quot;https:&#x2F;&#x2F;pytorch.org&#x2F;&quot;&gt;PyTorch&lt;&#x2F;a&gt;-style machine learning library in the &lt;a href=&quot;https:&#x2F;&#x2F;www.rust-lang.org&#x2F;&quot;&gt;Rust programming language&lt;&#x2F;a&gt;. This blog post is based on a library called &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;L2&quot;&gt;L2&lt;&#x2F;a&gt; that I finished working on a while ago.&lt;&#x2F;p&gt;
&lt;p&gt;I &lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#resources&quot;&gt;compiled&lt;&#x2F;a&gt; quite a long list of blog posts, articles, and GitHub repos that I found useful when I was working on L2, so take a look at that if that&#x27;s the type of stuff you&#x27;re interested in.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Disclaimers&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;L2 is a small project I was working on during the summer before uni for fun &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, so don&#x27;t expect it to be production-ready or bug-free. &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m going to assume that everyone who&#x27;s reading this knows or uses Rust relatively well and is familiar with how PyTorch and TF work at a high level. If you want to learn about these topics or just brush up on some things that you aren&#x27;t 💯 clear on, try looking through my &lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#resources&quot;&gt;resources&lt;&#x2F;a&gt; section.&lt;&#x2F;p&gt;
&lt;p&gt;L2 is surprisingly fast especially since I didn&#x27;t try very hard to optimize all the operators, it&#x27;s usually only less than one order of magnitude slower than PyTorch in most of the benchmarks that I ran. L2 only supports a CPU backend at the moment since I&#x27;m not familiar enough with Rust to start working with CUDA and cuDNN. So far, it doesn&#x27;t have any PyTorch-style high level abstractions that are really useful for machine learning like PyTorch&#x27;s &lt;code&gt;Parameter&lt;&#x2F;code&gt;, &lt;code&gt;Layer&lt;&#x2F;code&gt;, or &lt;code&gt;Module&lt;&#x2F;code&gt; classes. There might still be some bugs in the transpose operators and calling &lt;code&gt;.backward()&lt;&#x2F;code&gt; on broadcasted tensors. The autograd system won&#x27;t automatically clear unused buffers once they&#x27;ve been used so this won&#x27;t be as memory efficient as PyTorch.&lt;&#x2F;p&gt;
&lt;p&gt;I wrote dozens of tests and benchmarks to make sure that L2 was working properly when I was developing it. I&#x27;m going to be omitting tests in this blog post and instead just going to show some example code in &lt;code&gt;src&#x2F;bin&#x2F;main.rs&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;table-of-contents&quot;&gt;&lt;a name=&#x27;toc&#x27; href=&#x27;#toc&#x27;&gt;Table of Contents&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#table-of-contents&quot; aria-label=&quot;Anchor link for: table-of-contents&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;If you just want to skip to the code part of the tutorial, click &lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#baseline&quot;&gt;here&lt;&#x2F;a&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#tldr&quot;&gt;TL;DR&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#toc&quot;&gt;Table of Contents&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#background&quot;&gt;Background&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#getstarted&quot;&gt;Part 1: Let&#x27;s get started&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#baseline&quot;&gt;Part 2: A simple baseline&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#broadcasting&quot;&gt;Part 3: Broadcasting&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#ops&quot;&gt;Part 4: Ops&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#autograd&quot;&gt;Part 5: Autograd&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#advancedops&quot;&gt;Part 6: Advanced Ops&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#future&quot;&gt;Future work&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#conclusion&quot;&gt;Conclusion&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#resources&quot;&gt;Resources&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;l2&#x2F;#references&quot;&gt;References&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;background&quot;&gt;&lt;a name=&#x27;background&#x27; href=&#x27;#background&#x27;&gt;Background&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#background&quot; aria-label=&quot;Anchor link for: background&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Last summer &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, I &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;L2&#x2F;tree&#x2F;c%2B%2B&quot;&gt;wrote&lt;&#x2F;a&gt; a machine learning library as a way of getting better at using C++. The library wasn&#x27;t really that advanced (I didn&#x27;t have an autograd system like PyTorch does, instead I just did the backprop calculations by hand for each layer) or very fast (I pretty much passed everything by value and didn&#x27;t really put a focus on making my code fast and performant), but it was a good way at getting a lot of experience working with a lower level language like c++ that I&#x27;d never used before and I learned a lot about how machine learning libraries like Pytorch and Tensorflow work behind the scenes.&lt;&#x2F;p&gt;
&lt;p&gt;This summer, I did a complete rewrite of L2, this time in Rust, with a focus on making it as close to Pytorch as I could (speed and feature wise) and got to learn about and implement a lot of interesting and cool features that are used in all the popular machine learning libraries today.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m pretty satisfied with how L2 turned out, here&#x27;s the pitch I wrote for it on my GitHub repo:&lt;&#x2F;p&gt;
&lt;p&gt;L2 is a Pytorch-style Tensor+Autograd library written in Rust. It contains a multidimensional array class, &lt;code&gt;Tensor&lt;&#x2F;code&gt;, with support for strided arrays, numpy-style array slicing, broadcasting, and most major math operations (including fast, BLAS-accelerated matrix multiplication!). On top of this, L2 has a built-in efficient graph-based autograd engine that keeps track of all operations performed on a tensor and topologically sorts and traverses the graph to compute the gradients.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m also pretty happy with how the user-facing API of the library turned out:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;l2::tensor::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; x: Tensor &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::normal(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; y: Tensor &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::normal(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; z: Tensor &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;l2::matmul(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;x, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;y)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

z.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;backward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, z);
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;let-s-get-started&quot;&gt;&lt;a name=&#x27;getstarted&#x27; href=&#x27;#getstarted&#x27;&gt;Let&#x27;s get started&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#let-s-get-started&quot; aria-label=&quot;Anchor link for: let-s-get-started&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;So let&#x27;s get started. I&#x27;m pretty much just copying down the installation instructions from the official &lt;a href=&quot;https:&#x2F;&#x2F;www.rust-lang.org&#x2F;learn&#x2F;get-started&quot;&gt;get started&lt;&#x2F;a&gt; guide, so take a look at that if you want.&lt;&#x2F;p&gt;
&lt;p&gt;Install rustup to your computer:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro ~ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;% curl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --proto &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;=https&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --tlsv1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; -sSf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; https:&#x2F;&#x2F;sh.rustup.rs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;sh
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Switch the default rust version to nightly:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro ~ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;% rustup default nightly
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I&#x27;ll be using my preferred text editor &lt;a href=&quot;https:&#x2F;&#x2F;code.visualstudio.com&#x2F;&quot;&gt;VScode&lt;&#x2F;a&gt; in this post, but feel free to use whatever editor you prefer.&lt;&#x2F;p&gt;
&lt;p&gt;I highly recommend using the (soon to become) official Rust extension for VScode, &lt;a href=&quot;https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=matklad.rust-analyzer&quot;&gt;Rust-analyzer&lt;&#x2F;a&gt; instead of the old &lt;a href=&quot;https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=rust-lang.rust&quot;&gt;RLS&lt;&#x2F;a&gt; extension. Just install it from the marketplace and you should be ready to go.&lt;&#x2F;p&gt;
&lt;p&gt;Create a new Rust library called &lt;code&gt;l2&lt;&#x2F;code&gt; with:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro ~ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;% cargo new l2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --lib
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Install clippy (Rust&#x27;s official linter):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;You can take a look at all the lint rules and how to fix each one &lt;a href=&quot;https:&#x2F;&#x2F;rust-lang.github.io&#x2F;rust-clippy&#x2F;master&#x2F;index.html&quot;&gt;here&lt;&#x2F;a&gt;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro ~ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;% rustup component add clippy
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;And change rust-analyzer to use clippy as its default linter by creating a &lt;code&gt;.vscode&#x2F;settings.json&lt;&#x2F;code&gt; file and pasting this in it.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
	&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;rust-analyzer.checkOnSave.command&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;clippy&amp;quot;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;For debugging support, I use the &lt;a href=&quot;https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=vadimcn.vscode-lldb&quot;&gt;Code-LLDB&lt;&#x2F;a&gt; extension, so install that as well.&lt;&#x2F;p&gt;
&lt;p&gt;create a &lt;code&gt;.vscode&#x2F;launch.json&lt;&#x2F;code&gt; file and paste this into it:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
	&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;version&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;0.2.0&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
	&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;configurations&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: [
		{
			&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;type&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;lldb&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
			&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;request&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;launch&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
			&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;name&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Debug&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
			&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;program&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;${workspaceRoot}&#x2F;target&#x2F;debug&#x2F;main&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
			&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;args&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: [],
			&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;cwd&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;${workspaceRoot}&amp;quot;
		&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
	]
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Add a rust binary target &lt;code&gt;src&#x2F;bin&#x2F;main.rs&lt;&#x2F;code&gt; that will be linked against our library at &lt;code&gt;src&#x2F;lib.rs&lt;&#x2F;code&gt;. Your project should now have a directory structure like this:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.git&#x2F;
.vscode&#x2F;
    settings.json
    launch.json
src&#x2F;
    bin&#x2F;
        main.rs
    lib.rs
target&#x2F;
.gitignore
Cargo.lock
Cargo.toml
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;We&#x27;ll code up the library in &lt;code&gt;src&#x2F;lib.rs&lt;&#x2F;code&gt; and any other files in the &lt;code&gt;src&#x2F;&lt;&#x2F;code&gt; directory. We&#x27;ll use &lt;code&gt;src&#x2F;bin&#x2F;main.rs&lt;&#x2F;code&gt; to interact with L2 as you would when using it in your own project.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;a-simple-baseline&quot;&gt;&lt;a name=&#x27;baseline&#x27; href=&#x27;#baseline&#x27;&gt;A simple baseline&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#a-simple-baseline&quot; aria-label=&quot;Anchor link for: a-simple-baseline&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Ok, so let&#x27;s start by creating a simple &lt;code&gt;Tensor&lt;&#x2F;code&gt; struct and defining a few simple operations on it.&lt;&#x2F;p&gt;
&lt;p&gt;A &lt;code&gt;Tensor&lt;&#x2F;code&gt; is really just a multidimensional array. For this library, we&#x27;ll keep it simple and restrict tensors to have at most $2$ dimensions (You&#x27;ll see why later).&lt;&#x2F;p&gt;
&lt;p&gt;The simplest way to store a multidimensional array of say, dimensions $m \times n$ would be to create an array of length $m$ that holds a pointers to $m$ distinct arrays of length $n$, each holding the elements of a single row. This would be the simplest way to represent a &lt;code&gt;Tensor&lt;&#x2F;code&gt; but isn&#x27;t really optimal when you need to create and process large &lt;code&gt;Tensors&lt;&#x2F;code&gt; quickly.&lt;&#x2F;p&gt;
&lt;p&gt;Most (if not all) people use &lt;em&gt;strided arrays&lt;&#x2F;em&gt;, where elements of a multidimensional array are layed out contigously in memory (the $m \times n$ &lt;code&gt;Tensor&lt;&#x2F;code&gt; would then be represented as single array of length $m * n$).&lt;&#x2F;p&gt;
&lt;p&gt;Take a look at http:&#x2F;&#x2F;blog.ezyang.com&#x2F;2019&#x2F;05&#x2F;pytorch-internals&#x2F; for a good in-depth look into how PyTorch uses strided arrays. I&#x27;ll summarize the main parts below:&lt;&#x2F;p&gt;
&lt;p&gt;Say you have a $2 \times 2$ &lt;code&gt;Tensor&lt;&#x2F;code&gt; like this:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{bmatrix}
1 &amp;amp; 2 \
3 &amp;amp; 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;p&gt;If you wanted to represent this as a strided array, you could either store them in row-major or column-major order, storing either values from a single row or column contigously in memory (the same idea would still apply if you have a &lt;code&gt;Tensor&lt;&#x2F;code&gt; of more dimensions):&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text {row-major:}
\begin{bmatrix}
1, 2, 3, 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text {column-major:}
\begin{bmatrix}
1, 3, 2, 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Most machine learning libraries like Numpy, PyTorch, and Tensorflow store Tensors in row-major order by default. This lets you quickly access the next element in the same row just by moving one element to the right in the &lt;code&gt;Tensor&lt;&#x2F;code&gt;. Column-major order isn&#x27;t as commonly used, the only time I had to use it when I was integrating a BLAS library written in very optimized Fortran into L2 in order to use its super fast matrix multiplication implementations (using BLAS sped up my matrix multiplication code by about 200 times IIRC). &lt;&#x2F;p&gt;
&lt;p&gt;The choice of whether to store your data in column-major or row-major order depends on whether you prefer to have contigous access to elements in the first or last dimensions of your &lt;code&gt;Tensor&lt;&#x2F;code&gt;. For example, if you store a batch of $N$ three-channel image in a &lt;code&gt;Tensor&lt;&#x2F;code&gt; of dimensions ($256$, $256$, $3$), you would be able to either access the channels or the batch dimension contigously (i.e. have the elements in that dimension be next to each other in memory) depending on whether it&#x27;s stored in row-major or column-major order.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;em&gt;stride&lt;&#x2F;em&gt; for each dimension of a &lt;em&gt;strided array&lt;&#x2F;em&gt; is the number of elements you want to skip between neighboring elements of a &lt;code&gt;Tensor&lt;&#x2F;code&gt; in a particular dimension. For example, our original &lt;code&gt;Tensor&lt;&#x2F;code&gt; of shape $\begin{bmatrix} 2, 2 \end{bmatrix}$ has strides of $\begin{bmatrix} 2, 1 \end{bmatrix}$.&lt;&#x2F;p&gt;
&lt;p&gt;This means that if we want to advance one element in the column dimension (from the element $1$ to the element $3$) of the &lt;em&gt;logical&lt;&#x2F;em&gt; &lt;code&gt;Tensor&lt;&#x2F;code&gt;, we need to advance $2$ elements at a time in the &lt;em&gt;strided&lt;&#x2F;em&gt; &lt;code&gt;Tensor&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text {Logical Tensor:}&lt;&#x2F;p&gt;
&lt;p&gt;\begin{bmatrix}
1 &amp;amp; \color{gray} 2 \
3 &amp;amp; \color{gray} 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text {Strided Tensor:}
\begin{bmatrix}
1, \color{gray} 2, \color{white} 3, \color{gray} 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;p&gt;The same would be true for the other dimensions as well. If we want to advance one element in the row dimension (from the element $1$ to the element $2$) of the &lt;em&gt;logical&lt;&#x2F;em&gt; &lt;code&gt;Tensor&lt;&#x2F;code&gt;, we would advance $1$ element in the &lt;em&gt;strided&lt;&#x2F;em&gt; &lt;code&gt;Tensor&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text {Logical Tensor:}&lt;&#x2F;p&gt;
&lt;p&gt;\begin{bmatrix}
1 &amp;amp;  2 \
\color{gray} 3 &amp;amp; \color{gray} 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text {Strided Tensor:}
\begin{bmatrix}
1,  2, \color{gray} 3, 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;p&gt;If we wanted to get the &lt;em&gt;physical&lt;&#x2F;em&gt; location in memory of a specific element in the &lt;code&gt;Tensor&lt;&#x2F;code&gt; from the &lt;em&gt;logical&lt;&#x2F;em&gt; location, we can simply &amp;quot;multiply each index with the respective stride for that dimension, and sum them all together&amp;quot; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;. So for an example, if we want to get the &lt;em&gt;physical&lt;&#x2F;em&gt; index of the element at the &lt;em&gt;logical&lt;&#x2F;em&gt; indices $[ 1, 1]$, we would calculate it like this:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text{logical index: } [\color{red} 1, \color{blue} 1 \color{white}] \ \text{strides: } [\color{red} 2, \color{blue} 1 \color{white}]
$$&lt;&#x2F;p&gt;
&lt;br &#x2F;&gt;
&lt;p&gt;$$
\text{physical index} = {\color{blue} 1} {\color{white} *} {\color{blue} 1} {\color{white} +} {\color{red} 1} {\color{white} *} {\color{red} 2} 
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text{physical index} =  1 + 2 = 3
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text {element at [1, 1]} =
\begin{bmatrix}
\color{gray} 1,  2,  3, \color{white} 4
\end{bmatrix}
$$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;So now that we have that out of the way, let&#x27;s start writing some code.&lt;&#x2F;p&gt;
&lt;p&gt;In this section, we&#x27;ll make a basic &lt;code&gt;Tensor&lt;&#x2F;code&gt; struct the just creates and stores a strided array. We&#x27;ll also take advantage of Rust&#x27;s excellent error handling primitives to add robust error handling and add pretty printing of our &lt;code&gt;Tensors&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s make a new file at &lt;code&gt;src&#x2F;tensor.rs&lt;&#x2F;code&gt; to house our &lt;code&gt;Tensor&lt;&#x2F;code&gt; struct.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use crate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;::errors::TensorError;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;std::fmt;

#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, PartialEq)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub struct &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;,
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;,
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;strides&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; Pretty print Tensors
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; graph &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;format!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{:?} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{:?}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.data, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape);

        write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, graph)
    }
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Clone &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;Self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
        Tensor::new(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
    }
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; Calculate the number of elements in a tensor from the shape
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;calc_tensor_len_from_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]) -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; length &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; shape {
            length &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; i;
        }

        length
    }

    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; calculate the strides for each dimension from the shape
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;calc_strides_from_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]) -&amp;gt; Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; strides &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec::with_capacity(shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;());

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; current_stride &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;rev&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
            strides.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;insert&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, current_stride);
            current_stride &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; i;
        }

        strides
    }

    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; Create a new tensor from some data with a specific shape
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;new&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]) -&amp;gt; Result&amp;lt;Tensor,
        TensorError&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::calc_tensor_len_from_shape(shape)
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&amp;amp; !&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;is_empty&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            Ok(Tensor {
                data,
                shape: shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_vec&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
                strides: Tensor::calc_strides_from_shape(shape),
            })
        } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            Err(TensorError::InvalidTensor)
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s add the error handling struct &lt;code&gt;TensorError&lt;&#x2F;code&gt; to &lt;code&gt;src&#x2F;errors.rs&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;errors.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;std::error;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;std::fmt;

#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, Clone)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    MaxDimsError,
    InvalidTensor,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            TensorError::MaxDimsError &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(
                f,
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;L2 currently only supports
                tensors with up to 2 dimensions&amp;quot;
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
            TensorError::InvalidTensor &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Invalid parameters for Tensor&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
        }
    }
}

&lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; This is important for other errors to wrap this one.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;error::Error &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;source&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) -&amp;gt; Option&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(dyn error::Error + &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;static&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; Generic error, underlying cause isn&amp;#39;t tracked.
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;None
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Add the relevant imports to &lt;code&gt;src&#x2F;lib.rs&lt;&#x2F;code&gt; and &lt;code&gt;src&#x2F;bin&#x2F;main.rs&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;lib.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub mod &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;errors;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub mod &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;tensor;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;And finally, lets test out our library by creating a simple &lt;code&gt;Tensor&lt;&#x2F;code&gt; in &lt;code&gt;src&#x2F;bin&#x2F;main.rs&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;bin&#x2F;main.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;l2::errors::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;l2::tensor::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() -&amp;gt; Result&amp;lt;(), TensorError&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;])&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

    println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, x);

    Ok(())
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;and run &lt;code&gt;cargo run&lt;&#x2F;code&gt; to see the output.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; l2 % cargo run
   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Compiling&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; l2 v0.1.0 (&#x2F;Users&#x2F;bilal&#x2F;Desktop&#x2F;l2)
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Finished&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; dev &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;unoptimized + debuginfo&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; target(s) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; 1.02s
     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Running &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;`&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;target&#x2F;debug&#x2F;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;`

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 2.0, 3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;🎉! you now have a very simple machine learning library. Now that we have the general structure of the library set up, I&#x27;ll be speeding up the pace of this blog post.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;broadcasting&quot;&gt;&lt;a name=&#x27;broadcasting&#x27; href=&#x27;#broadcasting&#x27;&gt;Broadcasting&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#broadcasting&quot; aria-label=&quot;Anchor link for: broadcasting&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Storing a bunch of values in a &lt;code&gt;Tensor&lt;&#x2F;code&gt; is useless if we can&#x27;t operate over them.&lt;&#x2F;p&gt;
&lt;p&gt;Before we can create some &lt;code&gt;Tensor&lt;&#x2F;code&gt;—&lt;code&gt;Tensor&lt;&#x2F;code&gt; operations, we need to implement &lt;em&gt;broadcasting&lt;&#x2F;em&gt;. I won&#x27;t go into what exactly broadcasting is here, since there are a lot of better explanations out there. Numpy&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;numpy.org&#x2F;doc&#x2F;stable&#x2F;user&#x2F;basics.broadcasting.html#general-broadcasting-rules&quot;&gt;documentation&lt;&#x2F;a&gt; on their broadcasting rules is a good technical explanation.&lt;&#x2F;p&gt;
&lt;p&gt;One thing the numpy docs don&#x27;t go into is how to implement broadcasting. I struggled with how to best implement it when I was making my original C++ version of the library last year &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;5&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, but I eventually settled on the pretty simple and efficient solution of adding dimensions of size $1$ to the tensor with the fewer number of dimensions to make their shapes broadcastable, then setting the shapes and strides of a broadcasted dimension to $1$ and $0$ respectively. By doing it this way, the &lt;code&gt;Tensor&lt;&#x2F;code&gt; would use the same value across all values of a specific dimension.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;allow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(clippy::ptr_arg, clippy::type_complexity)]
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;broadcast&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;lhs_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;,
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;rhs_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;,
    ) -&amp;gt; Result&amp;lt;(Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;, Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;, Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;), TensorError&amp;gt; {

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; prepend lhs_shape with ones if the length of it is smaller than rhs_shape
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs_shape &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; ones &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; rhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()];
            [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;ones[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;lhs_shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]].&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;concat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            lhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        };

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; prepend rhs_shape with ones if the length of it is smaller than lhs_shape
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs_shape &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; ones &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; lhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()];
            [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;ones[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;rhs_shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]].&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;concat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            rhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        };

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; broadcasted_shape: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec::with_capacity(lhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;());
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; broadcasted_lhs_strides: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::calc_strides_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;lhs_shape);
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; broadcasted_rhs_strides: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::calc_strides_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;rhs_shape);

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; go over each dimension of lhs and rhs
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(i, (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;lhs, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;rhs)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;zip&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(rhs_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;enumerate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; if both dimensions are the same,
            &#x2F;&#x2F; the dimension of the broadcasted shape
            &#x2F;&#x2F; for this dimension doesn&amp;#39;t change
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;==&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs {
                broadcasted_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(lhs);

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; if the size of this dimension of lhs
            &#x2F;&#x2F; is 1, set the strides of lhs for that
            &#x2F;&#x2F; dimension to 0
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                broadcasted_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(rhs);
                broadcasted_lhs_strides[i] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; if the size of this dimension of rhs
            &#x2F;&#x2F; is 1, set the strides of rhs for
            &#x2F;&#x2F; that dimension to 0
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                broadcasted_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(lhs);
                broadcasted_rhs_strides[i] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; return an error if the tensors
            &#x2F;&#x2F; aren&amp;#39;t broadcastable
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Err(TensorError::BroadcastError);
            }
        }

        Ok((
            broadcasted_shape,
            broadcasted_lhs_strides,
            broadcasted_rhs_strides,
        ))
    }
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;errors.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
    BroadcastError,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError::BroadcastError &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Shapes are not broadcastable&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now that we&#x27;ve implemented broadcasting, we&#x27;ll add some operations over &lt;code&gt;Tensor&lt;&#x2F;code&gt;s in the next section so we can try it out.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;ops&quot;&gt;&lt;a name=&#x27;ops&#x27; href=&#x27;#ops&#x27;&gt;Ops&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#ops&quot; aria-label=&quot;Anchor link for: ops&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s start by defining a struct &lt;code&gt;Ops&lt;&#x2F;code&gt; that we&#x27;ll use to keep track of what operation should be performed on a tensor.&lt;&#x2F;p&gt;
&lt;p&gt;We&#x27;ll be storing the &lt;code&gt;Tensor&lt;&#x2F;code&gt;—&lt;code&gt;Tensor&lt;&#x2F;code&gt; ops in an enum called &lt;code&gt;TensorOp&lt;&#x2F;code&gt;, but we&#x27;ll wrap that in the &lt;code&gt;Ops&lt;&#x2F;code&gt; enum so we can add more different kinds of ops in the future (slicing, matmuls, transposes, etc).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;ops.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;std::fmt;

#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, PartialEq)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorOp {
    Add,
    Sub,
    Mul,
    Div,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorOp {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter&amp;lt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            TensorOp::Add &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Add&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
            TensorOp::Sub &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Subtract&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
            TensorOp::Mul &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Multiply&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
            TensorOp::Div &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Divide&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
        }
    }
}

#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, PartialEq)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    TensorOp(TensorOp),
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter&amp;lt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            Ops::TensorOp(tensor_op) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, tensor_op),
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;And now let&#x27;s add an &lt;code&gt;OpError&lt;&#x2F;code&gt; variant to our &lt;code&gt;TensorError&lt;&#x2F;code&gt; enum&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;errors.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
    OpError,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError::OpError &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Tensors cannot be operated on&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
        }
    }
}

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now that we have an &lt;code&gt;Ops&lt;&#x2F;code&gt; enum that we can use, let&#x27;s integrate it into &lt;code&gt;Tensor&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;std::ops::{Add, Div, Mul, Sub};

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use crate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;::ops::{Ops, TensorOp};

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; calculate the physical index of an element
    &#x2F;&#x2F; from a `Tensor`&amp;#39;s logical indices and strides
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;get_physical_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;logical_indices&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;],
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;strides&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]) -&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; physical_idx &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(i, idx) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; logical_indices.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;enumerate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
            physical_idx &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; idx &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; strides[i];
        }

        physical_idx
    }

    &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; perform op on lhs and rhs
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;lhs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;rhs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops) -&amp;gt;
        Result&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, TensorError&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; op {
            Ops::TensorOp(TensorOp::Add) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs),
            Ops::TensorOp(TensorOp::Sub) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs),
            Ops::TensorOp(TensorOp::Mul) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs),
            Ops::TensorOp(TensorOp::Div) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs),
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_ =&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Err(TensorError::OpError),
        }
    }

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;tensor_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Ops) -&amp;gt;
        Result&amp;lt;Tensor, TensorError&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; broadcast tensors
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(new_shape, lhs_strides, rhs_strides) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::broadcast(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;other.shape)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;is_empty&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;|| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Err(TensorError::MaxDimsError);
        }

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; allocate a new vector for the result of the op
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_data: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec::with_capacity(Tensor::
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;calc_tensor_len_from_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape));

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; call `Tensor::op()` on each element in the tensor
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; op_result &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::op(
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.data[Tensor::
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;get_physical_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[i], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;lhs_strides)],
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;other.data[Tensor::
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;get_physical_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[i], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;rhs_strides)],
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;op,
                )&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

                new_data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(op_result);
            } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; j &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] {
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; op_result &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::op(
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.data[Tensor::
                            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;get_physical_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[i, j], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;lhs_strides)],
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;other.data[Tensor::
                            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;get_physical_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[i, j], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;rhs_strides)],
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;op,
                    )&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

                    new_data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(op_result);
                }
            }
        }

        Tensor::new(new_data, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape)
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s also overload Rust&#x27;s built-in &lt;code&gt;Add&lt;&#x2F;code&gt;, &lt;code&gt;Sub&lt;&#x2F;code&gt;, &lt;code&gt;Mul&lt;&#x2F;code&gt;, and &lt;code&gt;Div&lt;&#x2F;code&gt; traits for &lt;code&gt;Tensor&lt;&#x2F;code&gt; so we can use the native plus and minus operators on &lt;code&gt;Tensor&lt;&#x2F;code&gt;s: &lt;code&gt;let c: Tensor = a + b;&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Add &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;add&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;tensor_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(other,
            Ops::TensorOp(TensorOp::Add)) {
            Ok(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; t,
            Err(e) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;panic!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;{}&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, e),
        }
    }
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Sub &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;sub&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;tensor_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(other,
            Ops::TensorOp(TensorOp::Sub)) {
            Ok(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; t,
            Err(e) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;panic!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;{}&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, e),
        }
    }
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Mul &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;mul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;tensor_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(other,
            Ops::TensorOp(TensorOp::Mul)) {
            Ok(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; t,
            Err(e) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;panic!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;{}&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, e),
        }
    }
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Div &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;div&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;tensor_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(other,
            Ops::TensorOp(TensorOp::Div)) {
            Ok(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; t,
            Err(e) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;panic!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;{}&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, e),
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now that we have the ops implemented, all we need to do now is to add &lt;code&gt;ops.rs&lt;&#x2F;code&gt; as a module in &lt;code&gt;lib.rs&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;lib.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mod &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;ops;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;and let&#x27;s try it out:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;bin&#x2F;main.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() -&amp;gt; Result&amp;lt;(), TensorError&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;])&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;])&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;b;

    println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, c);

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Just run &lt;code&gt;cargo run&lt;&#x2F;code&gt; in your terminal to see the results:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; l2 % cargo run
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Finished&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; dev &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;unoptimized + debuginfo&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; target(s) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; 0.75s
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Running &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;`&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;target&#x2F;debug&#x2F;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;`
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 4.0, 9.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1, 3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;autograd&quot;&gt;&lt;a name=&#x27;autograd&#x27; href=&#x27;#autograd&#x27;&gt;Autograd&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#autograd&quot; aria-label=&quot;Anchor link for: autograd&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;We need to implement one more operator before we can start working on our autograd system: &lt;code&gt;.pow()&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;ops.rs
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, PartialEq)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
    Pow(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter&amp;lt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;) -&amp;gt;
        fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops::Pow(pow) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Pow: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, pow),
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;pow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;exp&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) -&amp;gt;
        Result&amp;lt;Tensor, TensorError&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_data &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(|&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;val&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;| val.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;powf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(exp)).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;collect&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        Tensor::new(new_data, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape)
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now that that&#x27;s out of the way, lets go on to the fun stuff: Autograd. We&#x27;ll be implementing a simple but efficient graph based autograd system similar to what PyTorch uses.&lt;&#x2F;p&gt;
&lt;p&gt;Every &lt;code&gt;Tensor&lt;&#x2F;code&gt; struct will hold field(s) that hold references to its parent(s) as well as a field holding the op that was used to create it and a &lt;code&gt;Vec&amp;lt;f32&amp;gt;&lt;&#x2F;code&gt; to store its gradient.&lt;&#x2F;p&gt;
&lt;p&gt;Since we&#x27;re using Rust, a language famous for its focus on guaranteeing memory safety at compile time, we&#x27;ll need to put a little bit of thought into how to implement all this. A &lt;code&gt;Tensor&lt;&#x2F;code&gt; may or may not have either one or two immutable references to its parent &lt;code&gt;Tensors&lt;&#x2F;code&gt; and also may or may not have been created using an &lt;code&gt;Op&lt;&#x2F;code&gt;. We also need a way to compute a &lt;code&gt;Tensor&lt;&#x2F;code&gt;&#x27;s gradient wrt to its children.&lt;&#x2F;p&gt;
&lt;p&gt;To make everything simple, we&#x27;ll wrap the gradient of a &lt;code&gt;Tensor&lt;&#x2F;code&gt; in a &lt;code&gt;RefCell&lt;&#x2F;code&gt; so we can safely change its value by calling &lt;code&gt;.borrow_mut()&lt;&#x2F;code&gt; without needing to keep a mutable reference to it. &lt;em&gt;Keeping a mutable reference might not be possible if one &lt;code&gt;Tensor&lt;&#x2F;code&gt; has two distinct children — Rust only allows one mutable reference to be in scope at a time.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s get started by adding a few field to our original &lt;code&gt;Tensor&lt;&#x2F;code&gt; struct:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;use &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;std::cell::RefCell;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub struct &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
    ...

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;track_grad&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;bool&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;lhs_parent&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Option&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;gt;,
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;rhs_parent&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Option&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;gt;,
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;create_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Option&amp;lt;Ops&amp;gt;,
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derivative&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: RefCell&amp;lt;Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;&amp;gt;,
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you add this and press &lt;code&gt;⌘-S&lt;&#x2F;code&gt;, you&#x27;ll probably see that rust-analyzer starts throwing out dozens of warnings and errors. Now that we&#x27;re storing references to other &lt;code&gt;Tensor&lt;&#x2F;code&gt;s inside our &lt;code&gt;Tensor&lt;&#x2F;code&gt;, we need to add lifetime parameters to our struct so the Rust compiler can make sure that these references don&#x27;t go out of scope during any part of our program.&lt;&#x2F;p&gt;
&lt;p&gt;If you&#x27;re using VSCode with rust-analyzer like I am, fixing lifetime errors in Rust is pretty painless when the compiler literally guides you through it and tells you where the problem is, why it exists, and how to fix it :)&lt;&#x2F;p&gt;
&lt;p&gt;Here&#x27;s a diff showing the changes that I had to make:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub struct &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; struct Tensor&amp;lt;&amp;#39;a&amp;gt; {
    ...

-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;lhs_parent&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Option&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor,
&lt;&#x2F;span&gt;&lt;span style=&quot;background-color:#e05252;color:#ffffff;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;    rhs_parent: Option&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;    lhs_parent: Option&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;&amp;gt;,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;    rhs_parent: Option&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;&amp;gt;,
&lt;&#x2F;span&gt;&lt;span style=&quot;background-color:#e05252;color:#ffffff;&quot;&gt;}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;

-impl fmt::Display for Tensor {
+impl&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; fmt::Display for Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    ...
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Clone &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Clone &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;new&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;-&amp;gt; Result&amp;lt;Tensor, TensorError&amp;gt; {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;new&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;-&amp;gt; Result&amp;lt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;, TensorError&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::calc_tensor_len_from_shape(shape)
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&amp;amp; !&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;is_empty&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            Ok(Tensor {
                data,
                shape: shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_vec&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
                strides: Tensor::calc_strides_from_shape(shape),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;                track_grad: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;                create_op: None,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;                derivative: RefCell::new(
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; Tensor::calc_tensor_len_from_shape(shape)]),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;                lhs_parent: None,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;                rhs_parent: None,
            })
        } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            Err(TensorError::InvalidTensor)
        }
    }

}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Add &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Add &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;add&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;add&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor) -&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Sub &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Sub &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;sub&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;sub&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor) -&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Mul &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Mul &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;mul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;mul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor) -&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Div &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Div &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;type &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Output &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;;

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;div&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor) -&amp;gt; Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;div&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor) -&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;em&gt;Note: you might notice that you don&#x27;t need to declare a lifetime parameter on &lt;code&gt;other&lt;&#x2F;code&gt; in the &lt;code&gt;impl&lt;&#x2F;code&gt; blocks for &lt;code&gt;Add&lt;&#x2F;code&gt;, &lt;code&gt;Sub&lt;&#x2F;code&gt;, &lt;code&gt;Mul&lt;&#x2F;code&gt;, and &lt;code&gt;Div&lt;&#x2F;code&gt;. I&#x27;m including the lifetime parameters here since we&#x27;ll need to add them in the next step since the output of &lt;code&gt;Tensor::tensorop()&lt;&#x2F;code&gt; will store a reference to &lt;code&gt;other&lt;&#x2F;code&gt; as one of its parents. This means that lifetime parameters will be needed to make sure that the reference to the parent remains valid for the full lifetime of the output.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Now that we&#x27;ve satisfied the Rust compiler, let&#x27;s modify &lt;code&gt;Tensor::tensor_op()&lt;&#x2F;code&gt; and &lt;code&gt;Tensor::pow()&lt;&#x2F;code&gt; to use the new struct fields we just added.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

-    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;tensor_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Ops)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-       &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;-&amp;gt; Result&amp;lt;Tensor, TensorError&amp;gt; {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;tensor_op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;other&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;op&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: Ops)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;-&amp;gt; Result&amp;lt;Tensor, TensorError&amp;gt; {

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

-       &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(new_data, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            data: new_data,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            shape: new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_vec&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            strides: Tensor::
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;calc_strides_from_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            track_grad: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            create_op: Some(op),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            derivative: RefCell::new(
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape)]),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            lhs_parent: Some(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            rhs_parent: Some(other),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;})

    }

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;pow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;exp&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) -&amp;gt; Result&amp;lt;Tensor, TensorError&amp;gt; {

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

-       &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(new_data, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(Tensor {
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            data: new_data,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_vec&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            strides: Tensor::calc_strides_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            track_grad: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            create_op: Some(Ops::Pow(exp)),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            derivative: RefCell::new(
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape)]),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            lhs_parent: Some(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;            rhs_parent: None,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;})
    }
}

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Ok, we&#x27;re halfway there! We can now represent a sequence of operations as a computation graph. Let&#x27;s update our pretty-printing code to print out the structure of our internal representation (IR) of the computation graph.&lt;&#x2F;p&gt;
&lt;p&gt;This probably isn&#x27;t the most elegant way of implementing this but it works and I&#x27;m not motivated enough right now to try and improve it.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter&amp;lt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;recurse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;level&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;usize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) -&amp;gt; String {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; indent &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;  &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_string&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;repeat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(level);

            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; tensor.lhs_parent {
                Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;recurse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(t, level &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
                None &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;None&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_string&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            };

            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; tensor.rhs_parent {
                Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;recurse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(t, level &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
                None &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;None&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_string&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            };

            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; op &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;tensor.create_op {
                Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;format!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, t),
                None &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;None&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_string&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            };

            format!(
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{:?} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{:?} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;Lhs: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;Rhs: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;Op: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;TrackGrad: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{:?} &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;\n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{:?}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
                indent,
                tensor.data,
                indent,
                tensor.shape,
                indent,
                lhs,
                indent,
                rhs,
                indent,
                op,
                indent,
                tensor.track_grad,
                indent,
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(tensor.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;())
            )
        }

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; graph &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;recurse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;);

        write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, graph)
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;bin&#x2F;main.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() -&amp;gt; Result&amp;lt;(), TensorError&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;])&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;])&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;b;

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;pow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

    println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, d);

    Ok(())
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s run this and take a look at the resulting IR:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; l2 % cargo run
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 16.0, 81.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1, 3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:
  Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 4.0, 9.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1, 3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:
    Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 2.0, 3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1, 3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;0.0, 0.0, 0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:
    Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 2.0, 3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;0.0, 0.0, 0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Multiply
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;0.0, 0.0, 0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Pow: 2
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;0.0, 0.0, 0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Maybe it&#x27;s not the nicest looking graph, but it works well for when you&#x27;re trying to visually verify that your gradients are being calculated correctly.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now that we have a computation graph, we need to find a way to backpropogate through it.&lt;&#x2F;p&gt;
&lt;p&gt;The simplest way would be to recursively call a function named &lt;code&gt;backward()&lt;&#x2F;code&gt; on the tensor you want to calculate the gradient with respect to. &lt;code&gt;backward()&lt;&#x2F;code&gt; would first take the gradient of the current tensor (the gradient of the output tensor would be with respect to itself so its gradient is $1$) then use it to calculate (and accumulate, if necessary) the gradient of its parent(s) before calling &lt;code&gt;.backward()&lt;&#x2F;code&gt; on the parent &lt;code&gt;Tensor&lt;&#x2F;code&gt;(s) to recursively calculate the gradient for the entire computation graph.&lt;&#x2F;p&gt;
&lt;p&gt;There are a couple of problems with this:&lt;&#x2F;p&gt;
&lt;p&gt;First, recursively calling &lt;code&gt;.backward()&lt;&#x2F;code&gt; on the entire computation graph would be very memory-inefficient.&lt;&#x2F;p&gt;
&lt;p&gt;Second, if the computation graph has multiple branches (like in a Resnet), the backwards pass over the computation graph will have to be computed multiple times as the gradients for the parent &lt;code&gt;Tensor&lt;&#x2F;code&gt; of each branch in the network are accumulated. Doing it this way would have make computing the backwards pass &lt;em&gt;very&lt;&#x2F;em&gt; slow and inefficient.&lt;&#x2F;p&gt;
&lt;p&gt;Luckily, there is a better way of doing this. If we topologically sort and reverse the graph so that all the &lt;code&gt;Tensor&lt;&#x2F;code&gt;s are ordered in a way so that the gradients for all child &lt;code&gt;Tensor&lt;&#x2F;code&gt;s of a certain &lt;code&gt;Tensor&lt;&#x2F;code&gt; have already been computed and the gradient for the current &lt;code&gt;Tensor&lt;&#x2F;code&gt; has already been accumulated (if necessary), we won&#x27;t have to re-compute any parts of the graph.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s see how we could implement this in Rust:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;backward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) {
        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; from https:&#x2F;&#x2F;github.com&#x2F;evcu&#x2F;numpy_autograd&#x2F;blob&#x2F;master&#x2F;my_autograd.py#L147
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; seen: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec::new();
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; sorted: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec::new();

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;topological_sort&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;(
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;vr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor,
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;seen&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;&amp;gt;,
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;sorted&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;&amp;gt;,
        ) {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;!&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;seen.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;contains&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vr) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&amp;amp; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(vr.lhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;is_some&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;||&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; vr.rhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;is_some&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()) {
                seen.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(vr);

                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; vr.lhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;is_some&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;topological_sort&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(vr.lhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
                        seen, sorted);
                }
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; vr.rhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;is_some&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;topological_sort&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(vr.rhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
                        seen, sorted);
                }

                sorted.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(vr);
            }
        }

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; Topologically sort the computation graph
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;topological_sort&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; seen, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; sorted);

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; reverse it
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;        sorted.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;reverse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; Set the derivative of the output of the computation
        &#x2F;&#x2F; graph to itself to equal 1 (usually the derivative
        &#x2F;&#x2F; of the loss wrt itself)
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;sorted[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;].derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow_mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
            Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;sorted[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;].shape)];

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; t &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; sorted.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() {
            t.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;grad&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The &lt;code&gt;.grad()&lt;&#x2F;code&gt; function doens&#x27;t exist yet, but its purpose is to take the gradient of the current &lt;code&gt;Tensor&lt;&#x2F;code&gt; &lt;code&gt;t&lt;&#x2F;code&gt; and use it to compute the gradients of its parent(s). Since we wrapped the &lt;code&gt;derivative&lt;&#x2F;code&gt; field of &lt;code&gt;Tensor&lt;&#x2F;code&gt; in a &lt;code&gt;RefCell()&lt;&#x2F;code&gt;, we can use something like &lt;code&gt;*lhs_parent.borrow_mut() = gradient;&lt;&#x2F;code&gt; to safely mutate the parent&#x27;s gradient.&lt;&#x2F;p&gt;
&lt;p&gt;Here&#x27;s how I did it:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;grad&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) {
        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; get the gradient of the derivative of self wrt output
        &#x2F;&#x2F; d_x&#x2F;d_loss
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; if lhs_parent exists
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.lhs_parent {

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; calculate the gradient of lhs_parent wrt x
            &#x2F;&#x2F; d_lhs&#x2F;d_x
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                Some(Ops::TensorOp(TensorOp::Add)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
                        Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape)],
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Sub)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(
                        vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
                        Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape)],
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Mul)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.rhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()),
                Some(Ops::TensorOp(TensorOp::Div)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; temp &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.rhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
                        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;pow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

                    Tensor::new(temp.data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;temp.shape)
                }
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_ =&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Err(TensorError::GradError),
            }
            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; calculate the gradient of lhs_parent wrt loss
            &#x2F;&#x2F; d_lhs&#x2F;d_loss = d_lhs&#x2F;d_x * d_x&#x2F;d_loss
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_ =&amp;gt; &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d,
            };

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; accumulate the gradient of d_lhs&#x2F;d_loss if necessary
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs_prev &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(t.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;t.shape).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+ &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_lhs_prev;

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; assign to the derivative of the parent
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;t.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow_mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs.data;
        }

        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; if rhs_parent exists
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.rhs_parent {

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; calculate the gradient of rhs_parent wrt x
            &#x2F;&#x2F; d_rhs&#x2F;d_x
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                Some(Ops::TensorOp(TensorOp::Add)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(
                    vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape)],
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Sub)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(
                    vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape)],
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape,
                ),
                Some(Ops::TensorOp(TensorOp::Mul)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ok(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.lhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()),
                Some(Ops::TensorOp(TensorOp::Div)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; neg1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; t_powed &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; t.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;pow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; temp &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;neg1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.lhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; temp &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;temp &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;t_powed;

                    Tensor::new(temp.data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;temp.shape)
                }
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_ =&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Err(TensorError::GradError),
            }
            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; calculate the gradient of rhs_parent wrt loss
            &#x2F;&#x2F; d_rhs&#x2F;d_loss = d_rhs&#x2F;d_x * d_x&#x2F;d_loss
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_ =&amp;gt; &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d,
            };

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; accumulate the gradient of d_rhs&#x2F;d_loss if necessary
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs_prev &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(t.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;t.shape).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+ &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_rhs_prev;

            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; assign to the derivative of the parent
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;t.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow_mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs.data;
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;That should be pretty much it. Try it out:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;bin&#x2F;main.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() -&amp;gt; Result&amp;lt;(), TensorError&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;b;

    c.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;backward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

    println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, c);

    Ok(())
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; l2 % cargo run
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;6.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:
  Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:
  Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Multiply
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;🎉, you now have a semi-complete autograd engine!&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;advanced-ops&quot;&gt;&lt;a name=&#x27;advancedops&#x27; href=&#x27;#advancedops&#x27;&gt;Advanced Ops&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#advanced-ops&quot; aria-label=&quot;Anchor link for: advanced-ops&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s add support for fast matrix multiplications with BLAS.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;todo&lt;&#x2F;em&gt; talk about blas&lt;&#x2F;p&gt;
&lt;p&gt;First up, lets implement the &lt;code&gt;transpose()&lt;&#x2F;code&gt; operator&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;ops.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, PartialEq)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
    Transpose,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter&amp;lt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops::Transpose &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Transpose&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;grad&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.lhs_parent {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(Ops::Transpose) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]), &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; dummy value
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
        .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
            Some(Ops::Transpose) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;transpose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_ =&amp;gt; &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d,
        };
    }

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;transpose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) -&amp;gt; Result&amp;lt;Tensor, TensorError&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; transposed_shape &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; transposed_strides &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.strides.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        transposed_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;reverse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
        transposed_strides.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;reverse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_data: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec::with_capacity(
                Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;transposed_shape));

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;transposed_shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; transposed_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                new_data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.data[Tensor::
                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;get_physical_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[i], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;transposed_strides)]
                );
            } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; j &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;..&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;transposed_shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] {
                    new_data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;push&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.data[Tensor::
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;get_physical_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[i, j], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;transposed_strides)]);
                }
            }
        }

        Ok(Tensor {
            data: new_data,
            shape: transposed_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_vec&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            strides: Tensor::calc_strides_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;transposed_shape),
            track_grad: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
            create_op: Some(Ops::Transpose),
            derivative: RefCell::new(vec![
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;
                Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;transposed_shape)
            ]),
            lhs_parent: Some(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
            rhs_parent: None,
        })
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now that we have this, let&#x27;s add matmul support.&lt;&#x2F;p&gt;
&lt;p&gt;First up, let&#x27;s add a BLAS crate to &lt;code&gt;Cargo.toml&lt;&#x2F;code&gt;. Note that I&#x27;m using Apple&#x27;s accelerate as the BLAS library backend since its already installed on my Macbook pro, but you can change it to use &lt;a href=&quot;https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;blas-src&quot;&gt;another&lt;&#x2F;a&gt; BLAS library if you want.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[dependencies]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;blas &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;0.20.0&amp;quot;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;blas-src &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;= { &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;version &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;0.6&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;features &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;accelerate&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] }
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s add some Ops and errors for matmul&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;ops.rs

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, PartialEq)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
    Matmul,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter&amp;lt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Ops::Matmul &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(f, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Matmul&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;errors.rs
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;#[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;derive&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(Debug, Clone)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub enum &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
    MatmulShapeError,
}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Display &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;fmt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fmt::Formatter) -&amp;gt; fmt::Result {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;TensorError::MatmulShapeError &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;write!(
                f,
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Tensors must be two dimensions each and must be matrix multipliable&amp;quot;
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
        }
    }
}

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s add the matrix multiplication code&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    #[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;allow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(clippy::many_single_char_names)]
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;two_dimension_matmul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;lhs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;rhs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt;) {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;transpose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;transpose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; a: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; lhs.data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(|&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;val&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;val &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;collect&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; b: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; rhs.data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(|&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;val&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;val &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;collect&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c: Vec&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; Tensor::calc_tensor_len_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[lhs.shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;],
                rhs.shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]])];

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(m, n, k) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(
            lhs.shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;i32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
            rhs.shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;i32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
            lhs.shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;i32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
        );

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;unsafe &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;dgemm&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;N&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;N&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, m, n, k, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;a, m, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;b, k, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c, m);
        }

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;iter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;map&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(|&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;val&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;val &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;f32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;collect&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(c, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[rhs.shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], lhs.shape[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]]).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;transpose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c.data;

        out.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;append&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c);
    }

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;pub fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;matmul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;rhs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Tensor) -&amp;gt; Result&amp;lt;Tensor, TensorError&amp;gt; {
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_shape &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::validate_tensors(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;rhs)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;;

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;lt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;|| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Err(TensorError::MaxDimsError);
        }

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_data &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Vec::with_capacity(Tensor::
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;calc_tensor_len_from_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape));

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            Tensor::two_dimension_matmul(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, rhs, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; new_data)
        } &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Err(TensorError::MatmulShapeError);
        }

        Ok(Tensor {
            data: new_data,
            shape: new_shape.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;to_vec&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            strides: Tensor::calc_strides_from_shape(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape),
            track_grad: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;true&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;,
            create_op: Some(Ops::Matmul),
            derivative: RefCell::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;; Tensor::
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;calc_tensor_len_from_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;new_shape)]),
            lhs_parent: Some(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
            rhs_parent: Some(rhs),
        })
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now that we have that, let&#x27;s add autograd support for matmul&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;tensor.rs
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;impl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; Tensor&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;&amp;#39;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;grad&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) {

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.lhs_parent {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(Ops::Matmul) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.rhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;transpose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            }
            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(Ops::Matmul) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;matmul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_lhs).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;_ =&amp;gt; &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_lhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;* &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d,
            };
        }

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if let &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(t) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.rhs_parent {
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(Ops::Matmul) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.lhs_parent.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;transpose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            }
            .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;match &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.create_op {
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;...
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Some(Ops::Matmul) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;matmul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(),
            };

            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs_prev &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=
                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(t.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;clone&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;t.shape).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_rhs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+ &amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;d_rhs_prev;
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;t.derivative.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;borrow_mut&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; d_rhs.data;
        }
    }
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let&#x27;s try it out:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; src&#x2F;main.rs
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() -&amp;gt; Result&amp;lt;(), TensorError&amp;gt; {
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; a &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;4.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;5.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;6.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Tensor::new(vec![&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;4.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;5.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;6.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;let&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; c &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; a.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;matmul&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;amp;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;b).&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;unwrap&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

    c.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;backward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;();

    println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, c);

    Ok(())
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;bilal@Bilals-MacBook-Pro&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; l2 % cargo run
   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Compiling&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; l2 v0.1.0 (&#x2F;Users&#x2F;bilal&#x2F;Desktop&#x2F;l2)
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Finished&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; dev &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;unoptimized + debuginfo&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; target(s) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;in&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; 1.46s
     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Running &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;`&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;target&#x2F;debug&#x2F;main&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;`

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;22.0, 28.0, 49.0, 64.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;2, 2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:
  Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 2.0, 3.0, 4.0, 5.0, 6.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;2, 3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;3.0, 7.0, 11.0, 3.0, 7.0, 11.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:
  Value: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 2.0, 3.0, 4.0, 5.0, 6.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Shape: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;3, 2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Lhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Rhs:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; None
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;5.0, 5.0, 7.0, 7.0, 9.0, 9.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Op:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; Matmul
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TrackGrad:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; true
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Derivative: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;1.0, 1.0, 1.0, 1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Well thats pretty much it for the first draft. Ill see about adding more stuff when I redo this whole post.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;future-work&quot;&gt;&lt;a name=&#x27;future&#x27; href=&#x27;#future&#x27;&gt;Future Work&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#future-work&quot; aria-label=&quot;Anchor link for: future-work&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;rust arrays vs vec
&lt;ul&gt;
&lt;li&gt;const generics&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;jax&lt;&#x2F;li&gt;
&lt;li&gt;compiler in rust&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;conclusions&quot;&gt;&lt;a name=&#x27;conclusions&#x27; href=&#x27;#conclusions&#x27;&gt;Conclusions&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusions&quot; aria-label=&quot;Anchor link for: conclusions&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;em&gt;todo&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;benchmarks&lt;&#x2F;li&gt;
&lt;li&gt;subsections&lt;&#x2F;li&gt;
&lt;li&gt;gradient vs derivative&lt;&#x2F;li&gt;
&lt;li&gt;standardize code snippets&lt;&#x2F;li&gt;
&lt;li&gt;move implementing ops to beginning&lt;&#x2F;li&gt;
&lt;li&gt;naive matmul&lt;&#x2F;li&gt;
&lt;li&gt;slicing?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;resources&quot;&gt;&lt;a name=&#x27;resources&#x27; href=&#x27;#resources&#x27;&gt;Resources&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources&quot; aria-label=&quot;Anchor link for: resources&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;conclusions-1&quot;&gt;&lt;a name=&#x27;conclusions&#x27; href=&#x27;#conclusions&#x27;&gt;Conclusions&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusions-1&quot; aria-label=&quot;Anchor link for: conclusions-1&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;references&quot;&gt;&lt;a name=&#x27;references&#x27; href=&#x27;#references&#x27;&gt;References&lt;&#x2F;a&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#references&quot; aria-label=&quot;Anchor link for: references&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;1&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;1&lt;&#x2F;sup&gt;
&lt;p&gt;I guess the fact that I like to spend my last free summer working on a side project says a lot about me :p&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;2&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;2&lt;&#x2F;sup&gt;
&lt;p&gt;I&#x27;m almost certain that there are a few bugs in how I handle backpropogation through broadcasted tensors&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;3&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;3&lt;&#x2F;sup&gt;
&lt;p&gt;That&#x27;s the summer of 2019, for those of you reading this in the near or not so near future :)&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;4&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;4&lt;&#x2F;sup&gt;
&lt;p&gt;https:&#x2F;&#x2F;blog.ezyang.com&#x2F;2019&#x2F;05&#x2F;pytorch-internals&#x2F;&#x27;&amp;gt;http:&#x2F;&#x2F;blog.ezyang.com&#x2F;2019&#x2F;05&#x2F;pytorch-internals&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;5&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;5&lt;&#x2F;sup&gt;
&lt;p&gt;In my defense, I was pretty bad at algorithmy stuff back then&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
</description>
        </item>
        <item>
            <title>NLP Reseach Project Part 2</title>
            <pubDate>Fri, 17 Jul 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/nlp-research-part-2/</link>
            <guid>https://bkkaggle.github.io/blog/nlp-research-part-2/</guid>
            <description>&lt;hr &#x2F;&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;blog&#x2F;nlp-research-part-1&quot;&gt;Part 1: Best Practices for Finetuning Large Transformer Language models&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Part 2: How I (almost) replicated OpenAI&#x27;s GPT-2 (124M version)&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;tl-dr&quot;&gt;TL;DR&lt;a class=&quot;zola-anchor&quot; href=&quot;#tl-dr&quot; aria-label=&quot;Anchor link for: tl-dr&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;A few months ago I started working on a research project trying to pretrain my own, more efficient language model from scratch. I got access to a 128-core TPUv3 pod from the Tensorflow Reseach Cloud and used it to pretrain a $124$M parameter GPT-2 model to a perplexity pretty close to OpenAI&#x27;s results (my pretrained model was trained for about $1&#x2F;8$th of the number of iterations that OpenAI trained their model for and got $21$ ppl on OpenWebText compared to $17$ ppl for OpenAI&#x27;s model), and then pretrained an ALBERT-style GPT-2 (that I&#x27;m calling ALGPT2) language model with a factorized input embedding and layer-wise parameter sharing that would reduce the number of paramters in the model from $124$M to around $12$M.&lt;&#x2F;p&gt;
&lt;p&gt;Unfortunately, ALGPT-2 doesn&#x27;t perform as well as GPT-2 (ALGPT-2 gets $31$ ppl on OpenWebText compared to $21$ ppl for my pretrained GPT-2 model), but I&#x27;m writing this series of blog posts to go through everything I&#x27;ve learned over the last few months.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;the-idea&quot;&gt;The Idea&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-idea&quot; aria-label=&quot;Anchor link for: the-idea&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The main thing that I wanted to do from this sort-of &amp;quot;research project&amp;quot; that I was working on by myself this spring was to develop and train a more efficient version of the $124$M parameter version of &lt;a href=&quot;https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;better-language-models&#x2F;&quot;&gt;GPT-2&lt;&#x2F;a&gt;. I wanted to pretrain the $1.5$B parameter version of GPT-2 but since I only got access to the TPU pod for a week, I had to choose a model that would train in time. A $100$k iteration training run takes about $20$ hours to run which gave me plenty of time to run multiple experiments. In contrast, following OpenAI&#x27;s training procedure exactly and training for the full $800$k iterations would take up almost an entire week and use up most of my quota.&lt;&#x2F;p&gt;
&lt;p&gt;I was able to almost replicate the $124$M parameter version of GPT-2 by pretraining it to a perplexity pretty close to OpenAI&#x27;s results (my pretrained model used was trained for about $1&#x2F;8$th of the number of iterations that OpenAI trained their model for and got $21$ perplexity (ppl) on the standard OpenWebText dataset compared to $17$ ppl for OpenAI&#x27;s model),&lt;&#x2F;p&gt;
&lt;p&gt;My idea of making a more efficient transformer didn&#x27;t really work out since my pretrained transformer ended up being about $20$ppl worse than an equivalent GPT-2 model, but I wanted to writeup what I learned over the two or three months that I was working on this anyway.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;background&quot;&gt;Background&lt;a class=&quot;zola-anchor&quot; href=&quot;#background&quot; aria-label=&quot;Anchor link for: background&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;A little bit about myself: I&#x27;m an incoming software engineering student at the University of Waterloo and this post is supposed to be a writeup of a NLP research project that I was working on from around March to May of 2020 (right in the middle of the first Covid-19 lockdown of 2020, I&#x27;m currently writing this on July 15, 2020 while waiting for my Pix2PixHD model to train for a few hundred epochs on colab for a new project that I&#x27;m working on).&lt;&#x2F;p&gt;
&lt;p&gt;Over the last three or four years I&#x27;ve done a lot of machine learning related stuff. I started out back in early 2017 by going through the &lt;a href=&quot;https:&#x2F;&#x2F;www.google.com&#x2F;search?sxsrf=ALeKk010AvJn990-Vqb1MA50AAVbnMg8uw:1594828665718&amp;amp;q=Introduction+to+Machine+Learning+with+Python:+A+Guide+for+Data+Scientists&amp;amp;stick=H4sIAAAAAAAAAONgVuLVT9c3NEwyLzRLKs_Ke8RowS3w8sc9YSn9SWtOXmPU5OIKzsgvd80rySypFJLmYoOyBKX4uVB18ixi9fTMKynKTylNLsnMz1MoyVfwTUzOyMxLVfBJTSzKy8xLVyjPLMlQCKgsycjPs1JwVHAvzUxJVUjLL1JwSSxJVAhOzkwFGl9cUgwAxHs76ZgAAAA&amp;amp;biw=1920&amp;amp;bih=969&quot;&gt;Introduction to Machine Learning with Python&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;www.google.com&#x2F;search?q=hands+on+machine+learning+with+scikit-learn+and+tensorflow&amp;amp;oq=hands+on+mac&amp;amp;aqs=chrome.0.0j69i57j46j0l5.1936j0j7&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow&lt;&#x2F;a&gt; books. At the time, I didn&#x27;t really understand all the math behind neural networks, but it got me hooked on ML and then I took the &lt;a href=&quot;https:&#x2F;&#x2F;www.deeplearning.ai&#x2F;&quot;&gt;deeplearning.ai&lt;&#x2F;a&gt; courses on Coursera and the original &lt;a href=&quot;https:&#x2F;&#x2F;www.fast.ai&#x2F;&quot;&gt;fast.ai&lt;&#x2F;a&gt; course (back in late 2017 when they hadn&#x27;t switched over to Pytorch and still used Tensorflow and Keras).&lt;&#x2F;p&gt;
&lt;p&gt;I started competing on &lt;a href=&quot;https:&#x2F;&#x2F;kaggle.com&quot;&gt;Kaggle&lt;&#x2F;a&gt; in early 2018 and kept on competing in competitions non-stop for about a year and a half, winning a few medals and becoming a competitions expert (At one point I was ranked in the top $100$ Kagglers on the competitions leaderboard). Kaggle was a really nice way to get a lot of experience using neural networks because of the wide range of competitions and datasets that I had access to. I started out by doing a few semantic segmentation competitions then moved onto competing in NLP competitions. Since around mid 2019, I&#x27;ve been working on a bunch of different projects in ML and lower-level CS stuff. I worked on making a PyTorch-style machine learning library in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;L2&#x2F;tree&#x2F;c%2B%2B&quot;&gt;C++&lt;&#x2F;a&gt; and more recently in &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;L2&quot;&gt;Rust&lt;&#x2F;a&gt;, and for the last few months I&#x27;ve also been trying to keep up with all the new machine learning (esp. NLP) papers on arXiv.&lt;&#x2F;p&gt;
&lt;p&gt;I was pretty lucky that I started learning NLP right before transformers exploded in popularity, I remember when &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1301.3781&quot;&gt;word2vec&lt;&#x2F;a&gt; and LSTMs were still SOTA on a lot of NLP tasks, and it has been really interesting to see how much the field of NLP has changed in just a few years, going from when LSTMs with only a a handful of layers and somewhere on the order of $512$ units were considered to be large networks and computationally expensive to train, to training LSTMs with &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1409.0473&quot;&gt;attention&lt;&#x2F;a&gt; layers on top, to the original &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1706.03762&quot;&gt;transformer encoder&#x2F;decoder networks&lt;&#x2F;a&gt;, to &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1801.06146&quot;&gt;ULMFIT&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1802.05365&quot;&gt;ELMO&lt;&#x2F;a&gt;, then &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1810.04805&quot;&gt;BERT&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1907.11692&quot;&gt;RoBERTa&lt;&#x2F;a&gt;, &lt;a href=&quot;https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;better-language-models&#x2F;&quot;&gt;GPT-2&lt;&#x2F;a&gt;, and &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.10683&quot;&gt;T5&lt;&#x2F;a&gt;, to just a few months ago with the explosion of new, more efficient replacements for self-attention like the &lt;a href=&quot;https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;sparse-transformer&#x2F;&quot;&gt;Sparse Transformer&lt;&#x2F;a&gt;, the &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2001.04451&quot;&gt;Reformer&lt;&#x2F;a&gt;, and &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.00743&quot;&gt;Synthesizers&lt;&#x2F;a&gt;, and now &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2005.14165&quot;&gt;GPT-3&lt;&#x2F;a&gt;, which IMO has the potential to really change the whole field of NLP.&lt;&#x2F;p&gt;
&lt;p&gt;Just a few years ago we trained shallow recurrent networks on datasets, then pretrained large transformer language models on large datasets and finetuned on task-specific datasets. Now the whole idea of just training a gigantic language model on a huge dataset, then conditioning the model in a form of few-shot learning by prepending a few examples of a certain task to an input feels like it can really make NLP models a lot more accessible and easier to productionize as well as making human-chatbot interactions a lot more realistic than they are today.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve rambled on for long enough, lets get to the main topic of this post.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;gpt-2-and-albert&quot;&gt;GPT-2 and ALBERT&lt;a class=&quot;zola-anchor&quot; href=&quot;#gpt-2-and-albert&quot; aria-label=&quot;Anchor link for: gpt-2-and-albert&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;better-language-models&#x2F;&quot;&gt;GPT-2&lt;&#x2F;a&gt; is a transformer decoder. &lt;&#x2F;p&gt;
&lt;p&gt;The embedding layer at the root of the model maps a one-hot vector of a given token&#x27;s index (all the GPT-2 models use a vocabulary size of $50257$) to a $768$ dimensional vector (all GPT-2 numbers in this blog post will be for the $124$m parameter version of GPT-2).&lt;&#x2F;p&gt;
&lt;p&gt;The embedding matrix is followed by a stack of self-attention and feed-forward layers that each output a $768$ dimensional vector (keeping the number of outputs for each layer constant), which makes up the main part of the transformer.&lt;&#x2F;p&gt;
&lt;p&gt;The stack of self-attention layers is then followed by an output embedding (the weights of the input and output embeddings are tied to make training easier) that maps the $768$ dimensional vector that is the output of the last layer of the transformer to the same $50257$ dimensional vector that represents the probability of each token in the vocabulary being the next token in the sequence.&lt;&#x2F;p&gt;
&lt;p&gt;Take a look at &lt;a href=&quot;http:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-gpt2&#x2F;&quot;&gt;The Illustrated GPT-2&lt;&#x2F;a&gt; for a more in-depth look into GPT-2.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1909.11942&quot;&gt;ALBERT&lt;&#x2F;a&gt; (A Lite BERT) is a paper that takes a look at &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1810.04805&quot;&gt;BERT&lt;&#x2F;a&gt; and identifies some ways in which to make it more efficient and reduce the number of parameters in the model in four major ways: a factorized embedding, layer-wise parameter sharing, a sentence-order-prediction auxillary loss, and removing dropout.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;factorized-embedding&quot;&gt;Factorized embedding&lt;a class=&quot;zola-anchor&quot; href=&quot;#factorized-embedding&quot; aria-label=&quot;Anchor link for: factorized-embedding&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;GPT-2&#x27;s embedding has a lot of parameters. It&#x27;s really just a matrix of dimensions $50257 \times 768$. That means that the input embedding alone uses up almost $50257 \times 768 = \space \sim 38,000,000$ parameters which is a pretty big chunk of the $128$M total parameters in the model.&lt;&#x2F;p&gt;
&lt;p&gt;The ALBERT authors propose a factorized embedding with an intermediate embedding size of $128$: one embedding of size $50257 \times 128$ and another embedding of size $128 \times 768$. By breaking up the large embedding matrix into two smaller matrices, the total number of parameters used in the embedding goes from about $38$M to about $6$M.&lt;&#x2F;p&gt;
&lt;p&gt;$50257 \times 128 = \sim 6,000,000$&lt;&#x2F;p&gt;
&lt;p&gt;$128 \times 768 = \sim 100,000$&lt;&#x2F;p&gt;
&lt;p&gt;The authors try different intermediate embedding sizes and settle on $128$ as a good tradeoff betweeen parameters and performance.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;layer-wise-parameter-sharing&quot;&gt;Layer-wise parameter sharing&lt;a class=&quot;zola-anchor&quot; href=&quot;#layer-wise-parameter-sharing&quot; aria-label=&quot;Anchor link for: layer-wise-parameter-sharing&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;In a normal transformer model, the transformer layers are created something like this:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;class &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f0c678;&quot;&gt;BERT&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;nn.Module&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;):
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;__init__&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;n_layers&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;):
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;super&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;__init__&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.blocks &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;nn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;ModuleList&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Block&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;_ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;range&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(n_layers)])
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;forward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;):
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;block &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.blocks:
            x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;block&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(x)
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;ALBERT shares all parameters across the transformer layers something like this:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;class &lt;&#x2F;span&gt;&lt;span style=&quot;color:#f0c678;&quot;&gt;ALBERT&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;nn.Module&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;):
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;__init__&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;n_layers&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;):
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;super&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;__init__&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...

        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.n_layers &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;n_layers
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.block &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;Block&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;forward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;):
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;_ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.n_layers:
            x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;block&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(x)
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;By only defining one transformer block and looping around it &lt;code&gt;n_layers&lt;&#x2F;code&gt; times, ALBERT saves the GPU memory that would be used to store the parameters for all the layers.&lt;&#x2F;p&gt;
&lt;p&gt;Since we normally use $32$ bit floats to store parameters on the GPU, storing the $1.5$B parameter GPT-2 on the GPU will use up about $6$GB of the GPU&#x27;s memory — that&#x27;s a pretty big chunk of the $16$GB of memory that&#x27;s on a normal V100 GPU already being used up before taking into account the memory needed to store the model&#x27;s activations as well as any momentum parameters needed by the optimizer. In contrast, if you share parameters across all transformer layers in the $1.5$B parameter GPT-2, the resulting model will only have about $37$M parameters, the parameter-sharing version would only use up around $148$MB of GPU memory.&lt;&#x2F;p&gt;
&lt;p&gt;The authors try applying parameter sharing to BERT and see that it reduces performance but makes it easier to train larger and larger models.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a machine learning framework like JAX, which by default unrolls and inlines loops when it&#x27;s compiling your code with XLA, the size of the unrolled and inlined loop would make the computation graph really large and take a long time to compile. This is why you&#x27;re recommended to use somehting like &lt;a href=&quot;https:&#x2F;&#x2F;jax.readthedocs.io&#x2F;en&#x2F;latest&#x2F;_autosummary&#x2F;jax.lax.scan.html&quot;&gt;&lt;code&gt;lax.scan()&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; in these situations.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;sentence-order-prediction-auxillary-loss&quot;&gt;Sentence-order-prediction auxillary loss&lt;a class=&quot;zola-anchor&quot; href=&quot;#sentence-order-prediction-auxillary-loss&quot; aria-label=&quot;Anchor link for: sentence-order-prediction-auxillary-loss&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;The ALBERT authors add an auxillary loss to help training. Since language modelling is usually done autoregressively, I didn&#x27;t use this for my custom model.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;removing-dropout&quot;&gt;Removing dropout&lt;a class=&quot;zola-anchor&quot; href=&quot;#removing-dropout&quot; aria-label=&quot;Anchor link for: removing-dropout&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;The ALBERT authors remove all dropout from BERT and see that it significantly improves performance.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;That&#x27;s pretty much what my idea was: Take GPT-2, add a factorized embedding, share parameters across all transformer layers, remove dropout (I actually missed the part about ALBERT removing dropout until I was pretty far into my work, but I did run one or two runs without dropout to see how that works), and pretrain on a large dataset for a few hundred thousand iterations.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s no way that I could pretrain something like GPT-2 by myself, so I applied to the &lt;a href=&quot;https:&#x2F;&#x2F;www.tensorflow.org&#x2F;tfrc&quot;&gt;Tensorflow Research Cloud&lt;&#x2F;a&gt; (TFRC).&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The TFRC puts an emphasis on wanting to help researchers from non-traditional backgrounds which makes it an amazing resource for anyone who isn&#x27;t a &amp;quot;traditional&amp;quot; machine learning researcher. They were willing to give me, a 17 year old with no formal education or credentials (not even a high school diploma :&#x2F;), access to an extremely powerful cluster of TPUs at no cost. Being able to be a part of this program was really helpful to me, especially since I don&#x27;t have access to a dedicated GPU and usually rely on Colab&#x27;s free GPU to train my models.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I emailed the TFRC team to ask if I could get upgraded from $5$ separate individual TPUv3&#x27;s (with 8 cores each) to a TPU pod to pretrain a large language model. The very next day (!) I got an email back saying that I could get access to a preemptible 128-core TPUv3 Pod for 7 days which unfortunately wasn&#x27;t long enough for me to pretrain the $1.5$B parameter model but was enough to train a few runs on the $124$M model.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;setup&quot;&gt;Setup&lt;a class=&quot;zola-anchor&quot; href=&quot;#setup&quot; aria-label=&quot;Anchor link for: setup&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;So for setup I&#x27;ll be going through all the steps that I took to setup my VM and TPU Pod and preprocess the dataset as well.&lt;&#x2F;p&gt;
&lt;p&gt;When I was working on this project, I set up two VMs; One with a lot of RAM and CPU cores to process the data quickly and another small instance to run the TPU training script. &lt;em&gt;One of the nice things about training on TPUs and TPU pods is that as long as your data has been preprocessed as a set of TFRecord files, you don&#x27;t need a really powerful VM instance which saves you a lot of money&#x2F;compute credits.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;You can look at &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;blob&#x2F;master&#x2F;Markdown&#x2F;CLOUD.md&quot;&gt;this&lt;&#x2F;a&gt; for a full list of every command that I used to setup the VM and preprocess the dataset.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;openwebtext&quot;&gt;OpenWebText&lt;a class=&quot;zola-anchor&quot; href=&quot;#openwebtext&quot; aria-label=&quot;Anchor link for: openwebtext&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I used a &lt;code&gt;n-1-standard-16&lt;&#x2F;code&gt; instance with TF2.1 to process the OpenWebText dataset. Make sure that you use an instance with a SSD instead of the default HDD because processing the dataset involves processing a lot of very small text files and is mostly limited by your drive&#x27;s io speed. &lt;em&gt;I made the mistake of using a HDD and just extracting the dataset&#x27;s TAR archives took about 7 hours.&lt;&#x2F;em&gt; I put all the data in a folder at &lt;code&gt;~&#x2F;data&#x2F;openwebtext&#x2F;&lt;&#x2F;code&gt; so modify it if you want to download the data elsewhere.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;TIL: most common linux utilities (like &lt;code&gt;ls&lt;&#x2F;code&gt;, &lt;code&gt;mv&lt;&#x2F;code&gt;, and &lt;code&gt;cat&lt;&#x2F;code&gt;) aren&#x27;t really that optimized for working with almost 10 million files like in OpenWebText. Just counting the number of text files in the dataset could take several minutes._&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Download the &lt;a href=&quot;https:&#x2F;&#x2F;skylion007.github.io&#x2F;OpenWebTextCorpus&#x2F;&quot;&gt;OpenWebText&lt;&#x2F;a&gt; dataset (which is really just a tar archive of a bunch of tar archives that contain a lot of text files) and extract it:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;gdown&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; https:&#x2F;&#x2F;drive.google.com&#x2F;uc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;?&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;id=1EA5V0oetDCOke7afsktL_JDQ-ETtNOvx
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tar -xf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; openwebtext.tar.xz
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;cat &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;.xz &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tar -J -xf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; -&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; -i
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The dataset is about 12GB compressed and 53GB uncompressed and has just about 8 million text files.&lt;&#x2F;p&gt;
&lt;p&gt;I moved the first $100,000$ files in the dataset to a separate directory to create a validation set:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;ls -f &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;head -100000 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;xargs -i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; mv {} ..&#x2F;openwebtext-valid&#x2F;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;tokenization&quot;&gt;Tokenization&lt;a class=&quot;zola-anchor&quot; href=&quot;#tokenization&quot; aria-label=&quot;Anchor link for: tokenization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I trained a Byte-level BPE tokenizer with a vocabulary size of $50,257$ (The same as GPT-2) on a $1$M file subset of the training set (I&#x27;m not sure if GPT-2 trains the tokenizer on the entire dataset or on just a subset, but I know that the &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1909.05858&quot;&gt;CTRL&lt;&#x2F;a&gt; paper trains their tokenizer on a 5% split of their training set.). I used Hugginface&#x27;s fast Rust-based &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;tokenizers&quot;&gt;Tokenizers&lt;&#x2F;a&gt; library and their &lt;code&gt;ByteLevelBPETokenizer&lt;&#x2F;code&gt; tokenizer.&lt;&#x2F;p&gt;
&lt;p&gt;You can use my script &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;blob&#x2F;master&#x2F;train_tokenizer.py&quot;&gt;here&lt;&#x2F;a&gt; and run&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;python3 train_tokenizer.py &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;--&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;train_path .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;openwebtext&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F; --&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;save_path .&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;tokenizer&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;\
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;--&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;vocab_size &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;50257 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;--&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;n_files &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1000000
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;to train the tokenizer, or just take a look at this for the main details (It just trains a tokenizer and saves it as well as a configuration file to disk):&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;os
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;glob
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;json

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;tokenizers &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;ByteLevelBPETokenizer

paths &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;glob.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;glob&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(os.path.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;join&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;.&#x2F;data&#x2F;openwebtext&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;*&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;))[:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1000000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]

tok &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;ByteLevelBPETokenizer&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
tok.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;train&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;files&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;paths, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;vocab_size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;args.vocab_size, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;special_tokens&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;args.control_codes)
tok.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;save&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;.&#x2F;tokenizer&#x2F;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)

tokenizer_config &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;max_len&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1024
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;with &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;open&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(os.path.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;join&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;.&#x2F;tokenizer&#x2F;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;tokenizer_config.json&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;), &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;w&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;fp:
    json.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;dump&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(tokenizer_config, fp)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;tfrecords&quot;&gt;TFRecords&lt;a class=&quot;zola-anchor&quot; href=&quot;#tfrecords&quot; aria-label=&quot;Anchor link for: tfrecords&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TPU Pods expect your data to be available as a set of TFRecord files in a GCP cloud bucket that get downloaded to each of your TPU board&#x27;s built in powerful VM that will take care of de-serializing the files and feeding it to the TPU chips. Make sure that your GCP bucket and your TPU pod are in the same compute zone, otherwise you&#x27;ll quickly rack up a lot of charges by transferring hundreds of GBs of data across compute zones.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here&#x27;s a thing that&#x27;s not very well documented when working with TPU Pods (this doesn&#x27;t really apply to TPUs as much): TPU Pods create a lot (100s of GBs) of logs that get sent to Stackdriver, where you get charged about 50 cents for each GiB of logs ingested beyond a certain limit (I think it&#x27;s around 50GiB&#x2F;month). In just a few days of training, I ended up being charged about a $$100$ IIRC. Luckily, I still had most of the free GCP credits so this didn&#x27;t end up being a major problem for me, but make sure to turn off ingesting logs for TPUs.&lt;&#x2F;p&gt;
&lt;p&gt;I ran into a problem early on when I got access to the TPU pod where my code would work perfectly on a single TPU, but would throw an &lt;code&gt;Out of range: End of sequence&lt;&#x2F;code&gt; &lt;a href=&quot;https:&#x2F;&#x2F;gist.github.com&#x2F;bkkaggle&#x2F;ee63a04cd86c5fd45c41dc0b7ce109eb&quot;&gt;error&lt;&#x2F;a&gt; when running it on a TPU pod. I struggled with this for a pretty long time until I took a look at &lt;a href=&quot;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;flower-classification-with-tpus&#x2F;discussion&#x2F;130199&quot;&gt;this&lt;&#x2F;a&gt; Kaggle discussion post that says that TPUs expect each TPU board (8 cores) to get its own TFrecord file (until that point, I was splitting the train set into 8 TFRecord files where I should&#x27;ve been splitting it into 16 (128 cores &#x2F; 8 cores per board) TFRecord files.&lt;&#x2F;p&gt;
&lt;p&gt;TPUs are awesome for scaling to huge models and huge datasets, but there are a lot of TPU-specific information (especially for TPU Pods) that you need to know that&#x27;s not covered in the documentation and isn&#x27;t easy to find._**&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;You can use my script &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;blob&#x2F;master&#x2F;make_tfrecords.py&quot;&gt;here&lt;&#x2F;a&gt; and run&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;python3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; make_tfrecords.py&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; .&#x2F;data&#x2F;openwebtext&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --save_path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; .&#x2F;train&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --files_per_tfrecord&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; 500000 \
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;    --use_control_codes --seq_len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; 1024&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --min_seq_len --tokenizer&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; .&#x2F;tokenizer&#x2F;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;python3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; make_tfrecords.py&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; .&#x2F;data&#x2F;openwebtext-valid&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --save_path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; .&#x2F;val&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --files_per_tfrecord&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; 50000 \
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;    --use_control_codes --seq_len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; 1024&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt; --min_seq_len --tokenizer&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; .&#x2F;tokenizer&#x2F;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;to convert the raw text files from the train and validation splits into two sets of $16$ TFRecord files.&lt;&#x2F;p&gt;
&lt;p&gt;I ran a quick analysis on the average lengths of text fields in the dataset, $67$% of files have less than $1024$ tokens, $35$% of files have less than $512$ tokens, and only $10$% of files have less than $256$ tokens. This means that if I wanted to make the dataset as clean as possible and have each input sequence to the model be of a single contigous stream of $1024$ tokens, the dataset&#x27;s size would be a lot smaller. For this reason, everyone prepends a token like &lt;code&gt;&amp;lt;|endoftext|&amp;gt;&lt;&#x2F;code&gt; to the beginning of each sequence and concatenates together sequences with lengths smaller than $1024$. The specifics of how exactly you do that (e.g. do you treat the dataset as single stream of tokens and just break it up into sequences of length $1024$, or do you keep track of sequences smaller that $1024$ and just concatenate them together into a single sequence) really shouldn&#x27;t make too big of a difference in your model&#x27;s performance, but you can take a look at my implementation &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;blob&#x2F;master&#x2F;make_tfrecords.py&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;My version doesn&#x27;t take full advantage of the fast, multithreaded &lt;code&gt;batch_encode_plus()&lt;&#x2F;code&gt; way to tokenize large datasets in parallel since it only keeps the first &lt;code&gt;context_len&lt;&#x2F;code&gt; tokens in each line of the files which makes dealing with files with more or less than $1024$ tokens harder. Because of this, tokenizing the dataset takes about $8$ hours, which is something I want to improve.&lt;&#x2F;p&gt;
&lt;p&gt;The train set comes out to about $26$GB and consists of about $8$M text files that have been transformed into just under $7$M tfrecord examples, each with $1024$ tokens (same as GPT-2). The validation set comes out to about $300$MB and consists of about $100$K text files that have been transformed into just about $90$K tfrecord examples, each with $1024$ tokens (also the same as GPT-2).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;code&quot;&gt;Code&lt;a class=&quot;zola-anchor&quot; href=&quot;#code&quot; aria-label=&quot;Anchor link for: code&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Since I&#x27;m using TPUs, the only real library that you can practically use right now would be Tensorflow. I didn&#x27;t want to have to go through the learning curve of learning how to make custom training loops and stuff in TF2 so I just stuck to using Keras. You can take a look at my training script (It&#x27;s pretty short) &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;blob&#x2F;master&#x2F;train_tfrecords.py&quot;&gt;here&lt;&#x2F;a&gt;. It&#x27;s pretty simple so I&#x27;m not going to copy over the entire training script, but I will talk about a few small code snippets.&lt;&#x2F;p&gt;
&lt;p&gt;I usually like to add a ptvsd breakpoint to my script so I can debug my training script locally with vscode before pushing it up to my VM&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;args.debug:
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;ptvsd
    ptvsd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;enable_attach&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;address&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;localhost&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;5678&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;),
                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;redirect_output&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
    ptvsd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;wait_for_attach&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;breakpoint&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I&#x27;m using &lt;a href=&quot;https:&#x2F;&#x2F;www.wandb.com&#x2F;&quot;&gt;Weights&amp;amp;Biases&lt;&#x2F;a&gt; to keep track of my experiments and save checkpoints.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;    wandb.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;login&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()
    wandb.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;init&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;project&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;lm-finetuning&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;config&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;args, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tags&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;args.tags)

    ...

    wandb_callback &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;WandbCallback&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;save_model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Usually when you&#x27;re using a TPU with Keras, you pass in the IP address and port of the TPU to &lt;code&gt;TPUClusterResolver&lt;&#x2F;code&gt;, but you pass the name of the TPU itself to the resolver when using a TPU Pod.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;resolver &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;tf.distribute.cluster_resolver.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;TPUClusterResolver&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tpu&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;args.tpu)
tf.config.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;experimental_connect_to_cluster&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(resolver)
tf.tpu.experimental.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;initialize_tpu_system&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(resolver)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;replicating-gpt-2&quot;&gt;Replicating GPT-2&lt;a class=&quot;zola-anchor&quot; href=&quot;#replicating-gpt-2&quot; aria-label=&quot;Anchor link for: replicating-gpt-2&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I tried to use as many of the original hyperparameters that OpenAI used when I was replicating their $124$M parameter version of GPT-2, but I had to modify a few things so I could train everything in time.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: For some reason, the authors of the GPT-2 paper don&#x27;t state exactly what learning rates they used for training their models and instead just state &amp;quot;The learning rate of each model was manually tuned for the best perplexity on a 5% held-out sample of WebText&amp;quot;.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;OpenAI trains their models for a total of $800$K iterations at a batch size of $512$ (Which comes out to around a total of $60$ epochs through the training set).&lt;&#x2F;p&gt;
&lt;p&gt;I trained my GPT-2 model for $1&#x2F;8th$ the number of iterations that OpenAI trained theirs for (a total of around $100$K iterations) since each $100$K iteration training run took about $20$ hours to run on my 128-core TPU Pod. If I wanted to train GPT-2 for the same number of iterations as OpenAI, a single training run would have used up most of my one week of access to the pod.&lt;&#x2F;p&gt;
&lt;p&gt;Since my TPU pod was preemptible and resets every $24$ hours I usually had to resume my training run at least once and is the reason why all of these graphs usually have two or more training runs on them.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;replicating-gpt-2-1&quot;&gt;Replicating GPT-2&lt;a class=&quot;zola-anchor&quot; href=&quot;#replicating-gpt-2-1&quot; aria-label=&quot;Anchor link for: replicating-gpt-2-1&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;So here&#x27;s my model that came really close to replicating GPT-2, the training perplexity is about $21.5$ at the end of the almost $90$K training iterations. For comparison, GPT-2 gets a training perplexity about $17.5$ ppl after about $800$K training iterations, so a difference of only about $4$ ppl.&lt;&#x2F;p&gt;
&lt;p&gt;I made a &lt;a href=&quot;https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;19Q0M9lMI4FqE7sosepkNVeIvan39SVFI?usp=sharing&quot;&gt;colab notebook&lt;&#x2F;a&gt; showing how to use my pretrained GPT-2 model to generate text&lt;&#x2F;p&gt;
&lt;iframe
    title=&#x27;Replicating GPT-2&#x27;
    src=&#x27;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;reports&#x2F;Replicating-GPT-2-(124M)--VmlldzoxNzE3Mzc&#x27;
    height=&#x27;600px&#x27;
    width=&#x27;100%&#x27;
&gt; &lt;&#x2F;iframe&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;adamw-vs-adafactor&quot;&gt;AdamW vs Adafactor&lt;a class=&quot;zola-anchor&quot; href=&quot;#adamw-vs-adafactor&quot; aria-label=&quot;Anchor link for: adamw-vs-adafactor&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I wanted to use the memory-saving &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1804.04235&quot;&gt;Adafactor&lt;&#x2F;a&gt; optimizer to make it easier to train larger language models but all of my Adafactor training runs were a lot (~5ppl IIRC) worse than using AdamW (This may be due to not using Adafactor&#x27;s momentum parameter or relative update scale, so this is something I want to look into more soon).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;learning-rates&quot;&gt;Learning rates&lt;a class=&quot;zola-anchor&quot; href=&quot;#learning-rates&quot; aria-label=&quot;Anchor link for: learning-rates&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I started out with using Adam&#x27;s default learning rate of $1e-4$ but I quickly figured out that I could train my models a lot faster by using a higher learning rate like $1e-3$.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Section 2 of the &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2005.14165.pdf&quot;&gt;GPT-3&lt;&#x2F;a&gt; paper lists the learning rates the OpenAI team used for different sized models when training GPT-3. They use a learning rate of $6e-4$ for the $124$M version of their model and decrease the learning rate with model size.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;You can take a look at &lt;a href=&quot;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;reports&#x2F;adamw-1e-4-vs-1e-3--VmlldzoxNzE3NDc&quot;&gt;this&lt;&#x2F;a&gt; partial training run to see the difference between training at different learning rates.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;pretraining-algpt-2&quot;&gt;Pretraining ALGPT-2&lt;a class=&quot;zola-anchor&quot; href=&quot;#pretraining-algpt-2&quot; aria-label=&quot;Anchor link for: pretraining-algpt-2&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Since I was using the Huggingface &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&quot;&gt;Transformers&lt;&#x2F;a&gt; repository&#x27;s implementations of GPT-2 and ALBERT, I just &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;transformers&#x2F;tree&#x2F;albert-style&quot;&gt;forked&lt;&#x2F;a&gt; the repository and modified a few files to implement my ALGPT-2 model. You can take a look at all the changes that I had to make &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;transformers&#x2F;compare&#x2F;master...bkkaggle:albert-style&quot;&gt;here&lt;&#x2F;a&gt;, most of the changes are only to make ALGPT-2 compatible with the &#x2F;Transformers library and to be able to use the useful abstractions that it gives you, but most of the important code is in the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;transformers&#x2F;blob&#x2F;0f7c7c11e7b8bc8a275f3d16865b8a873c271571&#x2F;src&#x2F;transformers&#x2F;modeling_algpt2.py&quot;&gt;&lt;code&gt;modelling_algpt2.py&lt;&#x2F;code&gt; file&lt;&#x2F;a&gt; in which I just copied over the contents of &lt;code&gt;modelling_gpt2.py&lt;&#x2F;code&gt; and changed a few parts of the code. I&#x27;m only showing the changes that I made to the Pytorch version of ALGPT-2 here, the changes in the TF version are pretty similar to the Pytorch version and can be seen &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;transformers&#x2F;blob&#x2F;0f7c7c11e7b8bc8a275f3d16865b8a873c271571&#x2F;src&#x2F;transformers&#x2F;modeling_tf_algpt2.py&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;implementing-parameter-sharing&quot;&gt;Implementing parameter sharing&lt;a class=&quot;zola-anchor&quot; href=&quot;#implementing-parameter-sharing&quot; aria-label=&quot;Anchor link for: implementing-parameter-sharing&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Implementing parameter sharing only involves changing a few lines of code:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;class ALGPT2Model(ALGPT2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        ...

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;-       self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True)
-           for _ in range(config.n_layer)])
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+       self.h = Block(config.n_ctx, config, scale=True)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
        ...

    def forward(self, ...):

        ...

        if past is None:
            past_length = 0
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;-           past = [None] * len(self.h)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+           past = [None] * self.config.n_layer
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
        ...

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;-       for i, (block, layer_past) in enumerate(zip(self.h, past)):
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+       for i in range(self.config.n_layer):
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
            if self.output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;-
-           outputs = block(
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+           outputs = self.h(
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;                hidden_states,
                layer_past=layer_past,
                attention_mask=attention_mask,
                head_mask=head_mask[i],
                use_cache=use_cache,
            )
        ...

&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;implementing-a-factorized-embedding&quot;&gt;Implementing a factorized embedding&lt;a class=&quot;zola-anchor&quot; href=&quot;#implementing-a-factorized-embedding&quot; aria-label=&quot;Anchor link for: implementing-a-factorized-embedding&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Adding a factorized embedding is a little more work:&lt;&#x2F;p&gt;
&lt;p&gt;In the &lt;code&gt;config.json&lt;&#x2F;code&gt; that you use for your ALGPT-2 model, you need to specify that you want to use the ALGPT-2 and you need to specify the dimension of the factorized embedding that you want to use:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+	&amp;quot;architectures&amp;quot;: [&amp;quot;ALGPT2LMHeadModel&amp;quot;],
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;	&amp;quot;attn_pdrop&amp;quot;: 0.1,
	&amp;quot;bos_token_id&amp;quot;: 50256,
	&amp;quot;embd_pdrop&amp;quot;: 0.1,
	&amp;quot;eos_token_id&amp;quot;: 50256,
	&amp;quot;initializer_range&amp;quot;: 0.02,
	&amp;quot;layer_norm_epsilon&amp;quot;: 1e-5,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+	&amp;quot;model_type&amp;quot;: &amp;quot;algpt2&amp;quot;,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;	&amp;quot;n_ctx&amp;quot;: 1024,
	&amp;quot;n_embd&amp;quot;: 768,
	&amp;quot;n_head&amp;quot;: 12,
	&amp;quot;n_layer&amp;quot;: 12,
	&amp;quot;n_positions&amp;quot;: 1024,
	&amp;quot;resid_pdrop&amp;quot;: 0.1,
	&amp;quot;summary_activation&amp;quot;: null,
	&amp;quot;summary_first_dropout&amp;quot;: 0.1,
	&amp;quot;summary_proj_to_labels&amp;quot;: true,
	&amp;quot;summary_type&amp;quot;: &amp;quot;cls_index&amp;quot;,
	&amp;quot;summary_use_proj&amp;quot;: true,
	&amp;quot;vocab_size&amp;quot;: 50257,
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+	&amp;quot;embedding_size&amp;quot;: 128
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Back in &lt;code&gt;modelling_algpt2.py&lt;&#x2F;code&gt;, define the two factorized embedding matrices (the first second matrix that is really just a simple linear layer)&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;class ALGPT2Model(ALGPT2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        ...

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;-       self.wte = nn.Embedding(config.vocab_size, config.n_embd)
-       self.wpe = nn.Embedding(config.n_positions, config.n_embd)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+       self.wte = nn.Embedding(config.vocab_size, config.embedding_size)
+       self.wpe = nn.Embedding(config.n_positions, config.embedding_size)

+       self.projection_layer = nn.Linear(config.embedding_size, config.n_embd)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;

        ...

    def forward(self, ...):

        ...

        hidden_states = inputs_embeds + position_embeds + token_type_embeds

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+       hidden_states = self.projection_layer(hidden_states)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
        ...


class ALGPT2LMHeadModel(ALGPT2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        ...

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;-        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+        self.dense = nn.Linear(config.n_embd, config.embedding_size)
+        self.lm_head = nn.Linear(config.embedding_size, config.vocab_size, bias=False)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;
    def forward(self, ...):

        ...

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;-       lm_logits = self.lm_head(hidden_states)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;+       dense = self.dense(hidden_states)
+       lm_logits = self.lm_head(dense)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;        ...
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;effect-of-layer-wise-parameter-sharing&quot;&gt;Effect of layer-wise parameter sharing&lt;a class=&quot;zola-anchor&quot; href=&quot;#effect-of-layer-wise-parameter-sharing&quot; aria-label=&quot;Anchor link for: effect-of-layer-wise-parameter-sharing&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This version of ALGPT-2 has about $47$M parameters while GPT-2 has $124$M. This ALGPT-2 model with parameter sharing trains a lot faster than GPT-2 ($9$ hours vs $20$ hours for a $90$K iteration training run), but is consistently about $10$ ppl worse than GPT-2 ($31$ vs $21$ ppl).&lt;&#x2F;p&gt;
&lt;p&gt;This difference is quite a bit larger than the difference between ALBERT and BERT, but might be explained by masked language modelling being an easier task than autoregressive language modelling. Increasing the size of the ALGPT-2 model might make it more competitive with GPT-2.&lt;&#x2F;p&gt;
&lt;iframe
    title=&#x27;Effect of layer-wise parameter sharing&#x27;
    src=&#x27;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;reports&#x2F;Effect-of-layer-wise-parameter-sharing--VmlldzoxNzI0NzU&#x27;
    height=&#x27;600px&#x27;
    width=&#x27;100%&#x27;
&gt; &lt;&#x2F;iframe&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;effect-of-removing-dropout&quot;&gt;Effect of removing dropout&lt;a class=&quot;zola-anchor&quot; href=&quot;#effect-of-removing-dropout&quot; aria-label=&quot;Anchor link for: effect-of-removing-dropout&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I ran a &lt;a href=&quot;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;reports&#x2F;Effect-of-removing-dropout--VmlldzoxNzI0Nzk&quot;&gt;partial training run&lt;&#x2F;a&gt; on removing dropout from ALGPT-2. I didn&#x27;t run it for very long, but it looks like removing dropout gives you a slight improvement (~3ppl).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;effect-of-factorized-embeddings&quot;&gt;Effect of factorized embeddings&lt;a class=&quot;zola-anchor&quot; href=&quot;#effect-of-factorized-embeddings&quot; aria-label=&quot;Anchor link for: effect-of-factorized-embeddings&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I ran three experiments for $90$K iterations with three different values for the factorized embedding ($128$, $256$, and $512$) as well as the baseline version without a factorized embedding.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model&lt;&#x2F;th&gt;&lt;th&gt;ALGPT-2&lt;&#x2F;th&gt;&lt;th&gt;ALGPT-2 512&lt;&#x2F;th&gt;&lt;th&gt;ALGPT-2 256&lt;&#x2F;th&gt;&lt;th&gt;ALGPT-2 128&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Parameters&lt;&#x2F;td&gt;&lt;td&gt;47M&lt;&#x2F;td&gt;&lt;td&gt;34M&lt;&#x2F;td&gt;&lt;td&gt;20M&lt;&#x2F;td&gt;&lt;td&gt;13M&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Time&lt;&#x2F;td&gt;&lt;td&gt;~9H&lt;&#x2F;td&gt;&lt;td&gt;~9H&lt;&#x2F;td&gt;&lt;td&gt;~9H&lt;&#x2F;td&gt;&lt;td&gt;~9H&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Perplexity&lt;&#x2F;td&gt;&lt;td&gt;31&lt;&#x2F;td&gt;&lt;td&gt;31&lt;&#x2F;td&gt;&lt;td&gt;34&lt;&#x2F;td&gt;&lt;td&gt;38&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;There was practically no difference in the loss curves between the baseline and the $512$ run since the change in the number of parameters wasn&#x27;t that great. However, the training runs with factorized embeddings of sizes $256$ and $128$ were significantly worse than the baseline: $34$ and $38$ ppl respectively, a pretty big difference from the baseline of $31$ ppl.&lt;&#x2F;p&gt;
&lt;iframe
    title=&#x27;Effect of factorized embeddings&#x27;
    src=&#x27;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;reports&#x2F;Effect-of-factorized-embeddings--VmlldzoxNzI0ODU&#x27;
    height=&#x27;600px&#x27;
    width=&#x27;100%&#x27;
&gt; &lt;&#x2F;iframe&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;effect-of-model-size&quot;&gt;Effect of model size&lt;a class=&quot;zola-anchor&quot; href=&quot;#effect-of-model-size&quot; aria-label=&quot;Anchor link for: effect-of-model-size&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I only had the time to run one more full training run with ALGPT-2-medium (this one is comparable to the $345$M version of GPT-2). ALGPT-2-medium has about $66$M parameters and took twice as long as ALGPT-2 to train (a little more than $20$ hours). The larger model size made quite a big difference in performance, the training perplexity decreased $5$ppl from $31$ to $26$ ppl.&lt;&#x2F;p&gt;
&lt;iframe
    title=&#x27;Effect of model size&#x27;
    src=&#x27;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;reports&#x2F;Effect-of-model-size--VmlldzoxNzI0OTM&#x27;
    height=&#x27;600px&#x27;
    width=&#x27;100%&#x27;
&gt; &lt;&#x2F;iframe&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and next steps&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusion-and-next-steps&quot; aria-label=&quot;Anchor link for: conclusion-and-next-steps&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Well that&#x27;s pretty much everything that I did. After my TPU pod&#x27;s quota was used up, I started working on a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;L2&quot;&gt;few&lt;&#x2F;a&gt; &lt;a href=&quot;&#x2F;&quot;&gt;other&lt;&#x2F;a&gt;
&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;raytracer&quot;&gt;things&lt;&#x2F;a&gt; over the summer and just kept delaying writing up what I did for a couple of months until now.&lt;&#x2F;p&gt;
&lt;p&gt;There are a lot of things that I still want to work on or look into:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Training larger versions of ALGPT-2&lt;&#x2F;li&gt;
&lt;li&gt;Removing or replacing the normalization layers in transformers&lt;&#x2F;li&gt;
&lt;li&gt;Working on distilling&#x2F;shrinking language models with billions of parameters to make them more accessible&lt;&#x2F;li&gt;
&lt;li&gt;Apply something like &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1912.02164&quot;&gt;PPLM&lt;&#x2F;a&gt; to condition language models for few-shot inference (kinda like what GPT-3 does).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Thanks for reading through all this. If you think there&#x27;s any mistakes or inaccuracies in this post, please let me know.&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>GPU Memory Usage Breakdown</title>
            <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/memory-usage/</link>
            <guid>https://bkkaggle.github.io/blog/memory-usage/</guid>
            <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;normalization&quot;&gt;Notes Part 1: Normalization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;perplexity&quot;&gt;Notes Part 2: Perplexity&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;initialization&quot;&gt;Notes Part 3: Initialization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;memory-usage&quot;&gt;Notes Part 4: GPU Memory Usage Breakdown&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;adafactor&quot;&gt;Part 5: Adafactor&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;purpose&quot;&gt;Purpose&lt;a class=&quot;zola-anchor&quot; href=&quot;#purpose&quot; aria-label=&quot;Anchor link for: purpose&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in learning, help me keep track of everything I&#x27;ve learned over the last three years, and to practice my Latex skills.&lt;&#x2F;p&gt;
&lt;p&gt;This is my fourth blog post in the series, and this time I&#x27;m (again) really just Cmd+C&#x27;ing and Cmd+V&#x27;ing over some of my notes on memory usage in neural networks&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;background&quot;&gt;Background&lt;a class=&quot;zola-anchor&quot; href=&quot;#background&quot; aria-label=&quot;Anchor link for: background&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;GPU memory is used in a few main ways:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Memory to store the network&#x27;s parameters&lt;&#x2F;li&gt;
&lt;li&gt;Memory to store the network&#x27;s gradients&lt;&#x2F;li&gt;
&lt;li&gt;Memory to store the activations of the current batch&lt;&#x2F;li&gt;
&lt;li&gt;Memory used by optimizers (momentum, adam, etc) that stores running averages&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;parameter-memory&quot;&gt;Parameter memory&lt;a class=&quot;zola-anchor&quot; href=&quot;#parameter-memory&quot; aria-label=&quot;Anchor link for: parameter-memory&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Parameter memory usage depends on two things: The number of parameters and the amount of bytes used for each parameter.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Float 32 =&amp;gt; 4 bytes&lt;&#x2F;li&gt;
&lt;li&gt;Float 16 =&amp;gt; 2 bytes&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You can calculate parameter memory with the formula:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;parameter_memory = n_parameters * bytes_per_parameter
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;optimizer-memory&quot;&gt;Optimizer memory&lt;a class=&quot;zola-anchor&quot; href=&quot;#optimizer-memory&quot; aria-label=&quot;Anchor link for: optimizer-memory&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;--&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;SGD doesn&#x27;t store any extra memory&lt;&#x2F;li&gt;
&lt;li&gt;Momentum doubles parameter memory usage by storing one momentum parameter per parameter in a model&lt;&#x2F;li&gt;
&lt;li&gt;Adam stores 2 momenum parameters so Adam uses 3x the parameter memory of SGD&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;adafactor&quot;&gt;Adafactor:&lt;a class=&quot;zola-anchor&quot; href=&quot;#adafactor&quot; aria-label=&quot;Anchor link for: adafactor&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;By default adafactor uses n+m memory to store momentum parameters for a nxm matrix&lt;&#x2F;li&gt;
&lt;li&gt;If you enable Beta_1, it will be like using momentum + n+m memory, so a little more than 2x SGD&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
        </item>
        <item>
            <title>Initialization</title>
            <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/initialization/</link>
            <guid>https://bkkaggle.github.io/blog/initialization/</guid>
            <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;normalization&quot;&gt;Notes Part 1: Normalization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;perplexity&quot;&gt;Notes Part 2: Perplexity&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;initialization&quot;&gt;Notes Part 3: Initialization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;memory-usage&quot;&gt;Notes Part 4: GPU Memory Usage Breakdown&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;adafactor&quot;&gt;Part 5: Adafactor&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;purpose&quot;&gt;Purpose&lt;a class=&quot;zola-anchor&quot; href=&quot;#purpose&quot; aria-label=&quot;Anchor link for: purpose&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in learning, help me keep track of everything I&#x27;ve learned over the last three years, and to practice my Latex skills.&lt;&#x2F;p&gt;
&lt;p&gt;This is my third blog post in the series, and this time I&#x27;m really just Cmd+C&#x27;ing and Cmd+V&#x27;ing over some of my notes on initialization for neural networks&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;a class=&quot;zola-anchor&quot; href=&quot;#notation&quot; aria-label=&quot;Anchor link for: notation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;$\mu$: Mean&lt;&#x2F;li&gt;
&lt;li&gt;$\sigma ^ 2$: Standard Deviation&lt;&#x2F;li&gt;
&lt;li&gt;$c_{in}$: Number of input channels to a layer&lt;&#x2F;li&gt;
&lt;li&gt;$c_{out}$: Number of output channels to a layer&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;initialization&quot;&gt;Initialization&lt;a class=&quot;zola-anchor&quot; href=&quot;#initialization&quot; aria-label=&quot;Anchor link for: initialization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;$\mu$ and $\sigma ^ 2$ of activations should be close to $0$ and $1$ to prevent the gradients from exploding or vanishing&lt;&#x2F;li&gt;
&lt;li&gt;activations of layers have $\sigma ^ 2$ close to $\sqrt {c_{in}}$&lt;&#x2F;li&gt;
&lt;li&gt;so, to get the $\sigma ^ 2$ back to $1$, multiply randomly initialized weights by $1 &#x2F; sqrt(c_{in})$&lt;&#x2F;li&gt;
&lt;li&gt;this works well without activations, but results in vanishing or exploding gradients when used with a tanh or sigmoid activation function&lt;&#x2F;li&gt;
&lt;li&gt;bias weights should be initialized to $0$&lt;&#x2F;li&gt;
&lt;li&gt;intializations can either be from a uniform distribution or a normal distribution&lt;&#x2F;li&gt;
&lt;li&gt;use &lt;strong&gt;Xavier Initialization&lt;&#x2F;strong&gt; for sigmoid and softmax activations&lt;&#x2F;li&gt;
&lt;li&gt;use &lt;strong&gt;Kaiming Initialization&lt;&#x2F;strong&gt; for ReLU or Leaky ReLU activations&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;xavier-or-glorot-initialization&quot;&gt;Xavier or Glorot Initialization&lt;a class=&quot;zola-anchor&quot; href=&quot;#xavier-or-glorot-initialization&quot; aria-label=&quot;Anchor link for: xavier-or-glorot-initialization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;uniform-initialization&quot;&gt;Uniform initialization:&lt;a class=&quot;zola-anchor&quot; href=&quot;#uniform-initialization&quot; aria-label=&quot;Anchor link for: uniform-initialization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;bound a uniform distribution between $\pm \sqrt { \frac {6} {c_{in} + c_{out}}}$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;normal-initialization&quot;&gt;Normal initialization:&lt;a class=&quot;zola-anchor&quot; href=&quot;#normal-initialization&quot; aria-label=&quot;Anchor link for: normal-initialization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;multiply a normal distribution by $\sqrt \frac {2} {c_{in} + c_{out}}$&lt;&#x2F;li&gt;
&lt;li&gt;or create a normal distribution with $\mu = 0$ and $\sigma ^ 2 = \sqrt \frac {2} {c_{in} + c_{out}}$&lt;&#x2F;li&gt;
&lt;li&gt;helps keep identical variances across layers&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;kaiming-or-he-initialization&quot;&gt;Kaiming or He initialization&lt;a class=&quot;zola-anchor&quot; href=&quot;#kaiming-or-he-initialization&quot; aria-label=&quot;Anchor link for: kaiming-or-he-initialization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;when using a ReLU activation, $\sigma ^ 2$ will be close to $\sqrt \frac {c_{in}} {2}$, so multiplying the normally distributed activations by $\sqrt \frac {2} {c_{in}}$ will make the activations have a $\sigma ^ 2$ close to $1$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;uniform-initialization-1&quot;&gt;Uniform initialization:&lt;a class=&quot;zola-anchor&quot; href=&quot;#uniform-initialization-1&quot; aria-label=&quot;Anchor link for: uniform-initialization-1&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;bound a uniform distribution between $\pm \sqrt \frac {6} {c_{in}}$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;normal-initialization-1&quot;&gt;Normal initialization&lt;a class=&quot;zola-anchor&quot; href=&quot;#normal-initialization-1&quot; aria-label=&quot;Anchor link for: normal-initialization-1&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;multiply a normal distribution by $\sqrt \frac {2} {c_{in}}$&lt;&#x2F;li&gt;
&lt;li&gt;or create a normal distribution with $\mu = 0$ and $\sigma ^ 2 = \sqrt \frac {2} {c_{in}}$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;gain&quot;&gt;Gain&lt;a class=&quot;zola-anchor&quot; href=&quot;#gain&quot; aria-label=&quot;Anchor link for: gain&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;multiplied to init bounds&#x2F;stddevs&lt;&#x2F;li&gt;
&lt;li&gt;$\sqrt 2$ for ReLU&lt;&#x2F;li&gt;
&lt;li&gt;none for Kaiming&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;pytorch-defaults&quot;&gt;Pytorch defaults&lt;a class=&quot;zola-anchor&quot; href=&quot;#pytorch-defaults&quot; aria-label=&quot;Anchor link for: pytorch-defaults&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;most layers are initialized with Kaiming uniform as a reasonable default&lt;&#x2F;li&gt;
&lt;li&gt;use Kaiming with correct gain (https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;nn.html#torch.nn.init.calculate_gain)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources&quot; aria-label=&quot;Anchor link for: resources&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;15314&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;medium.com&#x2F;@sakeshpusuluri123&#x2F;activation-functions-and-weight-initialization-in-deep-learning-ebc326e62a5c&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;_modules&#x2F;torch&#x2F;nn&#x2F;init.html&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;whats-the-default-initialization-methods-for-layers&#x2F;3157&#x2F;21&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;towardsdatascience.com&#x2F;weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;towardsdatascience.com&#x2F;hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;nn.html#torch.nn.init.calculate_gain&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;mratsim&#x2F;Arraymancer&#x2F;blob&#x2F;master&#x2F;src&#x2F;nn&#x2F;init.nim&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;jamesmccaffrey.wordpress.com&#x2F;2018&#x2F;08&#x2F;21&#x2F;pytorch-neural-network-weights-and-biases-initialization&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
        </item>
        <item>
            <title>Machine Learning Flight Rules</title>
            <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/ml-flight-rules/</link>
            <guid>https://bkkaggle.github.io/blog/ml-flight-rules/</guid>
            <description>&lt;p&gt;&lt;em&gt;My Repository: https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;machine-learning-flight-rules&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;A guide for astronauts (now, people doing machine learning) about what to do when things go wrong.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-are-flight-rules&quot;&gt;What are &amp;quot;flight rules&amp;quot;?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-are-flight-rules&quot; aria-label=&quot;Anchor link for: what-are-flight-rules&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;em&gt;Copied from: https:&#x2F;&#x2F;github.com&#x2F;k88hudson&#x2F;git-flight-rules&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Flight Rules are the hard-earned body of knowledge recorded in manuals that list, step-by-step, what to do if X occurs, and why. Essentially, they are extremely detailed, scenario-specific standard operating procedures._&lt;&#x2F;p&gt;
&lt;p&gt;NASA has been capturing our missteps, disasters and solutions since the early 1960s, when Mercury-era ground teams first started gathering &amp;quot;lessons learned&amp;quot; into a compendium that now lists thousands of problematic situations, from engine failure to busted hatch handles to computer glitches, and their solutions._&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;— Chris Hadfield, &lt;em&gt;An Astronaut&#x27;s Guide to Life&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;general-tips&quot;&gt;General tips&lt;a class=&quot;zola-anchor&quot; href=&quot;#general-tips&quot; aria-label=&quot;Anchor link for: general-tips&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;https:&#x2F;&#x2F;karpathy.github.io&#x2F;2019&#x2F;04&#x2F;25&#x2F;recipe has some great best practices for training neural networks. Some of his tips include:&lt;&#x2F;p&gt;
&lt;h3 id=&quot;look-at-the-wrongly-classified-predictions-of-your-network&quot;&gt;Look at the wrongly classified predictions of your network&lt;a class=&quot;zola-anchor&quot; href=&quot;#look-at-the-wrongly-classified-predictions-of-your-network&quot; aria-label=&quot;Anchor link for: look-at-the-wrongly-classified-predictions-of-your-network&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This can help tell you what might be wrong with your dataset or model.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;always-set-the-random-seed&quot;&gt;Always set the random seed&lt;a class=&quot;zola-anchor&quot; href=&quot;#always-set-the-random-seed&quot; aria-label=&quot;Anchor link for: always-set-the-random-seed&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This will prevent (most, but not all!) variation in results between otherwise identical training runs.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;make-a-baseline-and-then-increase-the-size-of-your-model-until-it-overfits&quot;&gt;Make a baseline and then increase the size of your model until it overfits&lt;a class=&quot;zola-anchor&quot; href=&quot;#make-a-baseline-and-then-increase-the-size-of-your-model-until-it-overfits&quot; aria-label=&quot;Anchor link for: make-a-baseline-and-then-increase-the-size-of-your-model-until-it-overfits&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;use-a-very-simplified-baseline-to-test-that-your-code-works-correctly&quot;&gt;Use a very simplified baseline to test that your code works correctly&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-a-very-simplified-baseline-to-test-that-your-code-works-correctly&quot; aria-label=&quot;Anchor link for: use-a-very-simplified-baseline-to-test-that-your-code-works-correctly&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Use a simple model (e.g. a small resnet18 or linear regression) and confirm that your code works properly and as it is supposed to.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;overfit-on-a-single-batch&quot;&gt;Overfit on a single batch&lt;a class=&quot;zola-anchor&quot; href=&quot;#overfit-on-a-single-batch&quot; aria-label=&quot;Anchor link for: overfit-on-a-single-batch&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Try using as small of a batch size as you can (if you&#x27;re using batch normalization, that would be a batch of two examples). Your loss should go down to zero within a few iterations. If it doesn&#x27;t, that means you have a problem somewhere in your code.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;be-sure-that-you-re-data-has-been-correctly-processed&quot;&gt;Be sure that you&#x27;re data has been correctly processed&lt;a class=&quot;zola-anchor&quot; href=&quot;#be-sure-that-you-re-data-has-been-correctly-processed&quot; aria-label=&quot;Anchor link for: be-sure-that-you-re-data-has-been-correctly-processed&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Visualize your input data right before the &lt;code&gt;out = model(x)&lt;&#x2F;code&gt; to be sure that the data being sent to the network is correct (data has normalized properly, augmentations have been applied correctly, etc)&lt;&#x2F;p&gt;
&lt;h4 id=&quot;simple-models-complex-models&quot;&gt;Simple models -&amp;gt; complex models&lt;a class=&quot;zola-anchor&quot; href=&quot;#simple-models-complex-models&quot; aria-label=&quot;Anchor link for: simple-models-complex-models&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;In most cases, start with a simple model (eg: resnet18) then go on to using larger and more complex models (eg: SE-ResNeXt-101).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;start-with-a-simple-optimizer&quot;&gt;Start with a simple optimizer&lt;a class=&quot;zola-anchor&quot; href=&quot;#start-with-a-simple-optimizer&quot; aria-label=&quot;Anchor link for: start-with-a-simple-optimizer&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Adam is almost always a safe choice, It works well and doesn&#x27;t need extensive hyperparameter tuning. Kaparthy suggests using it with a learning rate of 3e-4.
I usually start with SGD with a learning rate of 0.1 and a momentum of 0.9 for most image classification and segmentation tasks.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;change-one-thing-at-a-time&quot;&gt;Change one thing at a time&lt;a class=&quot;zola-anchor&quot; href=&quot;#change-one-thing-at-a-time&quot; aria-label=&quot;Anchor link for: change-one-thing-at-a-time&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Change one hyperparameter&#x2F;augmentation&#x2F;architecture and see its effects on the performance of your network. Changing multiple things at a time won&#x27;t tell you what changes helped and which didn&#x27;t.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;regularize-your-model&quot;&gt;Regularize your model&lt;a class=&quot;zola-anchor&quot; href=&quot;#regularize-your-model&quot; aria-label=&quot;Anchor link for: regularize-your-model&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;get-more-data&quot;&gt;Get more data&lt;a class=&quot;zola-anchor&quot; href=&quot;#get-more-data&quot; aria-label=&quot;Anchor link for: get-more-data&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Training on more data will always decrease the amount of overfitting and is the easiest way to regularize a model&lt;&#x2F;p&gt;
&lt;h4 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;a class=&quot;zola-anchor&quot; href=&quot;#data-augmentation&quot; aria-label=&quot;Anchor link for: data-augmentation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This will artificially increase the size of your dataset and is the next best thing to collecting more data. Be sure that the augmentations you use make sense in the context of the task (flipping images of text in an OCR task left to right will hurt your model instead of helping it).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;use-a-pretrained-network&quot;&gt;Use a pretrained network&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-a-pretrained-network&quot; aria-label=&quot;Anchor link for: use-a-pretrained-network&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Pretrained networks (usually on Imagenet) help jumpstart your model especially when you have a smaller dataset. The domain of the pretrained network doesn&#x27;t usually prevent it from helping although pretraining on a similar domain will be better.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;decrease-the-batch-size&quot;&gt;Decrease the batch size&lt;a class=&quot;zola-anchor&quot; href=&quot;#decrease-the-batch-size&quot; aria-label=&quot;Anchor link for: decrease-the-batch-size&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Smaller batch sizes usually help increase regularization&lt;&#x2F;p&gt;
&lt;h4 id=&quot;use-early-stopping&quot;&gt;Use early stopping&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-early-stopping&quot; aria-label=&quot;Anchor link for: use-early-stopping&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Use the validation loss to only save the best performing checkpoint of the network after the val loss hasn&#x27;t gone down for a certain number of epochs&lt;&#x2F;p&gt;
&lt;h3 id=&quot;squeeze-out-more-performance-out-of-the-network&quot;&gt;Squeeze out more performance out of the network&lt;a class=&quot;zola-anchor&quot; href=&quot;#squeeze-out-more-performance-out-of-the-network&quot; aria-label=&quot;Anchor link for: squeeze-out-more-performance-out-of-the-network&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;ensemble&quot;&gt;Ensemble&lt;a class=&quot;zola-anchor&quot; href=&quot;#ensemble&quot; aria-label=&quot;Anchor link for: ensemble&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Ensemble multiple models either trained on different cross validation splits of the dataset or using different architectures. This always boosts performance by a few percentage points and gives you a more confident measure of the performance of the model on the dataset. Averaging metrics from models in an ensemble will help you figure out whether a change in the model is actually an improvement or random noise.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;use-early-stopping-on-the-val-metric&quot;&gt;Use early stopping on the val metric&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-early-stopping-on-the-val-metric&quot; aria-label=&quot;Anchor link for: use-early-stopping-on-the-val-metric&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;Increase the size of the model until you overfit, then add regularization&lt;&#x2F;li&gt;
&lt;li&gt;augmentation on mask&lt;&#x2F;li&gt;
&lt;li&gt;correlation in ensembles&lt;&#x2F;li&gt;
&lt;li&gt;noise in ensembling&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Another great resource for best practices when training neural networks is (http:&#x2F;&#x2F;amid.fish&#x2F;reproducing-deep-rl). This article focused on best practices for deep rl, but most of its recommendations are still useful on normal machine learning. Some of these tips include:&lt;&#x2F;p&gt;
&lt;h3 id=&quot;learn-to-deal-with-long-iteration-times&quot;&gt;Learn to deal with long iteration times&lt;a class=&quot;zola-anchor&quot; href=&quot;#learn-to-deal-with-long-iteration-times&quot; aria-label=&quot;Anchor link for: learn-to-deal-with-long-iteration-times&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Most normal programming (web development, IOS development, etc) iteration times usually range in the seconds, but iteration times in machine learning range from minutes to hours. This means that &amp;quot;experimenting a lot and thinking a little&amp;quot;, which is usually fine in other programming contexts, will make you waste a lot of time waiting for a training run to finish. Instead, spending more time thinking about what your code does and how it might not work will help you make less mistakes and waste less time.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;keep-a-log-of-what-you-re-working-on&quot;&gt;Keep a log of what you&#x27;re working on&lt;a class=&quot;zola-anchor&quot; href=&quot;#keep-a-log-of-what-you-re-working-on&quot; aria-label=&quot;Anchor link for: keep-a-log-of-what-you-re-working-on&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Keeping records (tensorboard graphs&#x2F;model checkpoints&#x2F;metrics) of training runs and configurations will really help you out when figuring out what worked and what didn&#x27;t. Additionally, keeping track of what you&#x27;re working on and your mindset as you&#x27;re working through a problem will help you when you have to come back to your work days or weeks later.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;try-to-predict-how-your-code-will-fail&quot;&gt;Try to predict how your code will fail&lt;a class=&quot;zola-anchor&quot; href=&quot;#try-to-predict-how-your-code-will-fail&quot; aria-label=&quot;Anchor link for: try-to-predict-how-your-code-will-fail&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Doing this will cut down on the amount of failures that seem obvious in retrospect. I&#x27;ve sometimes had problems where I knew what was wrong with my code before going through the code to debug it. To stop making as many obvious mistakes, I wouldn&#x27;t start a new training run if I was uncertain about whether it would work, and then would find and fix what might have gone wrong.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources&quot; aria-label=&quot;Anchor link for: resources&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;karpathy.github.io&#x2F;2019&#x2F;04&#x2F;25&#x2F;recipe&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;amid.fish&#x2F;reproducing-deep-rl&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;advanced-tips&quot;&gt;Advanced tips&lt;a class=&quot;zola-anchor&quot; href=&quot;#advanced-tips&quot; aria-label=&quot;Anchor link for: advanced-tips&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;some tips should be taken with a grain of salt&lt;&#x2F;li&gt;
&lt;li&gt;from: https:&#x2F;&#x2F;gist.github.com&#x2F;bkkaggle&#x2F;67bb9b5e6132e5d3c30e366c8d403369&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;basic-architectures-are-sometimes-better&quot;&gt;Basic architectures are sometimes better&lt;a class=&quot;zola-anchor&quot; href=&quot;#basic-architectures-are-sometimes-better&quot; aria-label=&quot;Anchor link for: basic-architectures-are-sometimes-better&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Always using the latest, most advanced, SOTA model for a task isn&#x27;t always the best choice. For example, Although more advanced semantic segmentation models like deeplab and pspnet are SOTA on datasets like PASCAL VOC and cityscapes, simpler architectures like U-nets are easier to train and adapt to new tasks and preform almost just as well on several recent kaggle competitions (https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;siim-acr-pneumothorax-segmentation&#x2F;discussion&#x2F;107824#latest-623920) (https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;tgs-salt-identification-challenge&#x2F;discussion&#x2F;69291#latest-592781).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;be-sure-that-code-that-you-copied-from-github-or-stackoverflow-is-correct&quot;&gt;Be sure that code that you copied from Github or Stackoverflow is correct&lt;a class=&quot;zola-anchor&quot; href=&quot;#be-sure-that-code-that-you-copied-from-github-or-stackoverflow-is-correct&quot; aria-label=&quot;Anchor link for: be-sure-that-code-that-you-copied-from-github-or-stackoverflow-is-correct&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;It&#x27;s a good idea to check code from Github and Stackoverflow to make sure it is correct and that you are using it in the correct way. In the Quora insincere questions classification Kaggle competition, a popular implementation of attention summed up the weighted features instead of weighting the actual features with the attention weights (https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;quora-insincere-questions-classification&#x2F;discussion&#x2F;79911) (https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;quora-insincere-questions-classification&#x2F;discussion&#x2F;76583#450210).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;don-t-excessively-tune-hyperparameters&quot;&gt;Don&#x27;t excessively tune hyperparameters&lt;a class=&quot;zola-anchor&quot; href=&quot;#don-t-excessively-tune-hyperparameters&quot; aria-label=&quot;Anchor link for: don-t-excessively-tune-hyperparameters&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Every time you tune hyperparameters on a validation set, you risk overfitting those hyperparameters to that validation set. If done correctly, the improvement from having better hyperparameters will outweigh the risk of having hyperparameters that don&#x27;t work well on the test set.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;set-up-cyclic-learning-rates-correctly&quot;&gt;Set up cyclic learning rates correctly&lt;a class=&quot;zola-anchor&quot; href=&quot;#set-up-cyclic-learning-rates-correctly&quot; aria-label=&quot;Anchor link for: set-up-cyclic-learning-rates-correctly&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you&#x27;re using a cyclic learning rate, be sure that the learning rate is at it&#x27;s lowest point when you have finished training.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;manually-init-layers&quot;&gt;Manually init layers&lt;a class=&quot;zola-anchor&quot; href=&quot;#manually-init-layers&quot; aria-label=&quot;Anchor link for: manually-init-layers&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Pytorch will automatically initialize layers for you, but depending on your activation function, you might want to use the correct gain for your activation function. Take a look at the pytorch &lt;a href=&quot;https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;nn.init.html&quot;&gt;documentation&lt;&#x2F;a&gt; for more information.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mixed-half-precision-training&quot;&gt;Mixed&#x2F;half precision training&lt;a class=&quot;zola-anchor&quot; href=&quot;#mixed-half-precision-training&quot; aria-label=&quot;Anchor link for: mixed-half-precision-training&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Mixed or half precision training lets you train on larger batch sizes and can speed up your training. Take a look at &lt;a href=&quot;https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;training-with-half-precision&#x2F;11815&quot;&gt;this&lt;&#x2F;a&gt; if you want to simply use half precision training.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-the-difference-between-mixed-and-half-precision-training&quot;&gt;What is the difference between mixed and half precision training?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-is-the-difference-between-mixed-and-half-precision-training&quot; aria-label=&quot;Anchor link for: what-is-the-difference-between-mixed-and-half-precision-training&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Nvidia&#x27;s Volta and Turing GPUs contain tensor cores that can do fast fp16 matrix multiplications and significantly speed up your training.&lt;&#x2F;p&gt;
&lt;p&gt;&amp;quot;True&amp;quot; half precision training casts the inputs and the model&#x27;s parameters to 16 bit floats and computes everything using 16 bit floats. The advantages of this are that fp16 floats only use half the amount of vram as normal fp32 floats, letting you double the batch size while training. This is the fastest and most optimized way to take advantage of tensor cores, but comes at a cost. Using fp16 floats for the model&#x27;s parameters and batch norm statistics means that if the gradients are small enough, they can underflow and be replaced by zeros.&lt;&#x2F;p&gt;
&lt;p&gt;Mixed precision solves these problems by keeping a master copy of the model&#x27;s parameters in 32 bit floats. The inputs and the model&#x27;s parameters are still cast to fp16, but after the backwards pass, the gradients are copied to the master copy and cast to fp32. The parameters are updated in fp32 to prevent gradients from underflowing, and the new, updated master copy&#x27;s parameters are cast to fp16 and copied to the original fp16 model. Nvidia&#x27;s apex library recommends using mixed precision in a different way by casting inputs to tensor core-friendly operations to fp16 and keeping other operations in fp32. Both of these mixed precision approaches have an overhead compared to half precision training, but are faster and use less vram than fp32 training.&lt;&#x2F;p&gt;
&lt;p&gt;Take a look at (https:&#x2F;&#x2F;medium.com&#x2F;huggingface&#x2F;training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255) for more information.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;apex-won-t-install-on-gcp-s-deep-learning-vm&quot;&gt;Apex won&#x27;t install on GCP&#x27;s deep learning vm&lt;a class=&quot;zola-anchor&quot; href=&quot;#apex-won-t-install-on-gcp-s-deep-learning-vm&quot; aria-label=&quot;Anchor link for: apex-won-t-install-on-gcp-s-deep-learning-vm&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This is a known issue with apex, take a look at (https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;apex&#x2F;issues&#x2F;259) for some possible solutions.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;resources-1&quot;&gt;Resources&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources-1&quot; aria-label=&quot;Anchor link for: resources-1&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you&#x27;re using pytorch, Nvidia&#x27;s apex library (https:&#x2F;&#x2F;docs.nvidia.com&#x2F;deeplearning&#x2F;sdk&#x2F;mixed-precision-training&#x2F;index.html) is the easiest way to start using mixed precision.
If you want to read more about half and mixed precision training, take a look at https:&#x2F;&#x2F;forums.fast.ai&#x2F;t&#x2F;mixed-precision-training&#x2F;20720&lt;&#x2F;p&gt;
&lt;h3 id=&quot;gradient-accumulation&quot;&gt;gradient accumulation&lt;a class=&quot;zola-anchor&quot; href=&quot;#gradient-accumulation&quot; aria-label=&quot;Anchor link for: gradient-accumulation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you want to train larger batches on a gpu without enough vram, gradient accumulation can help you out.&lt;&#x2F;p&gt;
&lt;p&gt;The basic idea is this: call &lt;code&gt;optimizer.step()&lt;&#x2F;code&gt; every n minibatches, accumulating the gradients at each minibatch, effectively training on a minibatch of size &lt;code&gt;batch_size x n&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Here&#x27;s a example showing how you could use gradient accumulation in pytorch, from (https:&#x2F;&#x2F;gist.github.com&#x2F;thomwolf&#x2F;ac7a7da6b1888c2eeac8ac8b9b05d3d3#file-gradient_accumulation-py):&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;model.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;zero_grad&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()                                   &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Reset gradients tensors
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;i, (inputs, labels) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5ebfcc;&quot;&gt;enumerate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(training_set):
    predictions &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(inputs)                     &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Forward pass
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;loss &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;loss_function&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(predictions, labels)       &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Compute loss function
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;loss &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;loss &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;accumulation_steps                &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Normalize our loss (if averaged)
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;loss.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;backward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()                                 &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Backward pass
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;% &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;accumulation_steps &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;:             &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Wait for several backward steps
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;optimizer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;step&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()                            &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Now we can do an optimizer step
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;model.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;zero_grad&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()                           &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Reset gradients tensors
        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;% &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;evaluation_steps &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;:           &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# Evaluate the model when we...
            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;evaluate_model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;()                        &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;# ...have no gradients accumulated
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you want to read more about gradient accumulation, check out this blog post (https:&#x2F;&#x2F;medium.com&#x2F;huggingface&#x2F;training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)&lt;&#x2F;p&gt;
&lt;h3 id=&quot;multi-gpu-machine-training&quot;&gt;multi gpu&#x2F;machine training&lt;a class=&quot;zola-anchor&quot; href=&quot;#multi-gpu-machine-training&quot; aria-label=&quot;Anchor link for: multi-gpu-machine-training&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you have multiple gpus, you can easily convert your current code to train your model on multiple gpus. Just follow the official tutorials on pytorch.org (https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;blitz&#x2F;data_parallel_tutorial.html). The only problem with this is that Pytorch&#x27;s build in &lt;code&gt;DataParallel&lt;&#x2F;code&gt; will gather the outputs from all the other gpus to gpu 1 to compute the loss and calculate gradients, using up more vram. There &lt;em&gt;is&lt;&#x2F;em&gt; an alternative to this though, just use this alternative balanced data parallel implementation (https:&#x2F;&#x2F;gist.github.com&#x2F;thomwolf&#x2F;7e2407fbd5945f07821adae3d9fd1312).&lt;&#x2F;p&gt;
&lt;p&gt;Take a look at (https:&#x2F;&#x2F;medium.com&#x2F;huggingface&#x2F;training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255#) for more information about multi gpu and distributed training.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;determinism&quot;&gt;determinism&lt;a class=&quot;zola-anchor&quot; href=&quot;#determinism&quot; aria-label=&quot;Anchor link for: determinism&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Pytorch will give you different results every time you run a script unless you set random seeds for python, numpy, and pytorch. Fortunately, doing this is very simple and only requires you to add a few lines to the top of each python file. There is a catch though, setting &lt;code&gt;torch.backends.cudnn.deterministic&lt;&#x2F;code&gt; to &lt;code&gt;True&lt;&#x2F;code&gt; will slightly slow down your network.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;SEED &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;42
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;np.random.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;seed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;SEED&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;manual_seed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;SEED&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
torch.cuda.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;manual_seed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;SEED&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
torch.backends.cudnn.deterministic &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;True
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you want a simple one-line way to do this, check out my &lt;code&gt;pytorch_zoo&lt;&#x2F;code&gt; library on github (https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo#seed_environmentseed42).&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;pytorch_zoo.utils &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;seed_environment

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;seed_environment&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;42&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you want more information on determinism in pytorch, take a look at these links:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;how-to-get-deterministic-behavior&#x2F;18177&#x2F;7&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;quora-insincere-questions-classification&#x2F;discussion&#x2F;72770&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;bminixhofer&#x2F;deterministic-neural-networks-using-pytorch&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;quora-insincere-questions-classification&#x2F;discussion&#x2F;72040&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;initialization&quot;&gt;Initialization&lt;a class=&quot;zola-anchor&quot; href=&quot;#initialization&quot; aria-label=&quot;Anchor link for: initialization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Take a look at my &lt;a href=&quot;&#x2F;blog&#x2F;2020&#x2F;7&#x2F;3&#x2F;initialization&quot;&gt;post&lt;&#x2F;a&gt; for more information.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;normalization&quot;&gt;Normalization&lt;a class=&quot;zola-anchor&quot; href=&quot;#normalization&quot; aria-label=&quot;Anchor link for: normalization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Take a look at my &lt;a href=&quot;&#x2F;blog&#x2F;2020&#x2F;3&#x2F;25&#x2F;normalization&quot;&gt;post&lt;&#x2F;a&gt; for an overview.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;batch-norm&quot;&gt;Batch norm&lt;a class=&quot;zola-anchor&quot; href=&quot;#batch-norm&quot; aria-label=&quot;Anchor link for: batch-norm&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The original batch normalization paper put the batch norm layer before the activation function, recent research shows that putting the batch norm layer after the activation gives better results. A great article on batch norm and why it works can be found here (https:&#x2F;&#x2F;blog.paperspace.com&#x2F;busting-the-myths-about-batch-normalization&#x2F;).&lt;&#x2F;p&gt;
&lt;h5 id=&quot;you-can-t-use-a-batch-size-of-1-with-batch-norm&quot;&gt;You can&#x27;t use a batch size of 1 with batch norm&lt;a class=&quot;zola-anchor&quot; href=&quot;#you-can-t-use-a-batch-size-of-1-with-batch-norm&quot; aria-label=&quot;Anchor link for: you-can-t-use-a-batch-size-of-1-with-batch-norm&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h5&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Batch norm relies on the mean and variance of all the elements in a batch, it won&#x27;t work if you&#x27;re using a batch size of one while training, so either skip over any leftover batches with batch sizes of 1 or increase the batch size to atleast 2.&lt;&#x2F;p&gt;
&lt;h5 id=&quot;be-sure-to-use-model-eval-with-batch-norm&quot;&gt;Be sure to use model.eval() with batch norm&lt;a class=&quot;zola-anchor&quot; href=&quot;#be-sure-to-use-model-eval-with-batch-norm&quot; aria-label=&quot;Anchor link for: be-sure-to-use-model-eval-with-batch-norm&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h5&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Run &lt;code&gt;model.eval()&lt;&#x2F;code&gt; before your validation loop to make sure pytorch uses the running mean and variance calculated over the training set. Also make sure to call &lt;code&gt;model.train()&lt;&#x2F;code&gt; before your training loop to start calculating the batch norm statistics again. You can read more about this at (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;what-does-model-eval-do-for-batchnorm-layer&#x2F;7146)&lt;&#x2F;p&gt;
&lt;h5 id=&quot;resources-2&quot;&gt;Resources&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources-2&quot; aria-label=&quot;Anchor link for: resources-2&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h5&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;http:&#x2F;&#x2F;mlexplained.com&#x2F;2018&#x2F;11&#x2F;30&#x2F;an-overview-of-normalization-methods-in-deep-learning&#x2F; is a really good blog post on the different types of normalizations and when to them.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;common-errors&quot;&gt;Common errors&lt;a class=&quot;zola-anchor&quot; href=&quot;#common-errors&quot; aria-label=&quot;Anchor link for: common-errors&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;pytorch&quot;&gt;Pytorch&lt;a class=&quot;zola-anchor&quot; href=&quot;#pytorch&quot; aria-label=&quot;Anchor link for: pytorch&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;losses&quot;&gt;Losses&lt;a class=&quot;zola-anchor&quot; href=&quot;#losses&quot; aria-label=&quot;Anchor link for: losses&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;cross-entropy-vs-nll-loss-for-multi-class-classification&quot;&gt;&lt;code&gt;cross_entropy&lt;&#x2F;code&gt; vs nll loss for multi-class classification&lt;a class=&quot;zola-anchor&quot; href=&quot;#cross-entropy-vs-nll-loss-for-multi-class-classification&quot; aria-label=&quot;Anchor link for: cross-entropy-vs-nll-loss-for-multi-class-classification&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Either pass the logits for a multi-class classification task to &lt;code&gt;log_softmax&lt;&#x2F;code&gt; first, then through the &lt;code&gt;nll&lt;&#x2F;code&gt; loss or pass the logits directly to &lt;code&gt;cross_entropy&lt;&#x2F;code&gt;. They will give you the same result, but &lt;code&gt;cross_entropy&lt;&#x2F;code&gt; is more numerically stable. Use &lt;code&gt;softmax&lt;&#x2F;code&gt; separately to convert logits into probabilities for prediction or for calculating metrics. Take a look at (https:&#x2F;&#x2F;sebastianraschka.com&#x2F;faq&#x2F;docs&#x2F;pytorch-crossentropy.html) for more information.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;binary-cross-entropy-vs-binary-cross-entropy-with-logits-for-binary-classification-tasks&quot;&gt;&lt;code&gt;binary_cross_entropy&lt;&#x2F;code&gt; vs &lt;code&gt;binary_cross_entropy_with_logits&lt;&#x2F;code&gt; for binary classification tasks&lt;a class=&quot;zola-anchor&quot; href=&quot;#binary-cross-entropy-vs-binary-cross-entropy-with-logits-for-binary-classification-tasks&quot; aria-label=&quot;Anchor link for: binary-cross-entropy-vs-binary-cross-entropy-with-logits-for-binary-classification-tasks&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Either pass the logits for a binary classification task to &lt;code&gt;sigmoid&lt;&#x2F;code&gt; first, then through &lt;code&gt;binary_cross_entropy&lt;&#x2F;code&gt; or pass the logits directly to &lt;code&gt;binary_cross_entropy_with_logits&lt;&#x2F;code&gt;. Just as the example above, they will give you the same result, but &lt;code&gt;binary_cross_entropy&lt;&#x2F;code&gt; is more numerically stable. Use &lt;code&gt;sigmoid&lt;&#x2F;code&gt; separately to conver the logits into probabilities for prediction or for calculating metrics. Again, take a look at (https:&#x2F;&#x2F;sebastianraschka.com&#x2F;faq&#x2F;docs&#x2F;pytorch-crossentropy.html) for more information.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;binary-classification-vs-multi-class-classification&quot;&gt;Binary classification vs multi-class classification&lt;a class=&quot;zola-anchor&quot; href=&quot;#binary-classification-vs-multi-class-classification&quot; aria-label=&quot;Anchor link for: binary-classification-vs-multi-class-classification&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;A binary classification task can also be represented as a multi-class classification task with two classes, positive and negative. They will give you the same result and should be numerically identical.&lt;&#x2F;p&gt;
&lt;p&gt;Here&#x27;s an example, taken from (https:&#x2F;&#x2F;sebastianraschka.com&#x2F;faq&#x2F;docs&#x2F;pytorch-crossentropy.html), on how you could do this:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;labels &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.float)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;probas &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.9&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.8&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.float)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.nn.functional.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;binary_cross_entropy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(probas, labels)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.1446&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)

&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;labels &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.long)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;probas &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;([[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.9&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;],
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.9&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;],
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;...                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.8&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;]], &lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.float)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;&amp;gt;&amp;gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.nn.functional.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;nll_loss&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;log&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(probas), labels)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0.1446&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;pin-memory-in-the-dataloader&quot;&gt;Pin memory in the dataloader&lt;a class=&quot;zola-anchor&quot; href=&quot;#pin-memory-in-the-dataloader&quot; aria-label=&quot;Anchor link for: pin-memory-in-the-dataloader&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Set &lt;code&gt;pin_memory&lt;&#x2F;code&gt; to &lt;code&gt;true&lt;&#x2F;code&gt; in your dataloader to speed up transferring your data from cpu to gpu. Take a look at this for more information (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;when-to-set-pin-memory-to-true&#x2F;19723).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;model-eval-vs-torch-no-grad&quot;&gt;&lt;code&gt;model.eval()&lt;&#x2F;code&gt; vs &lt;code&gt;torch.no_grad()&lt;&#x2F;code&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#model-eval-vs-torch-no-grad&quot; aria-label=&quot;Anchor link for: model-eval-vs-torch-no-grad&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;code&gt;model.eval()&lt;&#x2F;code&gt; will switch your dropout and batch norm layers to eval mode, turning off dropout and using the running mean and stddev for the batch norm layers. &lt;code&gt;torch.no_grad()&lt;&#x2F;code&gt; will tell pytorch to stop tracking operations, reducing memory usage and speeding up your evaluation loop. To use these properly, run &lt;code&gt;model.train()&lt;&#x2F;code&gt; before each training loop, run &lt;code&gt;model.eval()&lt;&#x2F;code&gt; before each evaluation loop, and wrap your evaluation loop with &lt;code&gt;with torch.no_grad():&lt;&#x2F;code&gt; Take a look at this for more information (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;model-eval-vs-with-torch-no-grad&#x2F;19615&#x2F;11).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-to-use-for-num-workers-in-the-dataloader&quot;&gt;What to use for &lt;code&gt;num_workers&lt;&#x2F;code&gt; in the dataloader&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-to-use-for-num-workers-in-the-dataloader&quot; aria-label=&quot;Anchor link for: what-to-use-for-num-workers-in-the-dataloader&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If your gpu utilization fluctuates a lot and generally remains low (&amp;lt; 90%), this might mean that your gpu is waiting for the cpu to finish processing all the elements in your batch and that &lt;code&gt;num_workers&lt;&#x2F;code&gt; might be your main bottleneck. &lt;code&gt;num_workers&lt;&#x2F;code&gt; in the dataloader is used to tell pytorch how many parallel workers to use to preprocess the data ahead of time. Set &lt;code&gt;num_workers&lt;&#x2F;code&gt; to the number of cores that you have in your cpu. This will fully utilize all your cpu cores to minimize the amount of time the gpu spends waiting for the cpu to process the data. If your gpu utilization still remains low, you should get more cpu cores or preprocess the data ahead of time and save it to disk. Take a look at these articles for more information: (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;guidelines-for-assigning-num-workers-to-dataloader) and (https:&#x2F;&#x2F;stanford.edu&#x2F;~shervine&#x2F;blog&#x2F;pytorch-how-to-generate-data-parallel).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tensorboard&quot;&gt;Tensorboard&lt;a class=&quot;zola-anchor&quot; href=&quot;#tensorboard&quot; aria-label=&quot;Anchor link for: tensorboard&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Tensorboard is really useful when you want to view your model&#x27;s training progress in real time. Now that Pytorch 1.1 is out, you can now log metrics directly to tensorboard from Pytorch.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-to-use-it&quot;&gt;How to use it&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-use-it&quot; aria-label=&quot;Anchor link for: how-to-use-it&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Follow these instructions for a quickstart (https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;intermediate&#x2F;tensorboard_tutorial.html).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;use-tensorboard-in-a-kaggle-kernel&quot;&gt;Use Tensorboard in a kaggle kernel&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-tensorboard-in-a-kaggle-kernel&quot; aria-label=&quot;Anchor link for: use-tensorboard-in-a-kaggle-kernel&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Just copy this code snippet into a cell at the top of your kernel&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;!mkdir logs
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;get_ipython&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;().&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;system_raw&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;#39;tensorboard --logdir .&#x2F;logs --host 0.0.0.0 --port 6006 &amp;amp;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
!ssh &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;o &lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;StrictHostKeyChecking no&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;R &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;80&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;:localhost:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;6006 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;serveo.net
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I also have another quickstart at my &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo#viewing-training-progress-with-tensorboard-in-a-kaggle-kernel&quot;&gt;pytorch_zoo&lt;&#x2F;a&gt; repository.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-do-all-the-tensorboard-histograms-mean&quot;&gt;What do all the Tensorboard histograms mean?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-do-all-the-tensorboard-histograms-mean&quot; aria-label=&quot;Anchor link for: what-do-all-the-tensorboard-histograms-mean&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Take a look at these stackoberflow posts:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;42315202&#x2F;understanding-tensorboard-weight-histograms&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;38149622&#x2F;what-is-a-good-explanation-of-how-to-read-the-histogram-feature-of-tensorboard&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;common-errors-1&quot;&gt;Common errors&lt;a class=&quot;zola-anchor&quot; href=&quot;#common-errors-1&quot; aria-label=&quot;Anchor link for: common-errors-1&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation&quot;&gt;RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation&lt;a class=&quot;zola-anchor&quot; href=&quot;#runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation&quot; aria-label=&quot;Anchor link for: runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;In place operations and operations on slices of tensors can cause problems with Pytorch&#x27;s autograd. To fix this, convert your inplace operation, &lt;code&gt;x[:, 0, :] += 1&lt;&#x2F;code&gt;, to a non inplace operation, &lt;code&gt;x[:, 0, :] = x[:, 0, :].clone() + 1&lt;&#x2F;code&gt;, and use &lt;code&gt;.clone()&lt;&#x2F;code&gt; to avoid problems with operations on tensor slices. Take a look at (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;encounter-the-runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation&#x2F;836) for more information.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;creating-mtgp-constants-failed-error&quot;&gt;Creating MTGP constants failed error&lt;a class=&quot;zola-anchor&quot; href=&quot;#creating-mtgp-constants-failed-error&quot; aria-label=&quot;Anchor link for: creating-mtgp-constants-failed-error&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This error happens when &amp;quot;using an embedding layer and passing out of range indexes (indexes &amp;gt; num_embeddings)&amp;quot; from (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;solved-creating-mtgp-constants-failed-error&#x2F;15084&#x2F;4). For more information, take a look at (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;solved-creating-mtgp-constants-failed-error&#x2F;15084).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;valueerror-expected-more-than-1-value-per-channel-when-training&quot;&gt;ValueError: Expected more than 1 value per channel when training&lt;a class=&quot;zola-anchor&quot; href=&quot;#valueerror-expected-more-than-1-value-per-channel-when-training&quot; aria-label=&quot;Anchor link for: valueerror-expected-more-than-1-value-per-channel-when-training&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This error happens when you&#x27;re using a batch size of 1 while training with batch norm. Batch norm expects to have a batch size of at least 2. For more information, take a look at (https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;4534)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;how-to&quot;&gt;How to&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to&quot; aria-label=&quot;Anchor link for: how-to&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;how-to-implement-gradient-clipping&quot;&gt;How to implement gradient clipping&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-implement-gradient-clipping&quot; aria-label=&quot;Anchor link for: how-to-implement-gradient-clipping&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Here&#x27;s the code for gradient clipping:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;torch.nn.utils.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;clip_grad_norm&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(model.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;parameters&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(), value)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you want to read more about gradient clipping in pytorch, take a look at (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;proper-way-to-do-gradient-clipping&#x2F;191).&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-to-implement-global-max-avg-pooling&quot;&gt;How to implement global max&#x2F;avg pooling&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-implement-global-max-avg-pooling&quot; aria-label=&quot;Anchor link for: how-to-implement-global-max-avg-pooling&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Follow the instructions from (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;global-max-pooling&#x2F;1345&#x2F;2)&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-to-release-gpu-memory&quot;&gt;How to release gpu memory&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-release-gpu-memory&quot; aria-label=&quot;Anchor link for: how-to-release-gpu-memory&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;There is no simple way to do this, but you can release as much memory as you can by running &lt;code&gt;torch.cuda.empty_cache()&lt;&#x2F;code&gt;. Take a look at (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;how-can-we-release-gpu-memory-cache&#x2F;14530) for more information.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-to-concatenate-hidden-states-of-a-bidirectional-lstm&quot;&gt;How to concatenate hidden states of a bidirectional lstm&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-concatenate-hidden-states-of-a-bidirectional-lstm&quot; aria-label=&quot;Anchor link for: how-to-concatenate-hidden-states-of-a-bidirectional-lstm&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Follow the instructions from (https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;concatenation-of-the-hidden-states-produced-by-a-bidirectional-lstm&#x2F;3686&#x2F;2).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;torchtext&quot;&gt;Torchtext&lt;a class=&quot;zola-anchor&quot; href=&quot;#torchtext&quot; aria-label=&quot;Anchor link for: torchtext&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Torchtext is Pytorch&#x27;s official NLP library, The library&#x27;s official &lt;a href=&quot;https:&#x2F;&#x2F;torchtext.readthedocs.io&#x2F;en&#x2F;latest&#x2F;index.html&quot;&gt;docs&lt;&#x2F;a&gt; are the best way to get started with the library, but are a bit limited and there are some blog posts that help you get a better sense of how to use the library:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;torchtext_translation_tutorial.html&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;text_sentiment_ngrams_tutorial.html&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;transformer_tutorial.html&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;anie.me&#x2F;On-Torchtext&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;mlexplained.com&#x2F;2018&#x2F;02&#x2F;08&#x2F;a-comprehensive-tutorial-to-torchtext&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;mlexplained.com&#x2F;2018&#x2F;02&#x2F;15&#x2F;language-modeling-tutorial-in-torchtext-practical-torchtext-part-2&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;towardsdatascience.com&#x2F;use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;towardsdatascience.com&#x2F;use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;torchtext_translation_tutorial.html&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;sort-batches-by-length&quot;&gt;Sort batches by length&lt;a class=&quot;zola-anchor&quot; href=&quot;#sort-batches-by-length&quot; aria-label=&quot;Anchor link for: sort-batches-by-length&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Your recurrent models will train best if all the examples in a batch have similar lengths. Since all the examples in a batch are padded with zeros to the length of the longest example, grouping examples with identical or similar lengths will make your model more efficient and waste less of the GPU&#x27;s memory. Use the iterator&#x27;s &lt;code&gt;sort_key&lt;&#x2F;code&gt; attribute to tell it to group examples of similar lengths into each batch. If you&#x27;re using &lt;code&gt;pack_padded_sequence&lt;&#x2F;code&gt;, set &lt;code&gt;sort_within_batch&lt;&#x2F;code&gt; to &lt;code&gt;True&lt;&#x2F;code&gt; since &lt;code&gt;pack_padded_sequence&lt;&#x2F;code&gt; expects examples in a batch to be in ascending order. Take a look at &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;text&#x2F;issues&#x2F;303&quot;&gt;this&lt;&#x2F;a&gt; for more information.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;text&#x2F;issues&#x2F;303&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;pretrained-embeddings&quot;&gt;Pretrained embeddings&lt;a class=&quot;zola-anchor&quot; href=&quot;#pretrained-embeddings&quot; aria-label=&quot;Anchor link for: pretrained-embeddings&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you want to use a pretrained embedding like word2vec or glove, you will have to load in the pretrained vectors and update the field&#x27;s vectors.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;# Load in the vectors
vectors = torchtext.vocab.Vectors(&amp;#39;&#x2F;path&#x2F;to&#x2F;vectors&amp;#39;)

# Create the text field
text_field = data.Field(tokenize=tokenizer, lower=True, batch_first=True, include_lengths=True)

# Built the vocab for the field using the train dataset
text_field.build_vocab(train_dataset)

# Set the vectors of the field to be the pretrained vectors
text_field.vocab.set_vectors(vectors.stoi, vectors.vectors, vectors.dim)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Take a look at &lt;a href=&quot;https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;aligning-torchtext-vocab-index-to-loaded-embedding-pre-trained-weights&#x2F;20878&quot;&gt;this&lt;&#x2F;a&gt; for more information.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;serializing-datasets&quot;&gt;Serializing datasets&lt;a class=&quot;zola-anchor&quot; href=&quot;#serializing-datasets&quot; aria-label=&quot;Anchor link for: serializing-datasets&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you&#x27;re working with large datasets that take time to load and process, being able to serialize and save processed datasets to disk is a really nice feature. Unfortunately, this feature is &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;text&#x2F;issues&#x2F;140&quot;&gt;still&lt;&#x2F;a&gt; a work in progress (the issue was created in 2017, and there doesn&#x27;t seem to be that much work being done on torchtext as of late 2019), so the only way to do this at the moment is to follow &lt;a href=&quot;https:&#x2F;&#x2F;towardsdatascience.com&#x2F;use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496&quot;&gt;this&lt;&#x2F;a&gt; article.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;kaggle&quot;&gt;Kaggle&lt;a class=&quot;zola-anchor&quot; href=&quot;#kaggle&quot; aria-label=&quot;Anchor link for: kaggle&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Here are some of tips and tricks I picked up while participating in kaggle competitions.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tips&quot;&gt;Tips&lt;a class=&quot;zola-anchor&quot; href=&quot;#tips&quot; aria-label=&quot;Anchor link for: tips&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;trust-your-local-validation&quot;&gt;Trust your local validation&lt;a class=&quot;zola-anchor&quot; href=&quot;#trust-your-local-validation&quot; aria-label=&quot;Anchor link for: trust-your-local-validation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Your score on your local validation set should be the most important, and sometimes the only, metric to pay attention to. Creating a validation set that you can trust to tell you whether you are or are not making progress is very important.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;optimize-for-the-metric&quot;&gt;Optimize for the metric&lt;a class=&quot;zola-anchor&quot; href=&quot;#optimize-for-the-metric&quot; aria-label=&quot;Anchor link for: optimize-for-the-metric&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The goal of kaggle competitions is to get the highest (or lowest, depending on the metric) score on a specific metric. To do this, you might need to modify your model&#x27;s loss function. For example, if the competition metric penalizes mistakes on rare classes more than common classes, oversampling or weighting the loss in favor of those classes can force the model to optimize for that metric.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;something-that-works-for-someone-might-not-work-for-you&quot;&gt;Something that works for someone might not work for you&lt;a class=&quot;zola-anchor&quot; href=&quot;#something-that-works-for-someone-might-not-work-for-you&quot; aria-label=&quot;Anchor link for: something-that-works-for-someone-might-not-work-for-you&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Just because someone says on the discussion forum that a particular technique or module works better for them doesn&#x27;t automatically mean that it will work for you.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;tricks&quot;&gt;Tricks&lt;a class=&quot;zola-anchor&quot; href=&quot;#tricks&quot; aria-label=&quot;Anchor link for: tricks&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;removing-negative-samples-from-a-dataset-is-equivalent-to-loss-weighting&quot;&gt;Removing negative samples from a dataset is equivalent to loss weighting&lt;a class=&quot;zola-anchor&quot; href=&quot;#removing-negative-samples-from-a-dataset-is-equivalent-to-loss-weighting&quot; aria-label=&quot;Anchor link for: removing-negative-samples-from-a-dataset-is-equivalent-to-loss-weighting&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This usually works well and is easier to do than loss weighting.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;thresholding&quot;&gt;Thresholding&lt;a class=&quot;zola-anchor&quot; href=&quot;#thresholding&quot; aria-label=&quot;Anchor link for: thresholding&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;h5 id=&quot;using-the-optimal-threshold-on-a-dataset-can-lead-to-brittle-results&quot;&gt;Using the optimal threshold on a dataset can lead to brittle results&lt;a class=&quot;zola-anchor&quot; href=&quot;#using-the-optimal-threshold-on-a-dataset-can-lead-to-brittle-results&quot; aria-label=&quot;Anchor link for: using-the-optimal-threshold-on-a-dataset-can-lead-to-brittle-results&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h5&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you choose thresholds for (binary) classification problems by choosing whatever value gives you the optimal score on a validation set, the threshold might be overfitting to the specific train-val split or to the specific architecture&#x2F;hyperparameters. This can have two effects. First, the optimial threshold you found on the val set might not be the optimal threshold on the held out test set, decreasing your score. Second, this makes comparing results between runs with different model architectures or hyperparameters more difficult. Using different thresholds means that a model that is actually worse might get a higher score than a better model if you find a &#x27;lucky&#x27; threshold.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;shakeup&quot;&gt;Shakeup&lt;a class=&quot;zola-anchor&quot; href=&quot;#shakeup&quot; aria-label=&quot;Anchor link for: shakeup&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Shakeup prediction is a powerful tool to predict the likely range of scores for your model when evaluated on an unknown test set. It was first introduced by the winner of a kaggle competition as a way to stabilize his models in (https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;planet-understanding-the-amazon-from-space&#x2F;discussion&#x2F;36809). It has also been used &lt;a href=&quot;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;tgs-salt-identification-challenge&#x2F;discussion&#x2F;67090&quot;&gt;here&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;quora-insincere-questions-classification&#x2F;discussion&#x2F;75821&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;encoding-categorical-features&quot;&gt;Encoding categorical features&lt;a class=&quot;zola-anchor&quot; href=&quot;#encoding-categorical-features&quot; aria-label=&quot;Anchor link for: encoding-categorical-features&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Encoding categorical features is a pretty important thing to do when working with tabular data.&lt;&#x2F;p&gt;
&lt;p&gt;Some resources I found for this are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;microsoft-malware-prediction&#x2F;discussion&#x2F;79045&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;vprokopev&#x2F;mean-likelihood-encodings-a-comprehensive-study&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;tnarik&#x2F;likelihood-encoding-of-categorical-features&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;microsoft-malware-prediction&#x2F;discussion&#x2F;76668&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;optimizing-code&quot;&gt;Optimizing code&lt;a class=&quot;zola-anchor&quot; href=&quot;#optimizing-code&quot; aria-label=&quot;Anchor link for: optimizing-code&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;save-processed-datasets-to-disk&quot;&gt;Save processed datasets to disk&lt;a class=&quot;zola-anchor&quot; href=&quot;#save-processed-datasets-to-disk&quot; aria-label=&quot;Anchor link for: save-processed-datasets-to-disk&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;As long as your dataset isn&#x27;t too large, saving the processed dataset to disk as a &lt;code&gt;.pkl&lt;&#x2F;code&gt; file, then loading it in whenever you need to use it, will save you time and will help increase your GPU utilization.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;use-multiprocessing&quot;&gt;Use multiprocessing&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-multiprocessing&quot; aria-label=&quot;Anchor link for: use-multiprocessing&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Python&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;docs.python.org&#x2F;2&#x2F;library&#x2F;multiprocessing.html&quot;&gt;&lt;code&gt;multiprocessing&lt;&#x2F;code&gt;&lt;&#x2F;a&gt; library can help you take full advantage of all the cores in your CPU.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;data-leaks&quot;&gt;Data Leaks&lt;a class=&quot;zola-anchor&quot; href=&quot;#data-leaks&quot; aria-label=&quot;Anchor link for: data-leaks&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Finding leaks in a dataset is a difficult, but sometimes useful skill.&lt;&#x2F;p&gt;
&lt;p&gt;Some good examples of how kagglers found leaks are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;raddar&#x2F;towards-de-anonymizing-the-data-some-insights&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;cpmpml&#x2F;raddar-magic-explained-a-bit&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;tools&quot;&gt;Tools&lt;a class=&quot;zola-anchor&quot; href=&quot;#tools&quot; aria-label=&quot;Anchor link for: tools&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;mxbi&#x2F;mlcrate&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo (I made this)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;ctr-click-through-rate-prediction-tools&quot;&gt;CTR (Click Through Rate prediction) tools&lt;a class=&quot;zola-anchor&quot; href=&quot;#ctr-click-through-rate-prediction-tools&quot; aria-label=&quot;Anchor link for: ctr-click-through-rate-prediction-tools&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;guoday&#x2F;ctrNet-tool&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;avazu-ctr-prediction&#x2F;discussion&#x2F;10927&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;microsoft-malware-prediction&#x2F;discussion&#x2F;75149&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;scirpus&#x2F;microsoft-libffm-munger&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;talkingdata-adtracking-fraud-detection&#x2F;discussion&#x2F;56497#331685&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;ftrl-follow-the-regularized-leader&quot;&gt;FTRL (Follow The Regularized Leader)&lt;a class=&quot;zola-anchor&quot; href=&quot;#ftrl-follow-the-regularized-leader&quot; aria-label=&quot;Anchor link for: ftrl-follow-the-regularized-leader&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;microsoft-malware-prediction&#x2F;discussion&#x2F;75246&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;ensembling&quot;&gt;Ensembling&lt;a class=&quot;zola-anchor&quot; href=&quot;#ensembling&quot; aria-label=&quot;Anchor link for: ensembling&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;correlation&quot;&gt;Correlation&lt;a class=&quot;zola-anchor&quot; href=&quot;#correlation&quot; aria-label=&quot;Anchor link for: correlation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Ensembling models with low correlations is better than ensembling models with high correlations.&lt;&#x2F;p&gt;
&lt;p&gt;More information can be found here:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;microsoft-malware-prediction&#x2F;discussion&#x2F;80368&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;jigsaw-toxic-comment-classification-challenge&#x2F;discussion&#x2F;51058&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;semantic-segmentation&quot;&gt;Semantic segmentation&lt;a class=&quot;zola-anchor&quot; href=&quot;#semantic-segmentation&quot; aria-label=&quot;Anchor link for: semantic-segmentation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Some good resources for semantic segmentation include:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;http:&#x2F;&#x2F;blog.qure.ai&#x2F;notes&#x2F;semantic-segmentation-deep-learning-review&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;tuatini.me&#x2F;practical-image-segmentation-with-unet&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.jeremyjordan.me&#x2F;semantic-segmentation&#x2F;#loss&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;becominghuman.ai&#x2F;investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;nlp&quot;&gt;NLP&lt;a class=&quot;zola-anchor&quot; href=&quot;#nlp&quot; aria-label=&quot;Anchor link for: nlp&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Take a look at some of these blog posts:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;http:&#x2F;&#x2F;ruder.io&#x2F;a-review-of-the-recent-history-of-nlp&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;medium.com&#x2F;huggingface&#x2F;learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;medium.com&#x2F;huggingface&#x2F;100-times-faster-natural-language-processing-in-python-ee32033bdced&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;awd-lstm&quot;&gt;awd-LSTM&lt;a class=&quot;zola-anchor&quot; href=&quot;#awd-lstm&quot; aria-label=&quot;Anchor link for: awd-lstm&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Take a look at these links:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;awd-lstm-lm&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.fast.ai&#x2F;2017&#x2F;08&#x2F;25&#x2F;language-modeling-sota&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;multitask-learning&quot;&gt;Multitask learning&lt;a class=&quot;zola-anchor&quot; href=&quot;#multitask-learning&quot; aria-label=&quot;Anchor link for: multitask-learning&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Take a look at these links:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;http:&#x2F;&#x2F;ruder.io&#x2F;multi-task&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;ruder.io&#x2F;multi-task-learning-nlp&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;combine-pretrained-embeddings&quot;&gt;Combine pretrained embeddings&lt;a class=&quot;zola-anchor&quot; href=&quot;#combine-pretrained-embeddings&quot; aria-label=&quot;Anchor link for: combine-pretrained-embeddings&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Adding&#x2F;concatenating&#x2F;(weighted) averaging multiple pretrained embeddings almost always leads to a boost in accuracy.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;reinitialize-random-embedding-matrices-between-models&quot;&gt;Reinitialize random embedding matrices between models&lt;a class=&quot;zola-anchor&quot; href=&quot;#reinitialize-random-embedding-matrices-between-models&quot; aria-label=&quot;Anchor link for: reinitialize-random-embedding-matrices-between-models&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Initializing embeddings for unknown words randomly helps increase the diversity between models.&lt;&#x2F;p&gt;
&lt;p&gt;From: (https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;quora-insincere-questions-classification&#x2F;discussion&#x2F;79720)&lt;&#x2F;p&gt;
&lt;h3 id=&quot;try-out-dropout-or-gaussian-noise-after-the-embedding-layer&quot;&gt;Try out dropout or gaussian noise after the embedding layer&lt;a class=&quot;zola-anchor&quot; href=&quot;#try-out-dropout-or-gaussian-noise-after-the-embedding-layer&quot; aria-label=&quot;Anchor link for: try-out-dropout-or-gaussian-noise-after-the-embedding-layer&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;It can help increase model diversity and decrease overfitting&lt;&#x2F;p&gt;
&lt;h3 id=&quot;correctly-use-masking-with-softmax&quot;&gt;Correctly use masking with softmax&lt;a class=&quot;zola-anchor&quot; href=&quot;#correctly-use-masking-with-softmax&quot; aria-label=&quot;Anchor link for: correctly-use-masking-with-softmax&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;use-dynamic-minibatches-when-training-sequence-models&quot;&gt;Use dynamic minibatches when training sequence models&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-dynamic-minibatches-when-training-sequence-models&quot; aria-label=&quot;Anchor link for: use-dynamic-minibatches-when-training-sequence-models&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Using this will try to create batches of examples with equal lengths to minimize unncessary padding and wasted calculations. The code to use this is available at (https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;jigsaw-unintended-bias-in-toxicity-classification&#x2F;discussion&#x2F;94779)&lt;&#x2F;p&gt;
&lt;h3 id=&quot;reduce-the-amount-of-oov-out-of-vocabulary-words&quot;&gt;Reduce the amount of OOV (Out Of Vocabulary) words&lt;a class=&quot;zola-anchor&quot; href=&quot;#reduce-the-amount-of-oov-out-of-vocabulary-words&quot; aria-label=&quot;Anchor link for: reduce-the-amount-of-oov-out-of-vocabulary-words&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;creating-a-vocabulary-on-the-train-val-sets-between-folds-can-lead-to-information-being-leaked-and-artificially-increasing-your-score&quot;&gt;Creating a vocabulary on the train, val sets between folds can lead to information being leaked and artificially increasing your score&lt;a class=&quot;zola-anchor&quot; href=&quot;#creating-a-vocabulary-on-the-train-val-sets-between-folds-can-lead-to-information-being-leaked-and-artificially-increasing-your-score&quot; aria-label=&quot;Anchor link for: creating-a-vocabulary-on-the-train-val-sets-between-folds-can-lead-to-information-being-leaked-and-artificially-increasing-your-score&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;quora-insincere-questions-classification&#x2F;discussion&#x2F;79556&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;how-to-use-pad-packed-sequence-and-pack-padded-sequence&quot;&gt;How to use &lt;code&gt;pad_packed_sequence&lt;&#x2F;code&gt; and &lt;code&gt;pack_padded_sequence&lt;&#x2F;code&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-use-pad-packed-sequence-and-pack-padded-sequence&quot; aria-label=&quot;Anchor link for: how-to-use-pad-packed-sequence-and-pack-padded-sequence&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Take a look at these links:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;packedsequence-for-seq2seq-model&#x2F;3907&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;solved-multiple-packedsequence-input-ordering&#x2F;2106&#x2F;7&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;transformers&quot;&gt;Transformers&lt;a class=&quot;zola-anchor&quot; href=&quot;#transformers&quot; aria-label=&quot;Anchor link for: transformers&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;p&gt;Take a look at these links:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;blog.floydhub.com&#x2F;the-transformer-in-pytorch&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;www.wildml.com&#x2F;2016&#x2F;01&#x2F;attention-and-memory-in-deep-learning-and-nlp&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;jalammar.github.io&#x2F;illustrated-transformer&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;gradient-boosting&quot;&gt;Gradient boosting&lt;a class=&quot;zola-anchor&quot; href=&quot;#gradient-boosting&quot; aria-label=&quot;Anchor link for: gradient-boosting&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;how-to-set-hyperparameters&quot;&gt;How to set hyperparameters&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-set-hyperparameters&quot; aria-label=&quot;Anchor link for: how-to-set-hyperparameters&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Laurae&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;sites.google.com&#x2F;view&#x2F;lauraepp&#x2F;parameters&quot;&gt;website&lt;&#x2F;a&gt; is the best place to understand what parameters to use and what values to set them to.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;resources-3&quot;&gt;Resources&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources-3&quot; aria-label=&quot;Anchor link for: resources-3&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;c&#x2F;microsoft-malware-prediction&#x2F;discussion&#x2F;78253&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;mlexplained.com&#x2F;2018&#x2F;01&#x2F;05&#x2F;lightgbm-and-xgboost-explained&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;xgboost.readthedocs.io&#x2F;en&#x2F;latest&#x2F;tutorials&#x2F;model.html&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;lightgbm.readthedocs.io&#x2F;en&#x2F;latest&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;xlearn-doc.readthedocs.io&#x2F;en&#x2F;latest&#x2F;index.html&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;catboost.ai&#x2F;docs&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;setting-up-your-environment&quot;&gt;Setting up your environment&lt;a class=&quot;zola-anchor&quot; href=&quot;#setting-up-your-environment&quot; aria-label=&quot;Anchor link for: setting-up-your-environment&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;jupyter-notebooks&quot;&gt;Jupyter notebooks&lt;a class=&quot;zola-anchor&quot; href=&quot;#jupyter-notebooks&quot; aria-label=&quot;Anchor link for: jupyter-notebooks&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;43759610&#x2F;how-to-add-python-3-6-kernel-alongside-3-5-on-jupyter&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;forums.fast.ai&#x2F;t&#x2F;jupyter-notebook-keyerror-allow-remote-access&#x2F;24392&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;python-3-6&quot;&gt;Python 3.6+&lt;a class=&quot;zola-anchor&quot; href=&quot;#python-3-6&quot; aria-label=&quot;Anchor link for: python-3-6&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.rosehosting.com&#x2F;blog&#x2F;how-to-install-python-3-6-4-on-debian-9&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h4 id=&quot;conda&quot;&gt;Conda&lt;a class=&quot;zola-anchor&quot; href=&quot;#conda&quot; aria-label=&quot;Anchor link for: conda&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;conda.io&#x2F;projects&#x2F;conda&#x2F;en&#x2F;latest&#x2F;user-guide&#x2F;tasks&#x2F;manage-environments.html#activating-an-environment&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;35245401&#x2F;combining-conda-environment-yml-with-pip-requirements-txt&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;42352841&#x2F;how-to-update-an-existing-conda-environment-with-a-yml-file&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;build-your-own-library&quot;&gt;Build your own library&lt;a class=&quot;zola-anchor&quot; href=&quot;#build-your-own-library&quot; aria-label=&quot;Anchor link for: build-your-own-library&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;I recently built my own machine learning &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;L2&quot;&gt;library&lt;&#x2F;a&gt;, here are some of the resources I used:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;medium.com&#x2F;@florian.caesar&#x2F;how-to-create-a-machine-learning-framework-from-scratch-in-491-steps-93428369a4eb&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;joelgrus&#x2F;joelnet&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;medium.com&#x2F;@johan.mabille&#x2F;how-we-wrote-xtensor-1-n-n-dimensional-containers-f79f9f4966a7&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;mlfromscratch.com&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;eisenjulian.github.io&#x2F;deep-learning-in-100-lines&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;blog.ezyang.com&#x2F;2019&#x2F;05&#x2F;pytorch-internals&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;resources-4&quot;&gt;Resources&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources-4&quot; aria-label=&quot;Anchor link for: resources-4&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;essential-tools&quot;&gt;Essential tools&lt;a class=&quot;zola-anchor&quot; href=&quot;#essential-tools&quot; aria-label=&quot;Anchor link for: essential-tools&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;paperswithcode.com - This website lists available implementations of papers along with leaderboards showing which models are currently SOTA on a range of tasks and datasets&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.arxiv-vanity.com - This site converts PDF papers from Arxiv to mobile-friendly responsive web pages.&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;www.arxiv-sanity.com - This site is a better way to keep up to date with popular and interesting papers.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;model-zoos&quot;&gt;Model zoos&lt;a class=&quot;zola-anchor&quot; href=&quot;#model-zoos&quot; aria-label=&quot;Anchor link for: model-zoos&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;modelzoo.co&#x2F;blog&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;modeldepot.io&#x2F;search&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;sebastianruder&#x2F;NLP-progress&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;arxiv-alternatives&quot;&gt;Arxiv alternatives&lt;a class=&quot;zola-anchor&quot; href=&quot;#arxiv-alternatives&quot; aria-label=&quot;Anchor link for: arxiv-alternatives&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.arxiv-vanity.com&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;www.arxiv-sanity.com&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.scihive.org&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;machine-learning-demos&quot;&gt;Machine learning demos&lt;a class=&quot;zola-anchor&quot; href=&quot;#machine-learning-demos&quot; aria-label=&quot;Anchor link for: machine-learning-demos&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;ganbreeder.app&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;talktotransformer.com&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;transformer.huggingface.co&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;research&#x2F;ai-playground&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;alantian.net&#x2F;ganshowcase&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;rowanzellers.com&#x2F;grover&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;nvidia-research-mingyuliu.com&#x2F;gaugan&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;http:&#x2F;&#x2F;nvidia-research-mingyuliu.com&#x2F;petswap&#x2F;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;link-aggregators&quot;&gt;Link aggregators&lt;a class=&quot;zola-anchor&quot; href=&quot;#link-aggregators&quot; aria-label=&quot;Anchor link for: link-aggregators&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;news.ycombinator.com&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.sciencewiki.com&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;git.news&#x2F;?ref=producthunt&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;machine-learning-as-a-service&quot;&gt;Machine learning as a service&lt;a class=&quot;zola-anchor&quot; href=&quot;#machine-learning-as-a-service&quot; aria-label=&quot;Anchor link for: machine-learning-as-a-service&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;runwayml.com&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;supervise.ly&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;coreml&quot;&gt;Coreml&lt;a class=&quot;zola-anchor&quot; href=&quot;#coreml&quot; aria-label=&quot;Anchor link for: coreml&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;developer.apple.com&#x2F;machine-learning&#x2F;models&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;swift-coreml-transformers&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.fritz.ai&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;courses&quot;&gt;Courses&lt;a class=&quot;zola-anchor&quot; href=&quot;#courses&quot; aria-label=&quot;Anchor link for: courses&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;fast.ai&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.coursera.org&#x2F;learn&#x2F;competitive-data-science&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.deeplearning.ai&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.kaggle.com&#x2F;learn&#x2F;overview&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;miscelaneous&quot;&gt;Miscelaneous&lt;a class=&quot;zola-anchor&quot; href=&quot;#miscelaneous&quot; aria-label=&quot;Anchor link for: miscelaneous&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;markus-beuckelmann.de&#x2F;blog&#x2F;boosting-numpy-blas.html&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;Wookai&#x2F;paper-tips-and-tricks&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;dennybritz&#x2F;deeplearning-papernotes&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;HarisIqbal88&#x2F;PlotNeuralNet&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;contributing&quot;&gt;Contributing&lt;a class=&quot;zola-anchor&quot; href=&quot;#contributing&quot; aria-label=&quot;Anchor link for: contributing&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;I&#x27;ve tried to make sure that all the information in this repository is accurate, but if you find something that you think is wrong, please let me know by opening an issue.&lt;&#x2F;p&gt;
&lt;p&gt;This repository is still a work in progress, so if you find a bug, think there is something missing, or have any suggestions for new features, feel free to open an issue or a pull request. Feel free to use the library or code from it in your own projects, and if you feel that some code used in this project hasn&#x27;t been properly accredited, please open an issue.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;authors&quot;&gt;Authors&lt;a class=&quot;zola-anchor&quot; href=&quot;#authors&quot; aria-label=&quot;Anchor link for: authors&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Bilal Khan&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;license&quot;&gt;License&lt;a class=&quot;zola-anchor&quot; href=&quot;#license&quot; aria-label=&quot;Anchor link for: license&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;p&gt;This project is licensed under the CC-BY-SA-4.0 License - see the &lt;a href=&quot;https:&#x2F;&#x2F;bkkaggle.github.io&#x2F;blog&#x2F;ml-flight-rules&#x2F;LICENSE&quot;&gt;license&lt;&#x2F;a&gt; file for details&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;a class=&quot;zola-anchor&quot; href=&quot;#acknowledgements&quot; aria-label=&quot;Anchor link for: acknowledgements&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;k88hudson&lt;&#x2F;em&gt; - &lt;em&gt;Parts of https:&#x2F;&#x2F;github.com&#x2F;k88hudson&#x2F;git-flight-rules were used in this repository&lt;&#x2F;em&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This repository was inspired by https:&#x2F;&#x2F;github.com&#x2F;k88hudson&#x2F;git-flight-rules and copied over parts of it&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>NLP Reseach Project Part 1</title>
            <pubDate>Mon, 22 Jun 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/nlp-research-part-1/</link>
            <guid>https://bkkaggle.github.io/blog/nlp-research-part-1/</guid>
            <description>&lt;hr &#x2F;&gt;
&lt;blockquote&gt;
&lt;p&gt;Part 1: Best Practices for Finetuning Large Transformer Language models&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;&#x2F;blog&#x2F;nlp-research-part-2&quot;&gt;Part 2: How I (almost) replicated OpenAI&#x27;s GPT-2 (124M version)&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;background&quot;&gt;Background&lt;a class=&quot;zola-anchor&quot; href=&quot;#background&quot; aria-label=&quot;Anchor link for: background&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;A few months ago I started working on a research project on how to best finetune GPT2-like language models for text generation. Once I ran a few experiments on that, I wanted to expand the scope of the project and try to pretrain my own, more efficient language model from scratch. I got access to a 128-core TPUv3 pod from the Tensorflow Reseach Cloud and used it to pretrain GPT2-124M to a perplexity pretty close to OpenAI&#x27;s results (my pretrained model used was trained for about $1&#x2F;8$th of the number of iterations that OpenAI trained their model for and got $21$ ppl on OpenWebText compared to $17$ ppl for OpenAI&#x27;s model), and then pretrained an ALBERT-style GPT2 (that I&#x27;m calling ALGPT2) language model with a factorized input embedding and parameter sharing that would reduce the number of paramters from 124M to around 12M.&lt;&#x2F;p&gt;
&lt;p&gt;Unfortunately, ALGPT2 doesn&#x27;t generate coherent, natural sounding text as well as GPT2 (ALGPT2 gets $31$ ppl on OpenWebText compared to $21$ ppl for my pretrained GPT2 model), but I&#x27;m writing this series of blog posts to go through everything I&#x27;ve learned over the last few months.&lt;&#x2F;p&gt;
&lt;p&gt;I have a cleaned-up version of my codebase on Github &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-training-research-project&quot;&gt;here&lt;&#x2F;a&gt;, and my original codebase with all my notes &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-finetuning&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;You can take a look at my Weights&amp;amp;Biases workspace with all my runs &lt;a href=&quot;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;objectives&quot;&gt;Objectives&lt;a class=&quot;zola-anchor&quot; href=&quot;#objectives&quot; aria-label=&quot;Anchor link for: objectives&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I don&#x27;t usually have access to a lot of compute (I mostly just use Google Colab) so I started out by limiting the scope of my project to finetuning or running inference on GPT2. I wrote down a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;blob&#x2F;master&#x2F;Markdown&#x2F;RESEARCH.md#objectives&quot;&gt;few notes&lt;&#x2F;a&gt; on what I wanted to look into:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;If I wanted to finetune a LM to generate text of a specific style&#x2F;content, what good defaults would I choose?&lt;&#x2F;li&gt;
&lt;li&gt;Find best practices or good defaults for finetuning tranformer language models for text generation.&lt;&#x2F;li&gt;
&lt;li&gt;Understand the effect of context len, model, and dataset size on generating coherent text&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;data&quot;&gt;Data&lt;a class=&quot;zola-anchor&quot; href=&quot;#data&quot; aria-label=&quot;Anchor link for: data&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I ran most of the finetuning experiments on WikiText-2, which was small enough (~10mb on disk with a total of ~2m words) that I could run experiments fast enough (usually within 5-10m) on the v100 or p100 that I usually got through Colab.&lt;&#x2F;p&gt;
&lt;p&gt;I also ran a few experiments using WikiText-103 (~500mb, 100m words) but these were a lot harder to do because the size of the dataset forced me to use smaller batch sizes which took too long.&lt;&#x2F;p&gt;
&lt;p&gt;Loading in larger datasets (like WikiText-103) into memory can become pretty inefficient because of Python&#x27;s overhead. IIRC, if you want to load the &lt;em&gt;entire&lt;&#x2F;em&gt; WikiText-103 train set into memory with Python and tokenize the whole thing in one go using Huggingface&#x27;s Tokenizers library and the GPT2 byte-level BPE tokenizer, it takes about 10 minutes and uses up about 60GB of RAM (Most of the time is spent tokenizing and most of the RAM is used loading the file into memory). Using something like &lt;a href=&quot;https:&#x2F;&#x2F;arrow.apache.org&#x2F;&quot;&gt;Apache Arrow&lt;&#x2F;a&gt; like the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;nlp&quot;&gt;Huggingface NLP&lt;&#x2F;a&gt; library should make this a whole lot more efficient.&lt;&#x2F;p&gt;
&lt;p&gt;The WikiText datasets are stored as a single text file, with one Wikipedia article per line. Another more efficient way of processing the data would be to load in the file line-by-line and tokenize each line in parallel using the Huggingface tokenizer library&#x27;s &lt;code&gt;batch_encode_plus()&lt;&#x2F;code&gt; function. This is a lot faster and efficient (taking up only 2GB of RAM and 2 minutes) but has its own drawbacks. &lt;code&gt;batch_encode_plus()&lt;&#x2F;code&gt; truncates sequences beyond that have more than &lt;code&gt;context_len&lt;&#x2F;code&gt; tokens and leaves sequences that are smaller than &lt;code&gt;context_len&lt;&#x2F;code&gt; as is, which means that you need to zero-pad the sequences that are smaller than &lt;code&gt;context_len&lt;&#x2F;code&gt; and discard any portion of a sequence beyond the first &lt;code&gt;context_len&lt;&#x2F;code&gt; tokens. For datasets that are used to benchmark the performance of a wide range of language models, this can lead to your model being harder to compare against other models that follow the commonly used convention of just tokenizing the entire dataset and chunking it into sequences of &lt;code&gt;context_len&lt;&#x2F;code&gt; length.&lt;&#x2F;p&gt;
&lt;p&gt;I wanted to make sure the way that I preprocessed the data made sure that the models that I finetuned on WikiText-2 and WikiText-103 would be comparable to other models, so in my code, I load in the entire dataset, tokenize it, and split it into contigous sequences of length &lt;code&gt;context_len&lt;&#x2F;code&gt;. There are a few other preprocessing-related factors that can affect how comparable results between different models can be, I &lt;a href=&quot;&#x2F;blog&#x2F;2020&#x2F;5&#x2F;14&#x2F;evaluating-language-models&#x2F;&quot;&gt;wrote a post on the topic&lt;&#x2F;a&gt; a while ago, check it out if you&#x27;re interested.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;frameworks&quot;&gt;Frameworks&lt;a class=&quot;zola-anchor&quot; href=&quot;#frameworks&quot; aria-label=&quot;Anchor link for: frameworks&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I originally wrote all my code in vanilla Pytorch. I wanted to try using Colab&#x27;s free TPUv3 board that has 8 TPU cores each with 16GB of RAM, each of which is a little slower that a single V100. Using the entire TPU board should be &lt;em&gt;at least&lt;&#x2F;em&gt; as fast as using a cluster of 8 V100s but at a much lower cost.&lt;&#x2F;p&gt;
&lt;p&gt;I tried using &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;PyTorchLightning&#x2F;pytorch-lightning&quot;&gt;Pytorch Lightning&lt;&#x2F;a&gt; to see if it would help make the Pytorch model run on the Colab TPU more easily, but after about a week of trying to use the library I switched over to having two training scripts; One in plain Pytorch and one in Tensorflow2.0 with Keras — Even though Pytorch Lightning was very well designed, the time and effort required to make sure the framework is working the way that I want wasn&#x27;t worth it in the end.&lt;&#x2F;p&gt;
&lt;p&gt;This was my first time working with TF&#x2F;Keras since around early 2018 when I switched to Pytorch (back in the 0.3 days when you still had to use &lt;code&gt;Variable&lt;&#x2F;code&gt;). TF2.0 is a lot better now than it used to be two or three years ago but still doesn&#x27;t feel as intuitive or easy to use as Pytorch. The documentation looks pretty good at first glance but there were a lot of gaps in the documentation when I was trying to figure out how to write and decode TFRecord files and scale my Keras code to TPUs and TPU pods.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Trying to get gradient accumulation to work with TPUs was especially hard, IIRC grad accumulation isn&#x27;t natively supported in Keras but there are a lot of independent implementations that people have open-sourced on Github, but they didn&#x27;t work well with TPUs.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I used the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&quot;&gt;Huggingface&#x2F;Transformers&lt;&#x2F;a&gt; repository for the GPT-2 model and &lt;a href=&quot;https:&#x2F;&#x2F;www.wandb.com&#x2F;&quot;&gt;Weights&amp;amp;Biases&lt;&#x2F;a&gt; to track all the experiments that I ran.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fun fact, I ran into a problem with Colab&#x27;s TPU a couple of times where I was silently downgraded from a TPUv3 to a TPUv2 and as a result I was getting a lot of OOM errors for a model and batch size that was working perfectly just a few hours ago. Colab doesn&#x27;t really advertise this and makes it almost impossible to know if you have been downgraded :(&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Pytorch recently released &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;xla&quot;&gt;Pytorch&#x2F;XLA&lt;&#x2F;a&gt; which is supposed to let you run Pytorch code on TPUs with only a few changes to your code. I spent quite a bit of time to try and make this work but using it is still a lot more complex than just using a GPU.&lt;&#x2F;p&gt;
&lt;p&gt;Pytorch&#x2F;XLA is a lot slower on Colab, which probably has something to do with Colab&#x27;s network connection to the TPUs being a lot slower. Using &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;xla&#x2F;issues&#x2F;1777&quot;&gt;some operations&lt;&#x2F;a&gt; that aren&#x27;t supported by Pytorch&#x2F;XLA can have a pretty drastic impact on the speed of your program, so if your code is running unusually slow on a TPU, unsupported ops are a common culprit. &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;xla&#x2F;issues&#x2F;1777&quot;&gt;For example&lt;&#x2F;a&gt;, I was trying to use the memory-saving Adafactor optimizer on Pytorch&#x2F;XLA but since I was using a non-Pytorch operation in one part of the code (using Python&#x27;s &lt;code&gt;sqrt()&lt;&#x2F;code&gt; function instead of &lt;code&gt;torch.sqrt()&lt;&#x2F;code&gt;), a single iteration was taking ~10 seconds compared to 10 iterations&#x2F;second for SGD.&lt;&#x2F;p&gt;
&lt;p&gt;TPU support for Pytorch works pretty differently from TPU support for Tensorflow. Each TPU has a powerful dedicated CPU and several 100GBs of RAM for data processing, so whenever you run TF code on a TPU, your data gets copied to each core&#x27;s CPU (unless you use TFRecord files, in which case each core&#x27;s CPU downloads and processes data directly from your cloud bucket) and gets processed there. By doing it in this way, you only need to rent a small cloud instance (like a n1-standard-1) and scaling your code from a single TPU board with 8 cores to a part of a TPU pod is (relatively) painless.&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, Pytorch&#x2F;XLA can&#x27;t &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;xla&#x2F;issues&#x2F;1742&quot;&gt;currently use&lt;&#x2F;a&gt; the TPU&#x27;s CPU and instead has to replicate the data $8$ times on your own VM for an $8$ core TPU board. If you want to use Pytorch&#x2F;XLA for a TPU pod, you have to create a VM group with one host VM for each $8$ core TPU board. This means that Pytorch&#x2F;XLA isn&#x27;t currently practical for large scale training, but it &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;xla&#x2F;issues&#x2F;1858&quot;&gt;looks like&lt;&#x2F;a&gt; the next version of TPUs will be a lot more optimized for Pytorch. It works alright for a small dataset like WikiText-2 but when I tried finetuning on WikiText-103 (~500mb, 100m words) I needed to upgrade my VM to have 80+ GB of RAM.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;finetuning&quot;&gt;Finetuning&lt;a class=&quot;zola-anchor&quot; href=&quot;#finetuning&quot; aria-label=&quot;Anchor link for: finetuning&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I wasn&#x27;t able to finetune GPT2-1.5b on a TPU with the AdamW optimizer even with the TPU&#x27;s built in bfloat16 support, so most of the experiments that I ran were with the memory-saving Adafactor optimizer with &lt;code&gt;beta1&lt;&#x2F;code&gt; set to zero to disable momentum. Enabling momentum might increase the performance of the Adafactor optimizer, but would also require storing an extra momentum value for each parameter and would make it harder to train larger models.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Fun fact: The AdamW optimizer implementation in Google&#x27;s official BERT &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;google-research&#x2F;bert&#x2F;blob&#x2F;master&#x2F;optimization.py#L65&quot;&gt;repository&lt;&#x2F;a&gt; excludes layernorm and bias parameters from weight decay and AFAICT is the only optimizer that does so. I tried running a few experiments with and without finetuning these parameters and didn&#x27;t find any significant difference in performance.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Most of the GPU experiments I did were with NVidia&#x27;s Apex library, with its $01$ mixed precision setting. I also tried running a few experiments on using only FP16, but the gradients would explode or vanish and the model wouldn&#x27;t train.&lt;&#x2F;p&gt;
&lt;p&gt;I have a &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;transformers&#x2F;tree&#x2F;grad-checkpointing&quot;&gt;forked version&lt;&#x2F;a&gt; of Huggingface&#x27;s Transformers repository where I&#x27;ve implemented gradient checkpointing for GPT-2. I haven&#x27;t been maintaining it but you can see all the changes that I did to make it work (it&#x27;s really only a few lines of code) &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;compare&#x2F;master...bkkaggle:grad-checkpointing&quot;&gt;here&lt;&#x2F;a&gt;. I tried training GPT2-XL with grad checkpointing which IIRC worked with a smaller context length of 256 but still threw OOM errors when finetuning at a context length of 1024.&lt;&#x2F;p&gt;
&lt;p&gt;For small datasets like WikiText-2, (WikiText-2 consists of about 2 million words, so it&#x27;s actually on the larger size for datasets that you might collect yourself) the model usually overfits within the first 1-3 epochs, so most of the experiments that I did trained for a single epoch — there really is no performance benefit to finetuning for any longer. I set the learning rate for all of my finetuning experiments to $5e-5$ (This was just the first value I tried, no hyperparameter tuning involved) and linearly increased the learning rate from $0$ to $5e-5$ over the first 10% of the training iterations and then linearly decayed it to zero over the rest of the training iterations.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: The Adafactor paper shows that warmup is strongly recommended to stabilize training, take a look at the &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1804.04235&quot;&gt;paper&lt;&#x2F;a&gt; for more details&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;If you want to finetune GPT2 on a dataset like WikiText-2, there&#x27;s a relationship between the batch size, learning rate, and the number of training iterations that you need to adjust to train effectively and avoid overfitting or plateauing. There&#x27;s a pretty important ratio that you need to keep constant between the batch size and the learning rate. A larger batch size means that there are fewer gradient updates performed if you keep the number of training iterations constant.&lt;&#x2F;p&gt;
&lt;p&gt;I have a &lt;a href=&quot;https:&#x2F;&#x2F;app.wandb.ai&#x2F;bkkaggle&#x2F;lm-finetuning&#x2F;reports&#x2F;1-epoch-context-len--Vmlldzo3NTI4MA&quot;&gt;WandB report&lt;&#x2F;a&gt; showing a few different training runs on WikiText-2 with different sized GPT2 models and context lengths. The results aren&#x27;t really that surprising, finetuning larger models at larger context lengths increases perplexity significantly.&lt;&#x2F;p&gt;
&lt;p&gt;I wrote a quick &lt;a href=&quot;https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1Vxh91ASFvCPgBL0I6ui97SyxtoLBvY3I?usp=sharing&quot;&gt;Colab notebook&lt;&#x2F;a&gt; on how to finetune and evaluate on WikiText-2.&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>Evaluating and Comparing Language Models</title>
            <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/evaluating-language-models/</link>
            <guid>https://bkkaggle.github.io/blog/evaluating-language-models/</guid>
            <description>&lt;h2 id=&quot;tl-dr&quot;&gt;Tl;dr&lt;a class=&quot;zola-anchor&quot; href=&quot;#tl-dr&quot; aria-label=&quot;Anchor link for: tl-dr&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;p&gt;The way you evaluate your language model can have a pretty big effect on validation loss and ppl values. Everyone should clearly report how their language models have been evaluated and try to evaluate their language models similarly to make comparing them easy.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This post is going to be a little different from my previous two posts, where I stuck to making posts to write down what I&#x27;ve learned about ML. This time, I&#x27;m still making notes, but I&#x27;ll also be writing about my work &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;483&quot;&gt;trying&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;gpt-2&#x2F;issues&#x2F;78&quot;&gt;to&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;491&quot;&gt;replicate&lt;&#x2F;a&gt; GPT-2&#x27;s zero-shot results on wikitext2 and wikitext103.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m currently working on finetuning gpt2-like models on small datasets and I wanted to compare the results of my finetuned models on wikitext2 to OpenAI&#x27;s baseline zero-shot results. This sounded like a pretty easy thing to do, but there are many ways that the authors of different papers choose to evaluate and compare their language models—and not all of them are easily comparable.&lt;&#x2F;p&gt;
&lt;p&gt;Different factors can have an impact on the val or test perplexity for a language model on a particular dataset—The &lt;strong&gt;vocabulary size&lt;&#x2F;strong&gt; of your language model, the &lt;strong&gt;context length&lt;&#x2F;strong&gt; that you use to evaluate on, and your &lt;strong&gt;evalutation method&lt;&#x2F;strong&gt; can all make a big difference.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;vocabulary-size&quot;&gt;Vocabulary size&lt;a class=&quot;zola-anchor&quot; href=&quot;#vocabulary-size&quot; aria-label=&quot;Anchor link for: vocabulary-size&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The size of the input vocabulary for your language model can make it easier or harder for your language model to predict the next token in a sequence, for example, a character level language model with 26 tokens (one for each letter of the english alphabet) will have a lower perplexity that a word level language model with hundreds of thousands of tokens. Think of it like this, it&#x27;s a lot easier to predict the next letter in the sentence &lt;code&gt;I’m a computer science and machine learning enthusias&lt;&#x2F;code&gt; (which would be the letter &lt;code&gt;t&lt;&#x2F;code&gt;) than the next word in the sentence &lt;code&gt;I&#x27;m a computer science and machine learning&lt;&#x2F;code&gt; (which is the word &lt;code&gt;enthusiast&lt;&#x2F;code&gt;). This would mean that a character-level language model would have a much lower perplexity value than a word-level model, and that you may be able to break SOTA on most language modelling datasets by just changing the vocabulary!&lt;&#x2F;p&gt;
&lt;p&gt;To make sure that models trained on using different tokenizers (word-level, character-level, BPE, etc) can be compared, you can normalize the loss of a language model with a vocabulary of $V_1$ tokens to a common vocabulary of $V_2$ tokens by multiplying the average loss of the language model with vocabulary size $V_1$ by the ratio between $V_1$ and $V_2$ (you could also sum the losses from the all the tokens and then divide by the number of tokens, but since the two give the identical result, I&#x27;ll just refer to the version where we take the average loss):&lt;&#x2F;p&gt;
&lt;p&gt;$$
normalized_loss = loss_{V_1} * \frac {V_1} {V_2}
$$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;context-length-and-evaluation-method&quot;&gt;Context Length and Evaluation Method&lt;a class=&quot;zola-anchor&quot; href=&quot;#context-length-and-evaluation-method&quot; aria-label=&quot;Anchor link for: context-length-and-evaluation-method&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Language models compute the probability of a sequence $s$ with $n$ tokens with:&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(s) = \prod_{i = 1}^{n} p(w_i | w_1 ... w_{i-1})
$$&lt;&#x2F;p&gt;
&lt;p&gt;Datasets can have thousands to millions to hundreds of millions of tokens, so sending the entire dataset to the language model at once isn&#x27;t possible. To make the calculation of the loss and the perplexity computationally possible, there are two approaches that I&#x27;ve seen other people use:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Splitting the dataset into chunks of length &lt;code&gt;context_len&lt;&#x2F;code&gt;, passing each chunk to the lm separately, and averaging the loss over all the chunks&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Using an overlapping sliding windows approach, still only passing chunks of length &lt;code&gt;context_len&lt;&#x2F;code&gt; to the model at a time, but overlapping $t$ tokens from the previous sequence and not counting these overlapped tokens when calculating the loss.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Approach #1 is the easiest to implement (Like in the official Pytorch example, your dataset would just load in the validation text file, tokenize it, and break it up into &lt;code&gt;context_len&lt;&#x2F;code&gt; chunks to be iterated over) but isn&#x27;t optimal since the lm won&#x27;t have any context to use when predicting the first token in each batch.&lt;&#x2F;p&gt;
&lt;p&gt;This is also the approach taken by most tutorials and reference implementations for evaluating language models. For example, the Pytorch examples for word-level language modelling on wikitext-2 &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, the AWD-LSTM repository &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, and the &#x2F;transformers library&#x27;s language modelling example &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; all evaluate on fixed chunks of length &lt;code&gt;context_len&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In contrast, approach #2 is used by Transformer-XL &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; and Megatron-LM &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;5&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; and is a little more difficult to implement, you still need to break the tokenized validation file into chunks of length &lt;code&gt;context_len&lt;&#x2F;code&gt; but only move the start of each chunk $t$ tokens ahead at a time. The value of $t$ that you choose will make a difference, if you priorize the precision of the resulting loss value and set $t = 1$, your loss will be closer to the true value over the dataset than if you choose $t = 30$ (like Megatron-LM), but using a lower value of $t$ will also increase the amount of time it will take to calculate the loss over the entire validation set, especially if it is very large. Using overlapping sliding windows also means that you will have to only count the loss of the non-overlapping segments, masking out the loss for the first $t$ tokens. The Transformer-XL &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; paper discusses this topic in section 3.1 and shows how its cached sequence of hidden states from previous timesteps lets it evaluate on overlapping sliding windows at a lower computational cost.&lt;&#x2F;p&gt;
&lt;p&gt;Whichever approach you choose, the value of &lt;code&gt;context_len&lt;&#x2F;code&gt; that you choose will also make a significant effect on your loss. On my experiments with gpt2, I could see a decrease of 4ppl across many model sizes (gpt2-medium, gpt2-large, gpt2-xl) just by increasing the context len that the models were evaluated on from 256 to 1024.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;gpt-2-and-zero-shot-results-on-wikitext2-and-wikitext103&quot;&gt;GPT-2 and zero-shot results on wikitext2 and wikitext103&lt;a class=&quot;zola-anchor&quot; href=&quot;#gpt-2-and-zero-shot-results-on-wikitext2-and-wikitext103&quot; aria-label=&quot;Anchor link for: gpt-2-and-zero-shot-results-on-wikitext2-and-wikitext103&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;OpenAI&#x27;s GPT-2 &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#6&quot;&gt;6&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; paper is pretty short on details when it comes to how they ran zero-shot (no finetuning!) evaluation on a range of datasets and several people have also had some trouble &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;483&quot;&gt;trying&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;gpt-2&#x2F;issues&#x2F;78&quot;&gt;to&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;491&quot;&gt;replicate&lt;&#x2F;a&gt; their results.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;model-size&lt;&#x2F;th&gt;&lt;th&gt;loss on wikitext103&#x27;s test set&lt;&#x2F;th&gt;&lt;th&gt;perplexity&lt;&#x2F;th&gt;&lt;th&gt;adjusted perplexity&lt;&#x2F;th&gt;&lt;th&gt;reported perplexities&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;gpt2&lt;&#x2F;td&gt;&lt;td&gt;3.149&lt;&#x2F;td&gt;&lt;td&gt;23.33&lt;&#x2F;td&gt;&lt;td&gt;35.12&lt;&#x2F;td&gt;&lt;td&gt;37.5&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;gpt2-medium&lt;&#x2F;td&gt;&lt;td&gt;2.923&lt;&#x2F;td&gt;&lt;td&gt;18.59&lt;&#x2F;td&gt;&lt;td&gt;27.18&lt;&#x2F;td&gt;&lt;td&gt;26.37&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;gpt2-large&lt;&#x2F;td&gt;&lt;td&gt;2.786&lt;&#x2F;td&gt;&lt;td&gt;16.23&lt;&#x2F;td&gt;&lt;td&gt;23.30&lt;&#x2F;td&gt;&lt;td&gt;22.05&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;gpt2-xl&lt;&#x2F;td&gt;&lt;td&gt;2.706&lt;&#x2F;td&gt;&lt;td&gt;14.97&lt;&#x2F;td&gt;&lt;td&gt;21.28&lt;&#x2F;td&gt;&lt;td&gt;17.48&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;I was able to get these results on WikiText-103&#x27;s test set that are pretty close (except for gpt2-xl, that&#x27;s off by almost 4ppl) to the paper&#x27;s reported results after a bit of experimenting, here&#x27;s what I did:&lt;&#x2F;p&gt;
&lt;p&gt;For my zero-shot results, I used a non-overlapping context length of 1024 tokens (using overlapping sliding windows should get you better results and get you to OpenAI&#x27;s results). As for adjusting the loss to account for GPT-2&#x27;s custom tokenizer, I used the normalized loss calculation from above with the original and tokenized number of tokens from the test file—I split the preprocessed test set on spaces to get $217646$ tokens, and with GPT-2&#x27;s tokenizer to get $249612$ tokens.&lt;&#x2F;p&gt;
&lt;p&gt;OpenAI says in section 3.1 that they used invertible detokenizers to remove tokenization artifacts from the processed WikiText-103 test set (&lt;code&gt;wiki.test.tokens&lt;&#x2F;code&gt;) (like extra spaces before and after punctuation marks) created by the original authors of the dataset. Since they didn&#x27;t provide details on what preprocessing artifacts they removed in either the paper or code, I used the Megatron-LM &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;5&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; project&#x27;s &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;Megatron-LM&#x2F;blob&#x2F;master&#x2F;tasks&#x2F;zeroshot_gpt2&#x2F;detokenizer.py&quot;&gt;invertible detokenizers&lt;&#x2F;a&gt; that they used for their own zero-shot evaluation results on WikiText-103.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;a class=&quot;zola-anchor&quot; href=&quot;#references&quot; aria-label=&quot;Anchor link for: references&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;examples&#x2F;blob&#x2F;master&#x2F;word_language_model&#x2F;main.py#L136&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;github.com&#x2F;salesforce&#x2F;awd-lstm-lm&#x2F;blob&#x2F;master&#x2F;finetune.py#L104&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;blob&#x2F;master&#x2F;examples&#x2F;run_language_modeling.py&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1901.02860&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;5&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1909.08053&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#6&quot;&gt;6&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;cdn.openai.com&#x2F;better-language-models&#x2F;language_models_are_unsupervised_multitask_learners.pdf&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>PyTorch Zoo</title>
            <pubDate>Thu, 30 Apr 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/pytorch-zoo/</link>
            <guid>https://bkkaggle.github.io/blog/pytorch-zoo/</guid>
            <description>&lt;p&gt;I originally wrote this blog post for the PyTorch blog which is available &lt;a href=&quot;https:&#x2F;&#x2F;medium.com&#x2F;pytorch&#x2F;pytorch-zoo-a-collection-of-useful-modules-and-utilities-in-pytorch-c05ca4d500d8?source=friends_link&amp;amp;sk=9fcc0180af0abbc01d26d3680bdab83b&quot;&gt;here&lt;&#x2F;a&gt; (Use this link to access my article with my friend link so you don&#x27;t need to worry about Medium paywalling my article)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;PyTorch Zoo is a collection of modules and utilities that I’ve found to be useful when working on machine learning projects and competitions. PyTorch Zoo contains several modules not available in PyTorch, like cyclical momentum and squeeze-and-excitation, as well as useful utilities like the ability to send notifications and set random seeds to get consistent results. PyTorch Zoo is meant to provide high-quality reference implementations of modules that don’t have official implementations in PyTorch and save you time that would have otherwise been spent searching for implementations on Github or coding the module yourself.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;img src=&#x27;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;bkkaggle&#x2F;pytorch_zoo&#x2F;master&#x2F;screenshot.png&#x27; width=&#x27;100%&#x27;&gt;&lt;&#x2F;img&gt;&lt;&#x2F;p&gt;
&lt;p&gt;From: &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The library is open-source on &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo&quot;&gt;Github&lt;&#x2F;a&gt; and is available as a pip package. Just run:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;pip&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt; install pytorch_zoo
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;to install it in your local development environment and check out the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo#documentation&quot;&gt;documentation&lt;&#x2F;a&gt; for in-depth examples on all the library’s features. I’ve included quite a few modules in PyTorch Zoo, so I’ll try to focus only on some of the ones that I found to be the most interesting for this blog post.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;cyclical-momentum&quot;&gt;Cyclical Momentum&lt;a class=&quot;zola-anchor&quot; href=&quot;#cyclical-momentum&quot; aria-label=&quot;Anchor link for: cyclical-momentum&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Cyclical momentum, which was first proposed in the same paper as cyclical learning rates &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, is usually used together with cyclical learning rates. It decreases the amount of momentum while the learning rate increases and increases the amount of momentum while the learning rate decreases, stabilizing training and allowing for the use of higher learning rates. Here&#x27;s an example of how you could use cyclical momentum just like a normal PyTorch scheduler:&lt;&#x2F;p&gt;
&lt;div &gt;
    &lt;script src=&quot;https:&amp;#x2F;&amp;#x2F;gist.github.com&amp;#x2F;bkkaggle&amp;#x2F;30981747e186c406ea9c3213df9eb510.js&quot;&gt;&lt;&#x2F;script&gt;
&lt;&#x2F;div&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;squeeze-and-excitation&quot;&gt;Squeeze and Excitation&lt;a class=&quot;zola-anchor&quot; href=&quot;#squeeze-and-excitation&quot; aria-label=&quot;Anchor link for: squeeze-and-excitation&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Squeeze and Excitation modules &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; can be easily integrated into existing models by just adding one of these modules after each convolutional block and improves the model’s performance without significantly impacting training time. All three variants of the squeeze-and-excitation block that were proposed in the original papers are available in PyTorch Zoo, see the &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo#modules&quot;&gt;documentation&lt;&#x2F;a&gt; for specific examples on how to use each one. Here&#x27;s an example of how you could use SqueezeAndExcitation in a convolutional block&lt;&#x2F;p&gt;
&lt;div &gt;
    &lt;script src=&quot;https:&amp;#x2F;&amp;#x2F;gist.github.com&amp;#x2F;bkkaggle&amp;#x2F;bd6e4d9a706f207235e193f85fedb8ec.js&quot;&gt;&lt;&#x2F;script&gt;
&lt;&#x2F;div&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;utilities&quot;&gt;Utilities&lt;a class=&quot;zola-anchor&quot; href=&quot;#utilities&quot; aria-label=&quot;Anchor link for: utilities&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;PyTorch Zoo also has a small range of utilities to make it easier to follow PyTorch best practices when doing things like saving a model to disk and setting random seeds, as well as easy to use one-liners to do things like sending push notifications when a training run ends.&lt;&#x2F;p&gt;
&lt;p&gt;Here’s an example of how you could use some of these utilities:&lt;&#x2F;p&gt;
&lt;div &gt;
    &lt;script src=&quot;https:&amp;#x2F;&amp;#x2F;gist.github.com&amp;#x2F;bkkaggle&amp;#x2F;6b00aeff6a3aa9f3ea62b3b26a358fca.js&quot;&gt;&lt;&#x2F;script&gt;
&lt;&#x2F;div&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusion&quot; aria-label=&quot;Anchor link for: conclusion&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;To learn more about PyTorch Zoo and its features, check out our &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;bkkaggle&#x2F;pytorch_zoo&quot;&gt;Github repository&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The project is still a work in progress, so if you find a bug, think there is something missing, or have any suggestions for new features or modules, feel free to open an issue or a pull request. Feel free to use the library or code from it in your own projects, and if you feel that some code used in this project hasn’t been properly accredited, please open an issue.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;a class=&quot;zola-anchor&quot; href=&quot;#references&quot; aria-label=&quot;Anchor link for: references&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h1&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.09820&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1709.01507&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.02579&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>Perplexity</title>
            <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/perplexity/</link>
            <guid>https://bkkaggle.github.io/blog/perplexity/</guid>
            <description>&lt;blockquote&gt;
&lt;p&gt;Updated on Aug 2, 2020: Add link to more resources&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;normalization&quot;&gt;Part 1: Normalization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;perplexity&quot;&gt;Part 2: Perplexity&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;initialization&quot;&gt;Part 3: Initialization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;memory-usage&quot;&gt;Part 4: GPU Memory Usage Breakdown&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;adafactor&quot;&gt;Part 5: Adafactor&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;purpose&quot;&gt;Purpose&lt;a class=&quot;zola-anchor&quot; href=&quot;#purpose&quot; aria-label=&quot;Anchor link for: purpose&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in learning, help me keep track of everything I&#x27;ve learned over the last three years, and to practice my Latex skills.&lt;&#x2F;p&gt;
&lt;p&gt;This is my second blog post in the series, and this time I&#x27;m taking notes on evaluation metrics in NLP.&lt;&#x2F;p&gt;
&lt;p&gt;Most of the content of this post comes from &lt;a href=&quot;https:&#x2F;&#x2F;huyenchip.com&#x2F;&quot;&gt;Chip Huyen&#x27;s&lt;&#x2F;a&gt; really good article in &lt;a href=&quot;https:&#x2F;&#x2F;thegradient.pub&#x2F;&quot;&gt;The Gradient&lt;&#x2F;a&gt; on &lt;a href=&quot;https:&#x2F;&#x2F;thegradient.pub&#x2F;understanding-evaluation-metrics-for-language-models&#x2F;&quot;&gt;Evaluation methods for language models&lt;&#x2F;a&gt; and the &lt;a href=&quot;https:&#x2F;&#x2F;www.deeplearningbook.org&#x2F;&quot;&gt;Deep Learning&lt;&#x2F;a&gt; book, so a big thank you to the authors and editors for making this perplexing (pun intended) topic easy to understand.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;let me be 100% clear here, I don&#x27;t want to come across like I&#x27;m taking someone else&#x27;s ideas and publishing them as my own. The purpose of this blog post is to take notes for myself so I can come back to this when I inevitably forget how to calculate perplexity.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Also, take a look at &lt;a href=&quot;https:&#x2F;&#x2F;sjmielke.com&#x2F;comparing-perplexities.htm&quot;&gt;this&lt;&#x2F;a&gt; for another good look at perplexity and the effect of tokenization on it.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;background&quot;&gt;Background&lt;a class=&quot;zola-anchor&quot; href=&quot;#background&quot; aria-label=&quot;Anchor link for: background&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Language models like GPT2 try to predict the next word (or subword&#x2F;character, we&#x27;ll use the term &lt;code&gt;token&lt;&#x2F;code&gt; in this blog post), in a context of tokens.&lt;&#x2F;p&gt;
&lt;p&gt;For example, when predicting the next word in the sentence &lt;code&gt;&amp;quot;I am a computer science and machine learning&amp;quot;&lt;&#x2F;code&gt;, the probability of the next work being &lt;code&gt;enthusiast&lt;&#x2F;code&gt; could be represented by&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(enthusiast | I \space am \space a \space computer \space science \space and \space machine \space learning)
$$&lt;&#x2F;p&gt;
&lt;p&gt;The probability of a sentence $s$, where $s$ is a sequence of n tokens $(w_{0}, w_{1}, ... w_{n})$ can be represented as&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(s) = \prod_{i = 1}^{n} p(w_i | w_1 ... w_{i-1})
$$&lt;&#x2F;p&gt;
&lt;p&gt;expanded, it looks like this:&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(s) = p(w_{1})p(w_{2} | w_{1})p(w_{3} | w_{1}, w_{2})...p(w_{n} | w_{1} w_{2} ... w_{n - 1})
$$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;information-theory&quot;&gt;Information Theory&lt;a class=&quot;zola-anchor&quot; href=&quot;#information-theory&quot; aria-label=&quot;Anchor link for: information-theory&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The amount of information given by a discrete event $x$ is calculated by the &lt;strong&gt;Self-Information&lt;&#x2F;strong&gt; equation &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;a name=&quot;5&quot;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;$$
I(x) = -log \space P(x)
$$&lt;&#x2F;p&gt;
&lt;p&gt;Information is normally written in one of two units, $nats$, in which case the logarithm has a base of $e$ or $bits$, with a base of $2$.&lt;&#x2F;p&gt;
&lt;p&gt;One $nat$ encodes the &amp;quot;amount of information gained by observing an event with a probability of $\frac {1} {e}$.&amp;quot; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;a name=&quot;5&quot;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;shannon-entropy&quot;&gt;Shannon Entropy&lt;a class=&quot;zola-anchor&quot; href=&quot;#shannon-entropy&quot; aria-label=&quot;Anchor link for: shannon-entropy&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Shannon entropy&lt;&#x2F;strong&gt; is the extension of the &lt;strong&gt;Self-Information&lt;&#x2F;strong&gt; equation to probability distributions and is a way to &amp;quot;quantify the amount of uncertainty in an entire probability distribution.&amp;quot; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;a name=&quot;5&quot;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;$$
H(x) = \mathbb E_{x \sim P} [log \space P(x)]
$$&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s a measure of how much information, on average is produced for each letter of a language &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;a name=&quot;1&quot;&gt;&lt;&#x2F;a&gt; and (if calculated in units of
$bits$) can also be defined as the average number of binary digits required to encode each letter in a vocabulary.&lt;&#x2F;p&gt;
&lt;p&gt;In NLP, the evaluation metric, &lt;strong&gt;Bits-per-character&lt;&#x2F;strong&gt; (BPC), is really just the entropy of a sequence, calculated with units of bits instead of nats.&lt;&#x2F;p&gt;
&lt;p&gt;Entropy calculated across language models that are trained over different context lengths aren&#x27;t exactly comparable, LMs with a longer context len will have more information from which to predict the next token. For example, given the sentence &lt;code&gt;I work with machine learning&lt;&#x2F;code&gt; it should be easier for a LM to predict the next word in the sequence &lt;code&gt;I work with machine&lt;&#x2F;code&gt;, than with just the first word: &lt;code&gt;I&lt;&#x2F;code&gt;. &lt;em&gt;(This is actually a major pain point when I was trying to reproduce gpt2&#x27;s ppl numbers on wikitext2 and wikitext103, it&#x27;s still unclear how the paper evaluated the ppl values on the tests sets for both datasets.)&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;perplexity&quot;&gt;Perplexity&lt;a class=&quot;zola-anchor&quot; href=&quot;#perplexity&quot; aria-label=&quot;Anchor link for: perplexity&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Perplexity&lt;&#x2F;strong&gt;: A measurement of how well a probability distribution or probability model predicts a sample &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;a name=&quot;2&quot;&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Perplexity is usually calculated with units of $nats$, so calculate it with the equation: $PPL = e^{loss}$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;dealing-with-different-tokenization-schemes&quot;&gt;Dealing with different tokenization schemes&lt;a class=&quot;zola-anchor&quot; href=&quot;#dealing-with-different-tokenization-schemes&quot; aria-label=&quot;Anchor link for: dealing-with-different-tokenization-schemes&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you want to convert the perplexity between models that have been trained using different tokenization schemes and have a different number of tokens that the LM can predict, multiply the cross-entropy loss of the first language model by the ratio of $(\text{n tokens first model} &#x2F; \text{n tokens seconds model})$&lt;&#x2F;p&gt;
&lt;p&gt;The adjusted perplexity value can be found with &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;a name=&quot;4&quot;&gt;&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;$$
adj_ppl = e^{loss * (\text{#tokens} &#x2F; \text{#tokens for other model})}
$$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;a class=&quot;zola-anchor&quot; href=&quot;#references&quot; aria-label=&quot;Anchor link for: references&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; Claude E Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):50–64, 1951.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Perplexity&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;5&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;stats.stackexchange.com&#x2F;questions&#x2F;211858&#x2F;how-to-compute-bits-per-character-bpc&#x2F;261789&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;Megatron-LM&#x2F;blob&#x2F;master&#x2F;evaluate_gpt2.py#L282&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; Chapter 3, Deep Learning, Ian Goodfellow, Yoshua Bengio and Aaron Courville, 2016, MIT Press&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>Normalization</title>
            <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/normalization/</link>
            <guid>https://bkkaggle.github.io/blog/normalization/</guid>
            <description>&lt;blockquote&gt;
&lt;p&gt;Updated on Jun 26, 2020: Fix BatchNorm and LayerNorm equations&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;normalization&quot;&gt;Notes Part 1: Normalization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;perplexity&quot;&gt;Notes Part 2: Perplexity&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;initialization&quot;&gt;Notes Part 3: Initialization&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;memory-usage&quot;&gt;Notes Part 4: GPU Memory Usage Breakdown&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;&#x2F;blog&#x2F;adafactor&quot;&gt;Part 5: Adafactor&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;purpose&quot;&gt;Purpose&lt;a class=&quot;zola-anchor&quot; href=&quot;#purpose&quot; aria-label=&quot;Anchor link for: purpose&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The purpose of these series of blog posts is to be a place to store my (still in-progress!) notes about topics in machine learning, help me keep track of everything I&#x27;ve learned over the last three years, and to practice my Latex skills.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m starting this series of blog posts by writing down my notes on the different types of normalization in neural networks. Let&#x27;s see how this goes.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;why-normalize&quot;&gt;Why normalize?&lt;a class=&quot;zola-anchor&quot; href=&quot;#why-normalize&quot; aria-label=&quot;Anchor link for: why-normalize&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Not normalizing input activations means that layers can transform activations to have very large or small means and standard deviations and cause the gradients to explode or vanish.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;a class=&quot;zola-anchor&quot; href=&quot;#batch-normalization&quot; aria-label=&quot;Anchor link for: batch-normalization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1502.03167&quot;&gt;Arxiv&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Tl;dr&lt;&#x2F;strong&gt;: Calculate the mean and standard deviation for each feature in the batch across the batch dimension and normalize to have a mean of $0$ and a standard deviation of $1$, then scale the resulting activations by two learned parameters $\gamma$ and $\beta$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;$$
\text{For a mini-batch of activations} \space B = { { x_{1} ... x_{m} } },
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mu_{B} \leftarrow \frac{1} {m} \sum_{i=1}^{m} x_{i}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\sigma_{B}^{2} \leftarrow \frac{1} {m} \sum_{i=1}^{m} (x_{i} - \mu_{B}) ^ 2
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{x} \leftarrow \frac {x_{i} - \mu_{B}} {\sqrt {\sigma_{B}^{2} + \epsilon}}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
y_{i} \leftarrow \gamma \hat{x_{i}} + \beta
$$&lt;&#x2F;p&gt;
&lt;p&gt;In Batch Normalization &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, you first calculate the mean and variance of the input tensor across the batch dimension, then subtract the input tensor by the mean $\mu_{B}$ and divide by the standard deviation (plus a small value to prevent dividing by $0$) $\sqrt {\sigma_{B}^{2}}$ to restrict the activations of the neural network to having a mean of $0$ and a standard deviation of $1$&lt;&#x2F;p&gt;
&lt;p&gt;You then scale the activations with learned parameters by rescaling the zero-mean activations by two learned parameters $\beta$ and $\gamma$.&lt;&#x2F;p&gt;
&lt;p&gt;The original paper claimed that the reason batch norm worked so well was by reducing &lt;strong&gt;internal covariate shift&lt;&#x2F;strong&gt; (&amp;quot;The change in the distribution of the input values to a learning algorithm&amp;quot; &lt;a href=&quot;https:&#x2F;&#x2F;mlexplained.com&#x2F;2018&#x2F;01&#x2F;10&#x2F;an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1&#x2F;&quot;&gt;link&lt;&#x2F;a&gt;), but more recent papers have disputed this and given other reasons to why it works so well.&lt;&#x2F;p&gt;
&lt;p&gt;This lets the network choose the mean and standard deviation that it wants for its activations before they are passed to a convolutional or fully connected layer.&lt;&#x2F;p&gt;
&lt;p&gt;One question that I&#x27;ve had over and over again related to batch norm is where exactly to place it in a network, and it looks like other people &lt;a href=&quot;https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;t&#x2F;batch-normalization-of-linear-layers&#x2F;20989&#x2F;2&quot;&gt;have&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;forums.fast.ai&#x2F;t&#x2F;where-should-i-place-the-batch-normalization-layer-s&#x2F;56825&quot;&gt;had&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MachineLearning&#x2F;comments&#x2F;67gonq&#x2F;d_batch_normalization_before_or_after_relu&#x2F;&quot;&gt;the&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;39691902&#x2F;ordering-of-batch-normalization-and-dropout&quot;&gt;same&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;keras-team&#x2F;keras&#x2F;issues&#x2F;1802&quot;&gt;  question&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The original paper places the batch norm layer after the convolutional layer and before the non-linearity, which is the default used by torchvision and other model zoos. It also claims that using batch norm can reduce or eliminate the need to use dropout, so the order could look like either of these:&lt;&#x2F;p&gt;
&lt;p&gt;$$
Conv \rightarrow BN \rightarrow ReLU \rightarrow Dropout
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
Conv \rightarrow BN \rightarrow ReLU
$$&lt;&#x2F;p&gt;
&lt;p&gt;Some benchmarks show that placing the batch norm layer after the non-linearity can perform better &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#8&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;$$
Conv \rightarrow ReLU  \rightarrow BN  \rightarrow Dropout
$$&lt;&#x2F;p&gt;
&lt;p&gt;but this isn&#x27;t widely used.&lt;&#x2F;p&gt;
&lt;p&gt;One major disadvantage with this is that the pre-normalized activations must be saved for the backwards pass. This means that if you add a batchnorm layer for each convolutional layer in your network (which is a common practice), your network will need about twice the memory to store the same batch size into the GPU. Beyond using up more GPU memory, batch norm doesn&#x27;t work with batch sizes of 1, and doesn&#x27;t perform well with small batch sizes since the calculated mean and standard deviation for each batch will change a lot from batch to batch and gives the model a very noisy estimate of the true distribution.&lt;&#x2F;p&gt;
&lt;p&gt;Another thing you should keep in mind about batch norm is that when training on multiple gpus or machines, that by default, each gpu will keep its own mean and standard deviation parameters, which can be a problem if the per-gpu batch size is too low. There are synchronized batch norm implementations available that should fix this. Another thing to keep in mind is what mean and standard deviation values to use when evaluating on a test set or finetuning on a new dataset.&lt;&#x2F;p&gt;
&lt;p&gt;Other work, like &lt;strong&gt;In-Place Batch normalization&lt;&#x2F;strong&gt; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; reduces the memory usage by recomputing the pre-batchnorm activations from the post-batchnorm activations, while others, like &lt;strong&gt;Fixup Initialization&lt;&#x2F;strong&gt; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, &lt;strong&gt;MetaInit&lt;&#x2F;strong&gt; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;5&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, &lt;strong&gt;LSUV&lt;&#x2F;strong&gt; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;6&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, and &lt;strong&gt;Delta Orthogonal&lt;&#x2F;strong&gt; &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#6&quot;&gt;7&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; use special initialization strategies to remove the need for batch normalization.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;layer-normalization&quot;&gt;Layer normalization&lt;a class=&quot;zola-anchor&quot; href=&quot;#layer-normalization&quot; aria-label=&quot;Anchor link for: layer-normalization&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1607.06450&quot;&gt;Arxiv&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Tl;dr&lt;&#x2F;strong&gt;: Calculate the mean and standard deviation for element in the batch across the feature dimension and normalize to have a mean of $0$ and a standard deviation of $1$, then scale the resulting activations by two learned parameters $\gamma$ and $\beta$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;For activations in a batch of shape $x_{ij}$, where $i$ is the batch dimension and $j$ is the feature dimension (assuming this is a simple feedforward network),&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mu_{i} \leftarrow \frac{1} {m} \sum_{j=1}^{m} x_{ij}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\sigma_{i}^{2} \leftarrow \frac{1} {m} \sum_{j=1}^{m} (x_{ij} - \mu_{i}) ^ 2
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{x} \leftarrow \frac {x_{ij} - \mu_{i}} {\sqrt {\sigma_{i}^{2} + \epsilon}}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
y_{ij} \leftarrow \gamma \hat{x_{ij}} + \beta
$$&lt;&#x2F;p&gt;
&lt;p&gt;Layer Normalization &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#7&quot;&gt;8&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, is almost identical to batch normalization except that layer norm normalizes across the feature dimension instead of the batch dimension. This means that layer norm calculates a mean and standard deviation value for for each element in the batch instead of for each feature over all elements in the batch.&lt;&#x2F;p&gt;
&lt;p&gt;Layer norm is used mostly for RNNs and Transformers and has the same GPU memory requirements as batch norm.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;a class=&quot;zola-anchor&quot; href=&quot;#resources&quot; aria-label=&quot;Anchor link for: resources&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;These are some of the amazing and very helpful blog posts, tutorials, and deep dives that have helped me learn about the topic and write this blog post.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;mlexplained.com&#x2F;2018&#x2F;11&#x2F;30&#x2F;an-overview-of-normalization-methods-in-deep-learning&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;issues&#x2F;1959&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;a class=&quot;zola-anchor&quot; href=&quot;#references&quot; aria-label=&quot;Anchor link for: references&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1502.03167&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1712.02616&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1901.09321&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;5&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;openreview.net&#x2F;pdf?id=SyeO5BBeUr&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#5&quot;&gt;6&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1511.06422&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#6&quot;&gt;7&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1806.05393&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#7&quot;&gt;8&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1607.06450&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#8&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;github.com&#x2F;ducha-aiki&#x2F;caffenet-benchmark&#x2F;blob&#x2F;master&#x2F;batchnorm.md&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>What is AI?</title>
            <pubDate>Mon, 23 Mar 2020 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/what-is-ai/</link>
            <guid>https://bkkaggle.github.io/blog/what-is-ai/</guid>
            <description>&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;docs.google.com&#x2F;document&#x2F;d&#x2F;1SUHN3l1kMzJmbtS27P5kOpOqUdWHyJgkykuOWOsf-9g&#x2F;edit?usp=sharing&quot;&gt;&lt;em&gt;Google Doc&lt;&#x2F;em&gt;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;This was originally a report I made for my ELA class that I&#x27;ve formatted into a blog post.&lt;&#x2F;em&gt;&lt;br &#x2F;&gt;
&lt;em&gt;Since I had to create the report in a specific way for the assignment, some of the information here has isn&#x27;t really relevant to most people who would be reading this blog post and is less technical.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Artificial Intelligence, or as it’s more commonly known, AI, has been said to either transform our world into a utopia, or bring about our doom. With so many widely publicized news stories about AI systems that can generate convincingly human-like text, defeat world champions at board games that were previously thought to be too hard for a computer, or generate images of human faces that appear indistinguishable from the real things, it can seem that we are close to a point at which AI may become self-aware and pose a threat to humans.&lt;&#x2F;p&gt;
&lt;p&gt;To truly understand if these fears of AI surpassing human intelligence and taking over the world are justified, we must first know, what is AI?&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-is-ai&quot;&gt;What is AI?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-is-ai&quot; aria-label=&quot;Anchor link for: what-is-ai&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;What is AI? For some, it conjures up pictures from movies like Terminator and The Matrix, of robots gaining sentience and taking over the world. The term “Artificial Intelligence” was coined in 1956, by computer scientist John McCarthy, who, in his article, defines AI as “The science and engineering of making intelligent machines, especially computer programs” &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;. Other people define AI in similar terms, saying that “AI is a collection of methods and ideas for building software that can do some of the things that humans can do with their brains” &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-is-the-history-of-ai&quot;&gt;What is the history of AI?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-is-the-history-of-ai&quot; aria-label=&quot;Anchor link for: what-is-the-history-of-ai&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The field of AI started in the years after the end of World War II. In 1947, the famous British mathematician and code-breaker Alan Turing gave a lecture on programming computers to develop intelligent machines &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;. Turing was also the creator of the Turing test, a test to determine “a machine’s ability to exhibit intelligent behavior similar to that of a human.”&lt;&#x2F;p&gt;
&lt;p&gt;In the 1960s, researchers at MIT developed a chatbot (a chatbot is a computer program that attempts to carry on a conversation with a human) called ELIZA which was able to pass the Turing test and show that it is possible for a computer program to create human-like text.&lt;&#x2F;p&gt;
&lt;p&gt;In the 1970s and 80s, neural networks, a family of algorithms that are loosely based on the neurons in a human brain, were developed. Neural networks excelled at learning patterns from large amounts of data and were used to automate tasks like reading addresses from envelopes.&lt;&#x2F;p&gt;
&lt;p&gt;In the 1990s and 2000s more progress was made in solving large problems in AI. In 1996, IBM’s Deep Blue computer beat the world’s best chess player, and in 2000, Honda released ASIMO, a humanoid robot that was capable of walking and recognizing objects and gestures.&lt;&#x2F;p&gt;
&lt;p&gt;In 2010, IBM’s Watson computer beat the best human competitors on the trivia game show Jeopardy!. Since around 2012, a lot of AI research is being done in machine learning - deep learning in particular. Deep learning involves stacking layers of neural networks on top of each other to create “deep” neural networks. Neural networks are now used in most of the widely used AI applications today - digital assistants like Siri and Alexa, self-driving cars, and recommendation algorithms from Netflix, Youtube, and other social media companies are all using neural networks in some part. Neural networks currently work better than other AI techniques in many areas because of their ability to learn from large amounts of data and because of the increasing amount of computational power available to train them &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; .&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;what-are-the-different-types-of-ai&quot;&gt;What are the different types of AI?&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-are-the-different-types-of-ai&quot; aria-label=&quot;Anchor link for: what-are-the-different-types-of-ai&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;AI is not one single area of research, it consists of many different branches that each have different views on how to build artificially intelligent systems. There are two main types of artificial intelligence, narrow AI and general AI. &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Most of the advances in AI have been in narrow AI: getting computers to learn how to do certain tasks as good as or better than a human. Although computers can now do certain tasks better than humans, narrow AI systems are highly specialized - a system designed for classifying images can&#x27;t be used to control a robot arm &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; . One example of a branch of narrow AI would be Machine Learning, or ML. Machine learning involves teaching a computer to iteratively learn to solve a task by giving it a large amount of data to learn from. In this way, machine learning lets computers learn how to do tasks without explicitly giving it instructions on how to do so &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; .&lt;&#x2F;p&gt;
&lt;p&gt;General AI involves computers that can generalize to a wide variety of tasks like humans do. So far, there has been very little progress on developing general AI, so any general AI systems are very likely decades away, if not more. &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;in-what-areas-is-ai-being-used&quot;&gt;In what areas is AI being used?&lt;a class=&quot;zola-anchor&quot; href=&quot;#in-what-areas-is-ai-being-used&quot; aria-label=&quot;Anchor link for: in-what-areas-is-ai-being-used&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;AI is being used by researchers in a wide variety of areas and for a wide variety of purposes. AI is being used in healthcare to predict the spread of the Coronavirus epidemic, predict with radiologist-level accuracy whether a person has cancer from an x-ray scan, and to more accurately predict the folded structure of proteins, which is a crucial step in designing new life-saving medicines.&lt;&#x2F;p&gt;
&lt;p&gt;AI is also being used by companies in three main ways. First, AI is being used for RPA (Robotic Process Automation), automating time-consuming administrative tasks like transferring data from emails to spreadsheets and databases. Second, AI is being used to gain cognitive insights (which involves using algorithms to “detect patterns in vast volumes of data and interpret their meaning”) by predicting what items customers will buy next and identifying credit card fraud in real time. Finally, AI is being used for cognitive engagement (using AI to engage with potential customers) with chatbots providing customer service at any time and creating customized care plans. &lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; &lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;a class=&quot;zola-anchor&quot; href=&quot;#conclusion&quot; aria-label=&quot;Anchor link for: conclusion&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Artificial intelligence today is limited to computers that can do certain tasks, sometimes as good as or even better than what a human could do, but highly specialized and limited to the scope of the task that it was trained to do. Once one knows the limitations of AI as we have it today, the claims that AI is close to surpassing human intelligence and taking over the world seem unfounded.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;a class=&quot;zola-anchor&quot; href=&quot;#references&quot; aria-label=&quot;Anchor link for: references&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h2&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; http:&#x2F;&#x2F;jmc.stanford.edu&#x2F;articles&#x2F;whatisai&#x2F;whatisai.pdf&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;www.skynettoday.com&#x2F;editorials&#x2F;ai-coverage-best-practices&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; https:&#x2F;&#x2F;blogs.nvidia.com&#x2F;blog&#x2F;2016&#x2F;07&#x2F;29&#x2F;whats-difference-artificial-intelligence-machine-learning-deep-learning-ai&#x2F;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#4&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; Davenport, Thomas H and Ronanki, Rajeev. “Artificial Intelligence for the real world.” Harvard Business Review. January-February 2018: Pages 110 and 112&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>demo</title>
            <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
            <link>https://bkkaggle.github.io/blog/demo/</link>
            <guid>https://bkkaggle.github.io/blog/demo/</guid>
            <description>&lt;p&gt;$$\sum$$&lt;&#x2F;p&gt;
&lt;p&gt;This is zerm, a minimalist theme for Zola based&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; off of &lt;a href=&quot;https:&#x2F;&#x2F;twitter.com&#x2F;panr&quot;&gt;panr&#x27;s&lt;&#x2F;a&gt;
theme for Hugo.&lt;&#x2F;p&gt;
&lt;p&gt;Inline code: &lt;code&gt;println!(&amp;quot;Wu Tang!&amp;quot;);&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;&quot;&gt;
&lt;code&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;fn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#5cb3fa;&quot;&gt;foo&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#eb6772;&quot;&gt;arg&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;: String) -&amp;gt; Result&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;u32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;Io::&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;Error&amp;gt; {
    println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Nice!&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;); &lt;&#x2F;span&gt;&lt;span style=&quot;font-style:italic;color:#5f697a;&quot;&gt;&#x2F;&#x2F; TODO: the thingy
    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cd74e8;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#adb7c9;&quot;&gt;!= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;0 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;{
        println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;How many ligatures can I contrive??&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;);
        println!(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#9acc76;&quot;&gt;&amp;quot;Turns out a lot! ==&amp;gt; -&#x2F;-&amp;gt; &amp;lt;!-- &amp;lt;$&amp;gt; &amp;gt;&amp;gt;=&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;);
    }
    Ok(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#db9d63;&quot;&gt;42&lt;&#x2F;span&gt;&lt;span style=&quot;color:#abb2bf;&quot;&gt;)
}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;blockquote&gt;
&lt;p&gt;In Hotel Rwanda, reminder to honor these street scholars who ask why
U.S. Defense is twenty percent of the tax dollar. Bush gave 6.46 billion to
Halliburton for troops support efforts in Iraq; meanwhile, the hood is hurting,
please believe that.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;header-iii&quot;&gt;Header III&lt;a class=&quot;zola-anchor&quot; href=&quot;#header-iii&quot; aria-label=&quot;Anchor link for: header-iii&quot;&gt;§&lt;&#x2F;a&gt;
&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;members&lt;&#x2F;th&gt;&lt;th&gt;age&lt;&#x2F;th&gt;&lt;th&gt;notable album&lt;&#x2F;th&gt;&lt;th&gt;to be messed with?&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;GZA&lt;&#x2F;td&gt;&lt;td&gt;52&lt;&#x2F;td&gt;&lt;td&gt;Liquid Swords&lt;&#x2F;td&gt;&lt;td&gt;no&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Inspectah Deck&lt;&#x2F;td&gt;&lt;td&gt;49&lt;&#x2F;td&gt;&lt;td&gt;CZARFACE&lt;&#x2F;td&gt;&lt;td&gt;&lt;code&gt;protect ya neck, boy&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;div &gt;
    &lt;iframe src=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;embed&#x2F;UUpuz8IObcs&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;&#x2F;iframe&gt;
&lt;&#x2F;div&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;1&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;1&lt;&#x2F;sup&gt;
&lt;p&gt;fork? port? a little bit of the former, more of the latter?&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
</description>
        </item>
    </channel>
</rss>
